[
["index.html", "Phân tích dữ liệu thực tế với R Lời mở đầu Tại sao nên đọc cuốn sách này Cấu trúc của sách Các phần mềm được sử dụng Lời cảm ơn", " Phân tích dữ liệu thực tế với R Hoàng Đức Anh 2018-09-24 Lời mở đầu Trong quá trình triển khai công việc thực tế tại nhiều tổ chức khác nhau, tôi nhận thấy một hiện thực là mặt bằng kiến thức thực tế về phân tích dữ liệu trên thị trường Việt Nam còn rất yếu. Phần lớn, các bạn làm trong ngành phân tích dữ liệu rơi vào một trong hai nhóm sau: Nhóm một, chưa có nền tảng về phân tích thống kê. Ở nhóm này, các bạn thường không được đào tạo bài bản hoặc không có đủ điều kiện (phần lớn là về thời gian) để học các kiến thức về phân tích thống kê. Do nhu cầu của công việc, các bạn có thành thạo các kỹ năng về xây dựng báo cáo và phân tích khám phá dữ liệu đơn giản với SQL và Excel. Các bạn này thường thuộc các nhóm phân tích báo cáo (Business Intelligence) tại các tổ chức lớn hoặc làm trong startup. Điểm mạnh của nhóm này là làm việc sát với các bộ phận kinh doanh, hiểu rõ nghiệp vụ và nhu cầu nghiệp vụ. Tuy nhiên, điểm yếu của các bạn lại là không thể ứng dụng hoặc tự học và không biết cách triển khai các ứng dụng của khoa học dữ liệu ở mức độ cao vào công việc thực tế. Nhóm hai, có nền tảng vững vàng về kiến thức thống kê, dự báo nhưng lại quá chú trọng vào các yếu tố kỹ thuật. Ở nhóm này, các bạn đều có nền tảng kiến thức về toán, thống kê rất tốt. Một số bạn được học và đào tạo cơ bản về khoa học dữ liệu, học máy và các ứng dụng của khoa học dữ liệu. Các bạn này có thiên hướng thích xây dựng mô hình dự báo, thích làm các bài toán lớn trong kinh doanh. Điểm mạnh của nhóm này là rất thông minh, chịu khó học hỏi và có thể áp dụng những kỹ thuật phân tích mới vào thực tế một cách nhanh chóng. Nhưng ngược lại, nhóm này lại có nhược điểm chết người là có thói quen chỉ tập trung vào việc phân tích dữ liệu mà thiếu đi cái nhìn tổng quát trong việc giải quyết bài toán thực tế. Không chỉ thế, nhóm này không có thế mạnh trong việc trình bày và giao tiếp, dẫn đến các kết quả thực tế không được các đơn vị kinh doanh nghiệp vụ đón nhận và sử dụng. Đối với một tổ chức muốn phát triển dựa vào dữ liệu và muốn biến các quyết định của tổ chức dựa vào phân tích dữ liệu, cả hai nhóm trên đều là các trạng thái nên tránh và phải cân bằng được cả hai. Cuốn sách này sẽ phân tích và giúp các bạn làm trong lĩnh vực phân tích dữ liệu hiểu rõ hơn các ưu nhược điểm của chính mình. Tại sao nên đọc cuốn sách này Việc viết cuốn sách này xuất phát thuần túy từ nhu cầu cá nhân của tác giả. Trong quá trình làm việc thực tế, bản thân tác giả luôn có mong muốn tổng hợp và đúc rút các kiến thức phân tích thực tế. Tuy nhiên, khi bắt đầu bắt tay vào xây dựng cuốn sách, tác giả cũng có mong muốn có thể đúc kết và giúp cho các tổ chức, bộ phận muốn tập trung vào và khai thác sức mạnh của phân tích dữ liệu trong thực tế có thể có thêm các nguồn tài liệu và kinh nghiệm, vốn rất ít được chia sẻ thực tế, để có thể thành công hơn trong công việc triển khai. Thêm vào đó, trong quá trình làm việc và giảng dạy cho các học viên, tác giả luôn nhận được câu hỏi: Thưa thày, em không được đào tạo bài bản về phân tích thống kê cũng như khoa học dữ liệu, liệu em có thể trở thành chuyên gia phân tích dữ liệu được không? Tác giả luôn trăn trở với câu hỏi trên cũng như với kinh nghiệm đào tạo thực tế và xây dựng năng lực phân tích dữ liệu ở VPBank, quyển sách này được viết ra theo cách tiếp cận ứng dụng của khoa học dữ liệu trong hoạt động kinh doanh. Do đó, ngôn ngữ cũng như cách tiếp cận trong cuốn sách này được cố gắng viết một cách đơn giản, dễ hiểu để trình bày các kiến thức, thuật ngữ khó hiểu của khoa học dữ liệu thành các ngôn ngữ bình dân phù hợp với nhiều đối tượng. Cuốn sách này sẽ không đi sâu vào lý thuyết của các thuật toán, mô hình thống kê mà sẽ cố gắng trả lời các khía cạnh sau. Thuật toán, mô hình đó là gì? Mô hình đó được sử dụng như thế nào? Khi giải thích cho các đơn vị kinh doanh, ta cần phải giải thích điều gì? Ứng dụng của mô hình trong thực tế Cấu trúc của sách Cuốn sách được chia làm 3 phần theo thứ tự từ dễ đến khó, bao gồm. Phần một, các kiến thức cơ bản Phần hai, các ứng dụng nâng cao Phần ba, các vấn đề khác. Chapters ?? introduces a new topic, and … Các phần mềm được sử dụng I used the knitr package (Xie 2015) and the bookdown package (Xie 2018) to compile my book. My R session information is shown below: xfun::session_info() ## R version 3.4.0 (2017-04-21) ## Platform: x86_64-w64-mingw32/x64 (64-bit) ## Running under: Windows 10 x64 (build 14393) ## ## Locale: ## LC_COLLATE=English_United States.1252 ## LC_CTYPE=English_United States.1252 ## LC_MONETARY=English_United States.1252 ## LC_NUMERIC=C ## LC_TIME=English_United States.1252 ## ## Package version: ## assertthat_0.2.0 backports_1.1.2 ## base64enc_0.1.3 BH_1.66.0.1 ## bindr_0.1.1 bindrcpp_0.2.2 ## bookdown_0.7 cli_1.0.0 ## colorspace_1.3-2 compiler_3.4.0 ## crayon_1.3.4 dichromat_2.0.0 ## digest_0.6.15 dplyr_0.7.4 ## evaluate_0.10.1 ggplot2_3.0.0 ## ggthemes_3.4.2 glue_1.3.0 ## graphics_3.4.0 grDevices_3.4.0 ## grid_3.4.0 gtable_0.2.0 ## highr_0.6 htmltools_0.3.6 ## jsonlite_1.5 knitr_1.20 ## labeling_0.3 lattice_0.20.35 ## lazyeval_0.2.1 magrittr_1.5 ## markdown_0.7.7 MASS_7.3.47 ## Matrix_1.2.9 methods_3.4.0 ## mgcv_1.8.17 mime_0.5 ## munsell_0.4.3 nlme_3.1.131 ## pillar_1.2.2 pkgconfig_2.0.1 ## plogr_0.2.0 plyr_1.8.4 ## R6_2.2.2 RColorBrewer_1.1.2 ## Rcpp_0.12.16 reshape2_1.4.3 ## rlang_0.2.0 rmarkdown_1.10 ## rprojroot_1.3-2 rstudioapi_0.7 ## scales_0.5.0 stats_3.4.0 ## stringi_1.1.7 stringr_1.3.0 ## tibble_1.4.2 tinytex_0.6 ## tools_3.4.0 utf8_1.1.3 ## utils_3.4.0 viridisLite_0.3.0 ## withr_2.1.2 xfun_0.3 ## yaml_2.1.18 Package names are in bold text (e.g., rmarkdown), and inline code and filenames are formatted in a typewriter font (e.g., knitr::knit('foo.Rmd')). Function names are followed by parentheses (e.g., bookdown::render_book()). Lời cảm ơn Cảm ơn những người sau đã giúp chúng tôi hoàn thành quyển sách Frida Gomam on the Mars Tài liệu tham khảo "],
["v-tac-gia.html", "Về tác giả", " Về tác giả Tác giả Hoàng Đức Anh hiên đang là trưởng phòng phân tích nâng cao của ngân hàng VPBank. Đức Anh được đào tạo về phân tích dữ liệu tại trường đại học kinh tế Vác-sa-va (Warsaw School of Economics), Vác-sa-va, cộng hòa Ba Lan ở cả hai cấp, đại học và thạc sỹ từ năm 2009-2014. "],
["intro.html", "Chương 1 Khoa học dữ liệu và nghề phân tích dữ liệu 1.1 Khoa học dữ liệu 1.2 Tại sao phân tích dữ liệu là nghề khó?", " Chương 1 Khoa học dữ liệu và nghề phân tích dữ liệu 1.1 Khoa học dữ liệu Định lý 1.1 (Pytago) Trong một tam giác vuông, bình phương cạnh huyền bằng tổng bình phương hai cạnh góc vuông \\[a^2 + b^2 = c^2\\] 1.2 Tại sao phân tích dữ liệu là nghề khó? Phân tích dữ liệu đòi hỏi cùng lúc thực hiện ba nhóm công việc sau: Hiểu biết về vấn đề kinh doanh. Nghe có vẻ đơn giản nhưng qua quá trình làm việc thực tế, kinh nghiệm của tác giả cho thấy đây có lẽ là phần dễ bị bỏ qua nhất bỏi lẽ mấy nguyên nhân sau. Sự khác biệt của vận hành kinh doanh so với hoạt động phân tích dữ liệu. Tư duy và thái độ của nhóm phân tích dữ liệu. Các bạn phân tích kinh doanh (hoặc đôi khi được gọi là phân tích kinh doanh) thường tự coi mình là đơn vị hỗ trợ và cung cấp dữ liệu theo yêu cầu. Với lối tư duy thụ động này, các bạn sẽ không có nhu cầu tìm hiểu cặn kẽ các hoạt động kinh doanh, dẫn đến không hiểu hoạt động kinh doanh đủ sâu để có thể tư vấn và thuyết phục các bên kinh doanh trong việc triển khai các dự án phân tích dữ liệu mới. Nghiệp vụ kinh doanh rất phức tạp và thiếu hệ thống tài liệu ghi chép dưới góc độ khái quát cho hoạt động phân tích dữ liệu. Đây là vấn đề phần lớn các nhóm phân tích dữ liệu gặp phải. Để giải quyết vấn đề này, trong phần sau tác giả sẽ đưa ra phương pháp tìm hiểu hoạt động kinh doanh theo 6 nhóm vấn đề. Khai thác và phân tích dữ liệu Trình bày, thuyết phục và tư vấn cho các bên kinh doanh về kết quả phân tích dữ liệu. Ba cấp độ của viết code: Khi sử dụng các công cụ phân tích dữ liệu (viết code), ta sẽ trải qua 3 cấp độ như sau: Viết thứ đơn giản. Ở cấp độ này, các bạn thường mới nhập môn phân tích dữ liệu, đầy lo lắng và thiếu tự tin ở bản thân và bắt đầu với những bài phân tích đơn giản để áp dụng các kiến thức mới học. Viết càng nguy hiểm càng tốt. Ở giai đoạn này, các bạn đã có một lượng kiến thức nền tương đối vững và bắt đầu đi vào các phương pháp nâng cao. Do đó, các bạn thường có xu hướng khiến mọi thứ trở nên nguy hiểm, tô vẽ và đưa ra nhiều yếu tố không thực sự cần thiết. Với các bạn có xu hướng trực quan hóa, sẽ là đưa ra các biểu đồ đầy màu sắc và cực kỳ nguy hiểm. Với các bạn có xu hướng xây dựng mô hình dự báo, sẽ là dùng mô hình dự báo, học máy hoặc deep-learning mọi lúc, mọi nơi. Kết quả sẽ khiến người đọc ấn tượng nhưng có thể chưa thực sự có tính ứng dụng cao. Chỉ viết những thứ có khả năng tái sử dụng, giải quyết vấn đề bằng phương pháp đơn giản nhất có thể có. Ở giai đoạn này, các bạn đã dung hòa được rất nhiều kiến thức của nghành phân tích dữ liệu với nhau và tiếp cận các vấn đề một cách mạch lạc, logic và rất chặt chẽ. Các bạn nắm rất vững khi nào nên dùng các phương pháp phân tích khám phá dữ liệu đơn giản thay cho các thuật toán phức tạp trong việc giải quyết các vấn đề kinh doanh. Định lý 1.2 (Pytago) Trong một tam giác vuông, bình phương cạnh huyền bằng tổng bình phương hai cạnh góc vuông \\[a^2 + b^2 = c^2\\] Figures and tables with captions will be placed in figure and table environments, respectively. library(tidyverse) iris %&gt;% ggplot(aes(Species, Sepal.Length)) + geom_boxplot(aes(fill = Species)) + theme_minimal() Hình 1.1: Here is a nice figure! Reference a figure by its code chunk label with the fig: prefix, e.g., see Figure 1.1. Similarly, you can reference tables generated from knitr::kable(), e.g., see Table ??. knitr::kable( head(iris, 20), caption = &#39;Here is a nice table!&#39;, booktabs = TRUE ) Bảng 1.1: Here is a nice table! Sepal.Length Sepal.Width Petal.Length Petal.Width Species 5.1 3.5 1.4 0.2 setosa 4.9 3.0 1.4 0.2 setosa 4.7 3.2 1.3 0.2 setosa 4.6 3.1 1.5 0.2 setosa 5.0 3.6 1.4 0.2 setosa 5.4 3.9 1.7 0.4 setosa 4.6 3.4 1.4 0.3 setosa 5.0 3.4 1.5 0.2 setosa 4.4 2.9 1.4 0.2 setosa 4.9 3.1 1.5 0.1 setosa 5.4 3.7 1.5 0.2 setosa 4.8 3.4 1.6 0.2 setosa 4.8 3.0 1.4 0.1 setosa 4.3 3.0 1.1 0.1 setosa 5.8 4.0 1.2 0.2 setosa 5.7 4.4 1.5 0.4 setosa 5.4 3.9 1.3 0.4 setosa 5.1 3.5 1.4 0.3 setosa 5.7 3.8 1.7 0.3 setosa 5.1 3.8 1.5 0.3 setosa You can write citations, too. For example, we are using the bookdown package (Xie 2018) in this sample book, which was built on top of R Markdown and knitr (Xie 2015). Tài liệu tham khảo "],
["cac-phuong-phap-phan-tich-d-liu-thc-t-ng-dung-vi-r.html", "Chương 2 Các phương pháp phân tích dữ liệu thực tế ứng dụng với R 2.1 Phần 1 2.2 Phần 2 2.3 Phần 3", " Chương 2 Các phương pháp phân tích dữ liệu thực tế ứng dụng với R We talk about the FOO method in this chapter. 2.1 Phần 1 2.2 Phần 2 2.3 Phần 3 "],
["ng-phap-cua-bin-i-d-liu-vi-dplyr.html", "Chương 3 Ngữ pháp của biến đổi dữ liệu với DPLYR 3.1 Giới thiệu về pipe operator 3.2 Các hàm cơ bản trong dplyr 3.3 Các hàm nâng cao trong dplyr", " Chương 3 Ngữ pháp của biến đổi dữ liệu với DPLYR Khi bắt tay vào công việc phân tích số liệu, việc đầu tiên ta cần phải làm là thu thập dữ liệu từ nhiều nguồn khác nhau. Sau khi hoàn thành xong bước này, ta sẽ phải dành phần lớn thời gian để làm sạch, biến đổi và tổng hợp dữ liệu nhằm tìm kiếm các insights hoặc chuẩn bị dữ liệu cho các bước xây dựng mô hình, dự báo. R rất mạnh trong việc biến đối dữ liệu và có rất nhiều package hỗ trợ cho công việc này. Tuy nhiên, thư viện nổi tiếng nhất trong R trong việc làm sạch và biến đổi dữ liệu là dplyr, một thư viện nổi tiếng với những tính năng chuyên cho việc xử lý, tổng hợp dữ liệu trước khi xây dựng mô hình phân tích dữ liệu. Chương này sẽ tập trung vào giới thiệu về những hàm cơ bản nhất của dplyr. Trước khi bắt đầu nội dung bài giảng, chúng ta có thể download và gọi gói dplyr. #install.packages(&quot;dplyr&quot;) library(dplyr) 3.1 Giới thiệu về pipe operator Khi viết các câu lệnh, thông thường ta có 2 cách viết phổ biến sau. Cách 1: Viết với các câu lệnh lồng vào nhau (nested). Với cách viết này, các hàm sẽ được viết lồng vào nhau và kết quả của hàm sẽ được tính toán theo thứ tự từ trong ra ngoài. Cách 2: Viết lưu dưới dạng các đối tượng trung gian. Với cách viết này, từng đối tượng sẽ được tính toán từng phần và kết quả sẽ được hiển thị một cách mạch lạc hơn. Tuy nhiên, nhược điểm của phương pháp này là sẽ tạo ra rất nhiều đối tượng trung gian, gây ra khó khăn trong việc theo dõi và quản lỹ. Giả sử ta cần tính toán độ lệch chuẩn của véc-tơ x, công thức tính độ lệch chuẩn sẽ là \\[\\sigma = \\frac{\\sum_{i=1}^n(x_i-\\overline{x})^2}{n-1}\\] Với hai cách viết code khác nhau, ta có thể tính độ lệch chuẩn theo hai cách. # Cách 1 # Tạo vector x x &lt;- seq(2, 100, 2) # Tính độ lệch chuẩn sqrt(sum((x-mean(x))^2)/(length(x)-1)) ## [1] 29.15 sd(x) ## [1] 29.15 # Cách 2 # Tạo vector x x &lt;- seq(2, 100, 2) # Tính tổng bình phương sum_sqr &lt;- sum((x-mean(x))^2) len_x &lt;- length(x) var &lt;- sum_sqr/(len_x - 1) sd &lt;- var^(1/2) sd ## [1] 29.15 Ta thấy kết quả ở hai cách tính là như nhau. Tuy nhiên, cách viết hai sẽ tạo ra nhiều đối tượng trung gian hơn cách viết 1 rất nhiều. Khi phân tích dữ liệu thực tế, ta sẽ phải áp dụng cả 2 cách viết code để có thể vận dụng linh hoạt trong từng trường hợp cụ thể. Trong R, có cách viết code thứ ba, được gọi là cách suwr dụng pipe operator (%&gt;%). Toán tử Pipe cho phép viết code theo cách đơn giản và dễ theo dõi giúp cho người đọc và người viết code trên R có thể theo dõi được code một cách dễ dàng nhất. Câu trúc của pipe như sau f(x, y) = x %&gt;% f(., y) Ví dụ của pipe. # Cách 1 mean(x) # Cách 2 x %&gt;% mean Ta có thể xem xét ví dụ phức tạp hơn. # Cách 1 - dùng cách viết thường summary(head(iris)) ## Sepal.Length Sepal.Width Petal.Length ## Min. :4.60 Min. :3.00 Min. :1.30 ## 1st Qu.:4.75 1st Qu.:3.12 1st Qu.:1.40 ## Median :4.95 Median :3.35 Median :1.40 ## Mean :4.95 Mean :3.38 Mean :1.45 ## 3rd Qu.:5.08 3rd Qu.:3.58 3rd Qu.:1.48 ## Max. :5.40 Max. :3.90 Max. :1.70 ## Petal.Width Species ## Min. :0.200 setosa :6 ## 1st Qu.:0.200 versicolor:0 ## Median :0.200 virginica :0 ## Mean :0.233 ## 3rd Qu.:0.200 ## Max. :0.400 # Cách 2 - dùng pipe iris %&gt;% head %&gt;% summary ## Sepal.Length Sepal.Width Petal.Length ## Min. :4.60 Min. :3.00 Min. :1.30 ## 1st Qu.:4.75 1st Qu.:3.12 1st Qu.:1.40 ## Median :4.95 Median :3.35 Median :1.40 ## Mean :4.95 Mean :3.38 Mean :1.45 ## 3rd Qu.:5.08 3rd Qu.:3.58 3rd Qu.:1.48 ## Max. :5.40 Max. :3.90 Max. :1.70 ## Petal.Width Species ## Min. :0.200 setosa :6 ## 1st Qu.:0.200 versicolor:0 ## Median :0.200 virginica :0 ## Mean :0.233 ## 3rd Qu.:0.200 ## Max. :0.400 Cả hai cách đều cho ra kết quả giống nhau. Tuy nhiên, cách hai sẽ dễ theo dõi, dễ đọc hơn cách 1 rất nhiều. Cách đọc hiểu quá trình thực hiện pipe như sau: Gọi tập dữ liệu iris để phân tích Thực hiện hàm head trên tập dữ liệu này, được kết quả bao nhiêu… … tiếp tục thực hiện hàm summary Như ta thấy, cách viết theo phong cách của pipe operator (%&gt;%) cho phép ta thực hiện các phép tính theo đúng mạch tư duy logic của bản thân. Điều này là một điểm rất mạnh mà hiện tại, mới chỉ ở R có toán tử %&gt;% áp dụng được cho mọi hàm. Một số đặc tính cơ bản của %&gt;%: Theo mặc định, Phía tay trái (LHS) sẽ được chuyển tiếp thành yếu tố đầu tiên của hàm được sử dụng phía tay phải (RHS), ví dụ: mean(x) ## [1] 51 # Tương đương với: x %&gt;% mean ## [1] 51 Khi LHS không còn là yếu tố đầu tiên của một hàm RHS, thì dấu “.” được sử dụng để định vị cho LHS, ví dụ: library(dplyr) # Cách 1 summary(lm(mpg ~ cyl, data = mtcars)) # Cách 2 mtcars %&gt;% lm(mpg ~ cyl, data = .) %&gt;% summary Trong tình huống trên, tham số về dữ liệu trong hàm lm không phải là ở đầu, mà sau phần công thức, nên chúng ta sẽ dùng dấu “.” như là đại diện của thực thể mtcars ở bên ngoài (LHS) của hàm lm. 3.2 Các hàm cơ bản trong dplyr Trong công việc biến đổi dữ liệu, bất kỳ ngôn ngữ phân tích nào cũng có 3 nhóm hàm lớn. Nhóm 1 - các hàm truy vấn dữ liệu: Lấy dữ liệu theo dòng, theo cột và theo điều kiện. Trong dplyr sẽ là các hàm select, filter và slice Nhóm 2 - Các hàm tổng hợp dữ liệu: Tính toán tổng hợp dữ liệu theo chiều. Trong dplyr sẽ là các hàm group_by, summarise Nhóm 3 - Các hàm biến đổi dữ liệu: Tạo mới, biến đổi các dữ liệu cũ thành các dữ liệu mới. Trong dplyr sẽ là các hàm thuộc nhóm mutate, join, bind Trong phần này, chúng ta sẽ giới thiệu nhanh các nhóm câu lệnh cơ bản trên. 3.2.1 Nhóm câu lệnh truy vấn dữ liệu Khi truy vấn dữ liệu, ta thường phải thực hiện 3 nhóm công việc sau. Lấy theo cột Lấy theo dòng Lấy theo điều kiện Xét về mặt bản chất, lấy theo điều kiện là một trường hợp đặc biệt của việc lấy theo dòng. Đối với các ngôn ngữ như SQL, sẽ không phân biệt hai loại này. Tuy nhiên, vì R lưu thứ tự của từng quan sát trong dataframe, nên việc phân biệt được hai loại truy vấn trên là cần thiết. 3.2.1.1 Lấy các cột trong dataframe với select data %&gt;% select(var1, var2, …) Trong đó, var1, var2 là tên các cột cần truy vấn. Đặc biệt, R rất linh hoạt trong việc lọc theo cột. Ta có thể truy vấn theo tên, theo thứ tự hoặc thậm chí theo các khoảng thứ tự các biến. Xem ví dụ sau. library(dplyr) # Xêm tên các biến trong mtcars mtcars %&gt;% names ## [1] &quot;mpg&quot; &quot;cyl&quot; &quot;disp&quot; &quot;hp&quot; &quot;drat&quot; &quot;wt&quot; &quot;qsec&quot; ## [8] &quot;vs&quot; &quot;am&quot; &quot;gear&quot; &quot;carb&quot; # Chọn cột mpg và cyl mtcars %&gt;% select(mpg, cyl) %&gt;% head ## mpg cyl ## Mazda RX4 21.0 6 ## Mazda RX4 Wag 21.0 6 ## Datsun 710 22.8 4 ## Hornet 4 Drive 21.4 6 ## Hornet Sportabout 18.7 8 ## Valiant 18.1 6 # Chọn cột thứ nhất và thứ hai mtcars %&gt;% select(1,2) %&gt;% head ## mpg cyl ## Mazda RX4 21.0 6 ## Mazda RX4 Wag 21.0 6 ## Datsun 710 22.8 4 ## Hornet 4 Drive 21.4 6 ## Hornet Sportabout 18.7 8 ## Valiant 18.1 6 # Chọn cột thứ 3 đến cột thứ 6 mtcars %&gt;% select(3:6) %&gt;% head ## disp hp drat wt ## Mazda RX4 160 110 3.90 2.620 ## Mazda RX4 Wag 160 110 3.90 2.875 ## Datsun 710 108 93 3.85 2.320 ## Hornet 4 Drive 258 110 3.08 3.215 ## Hornet Sportabout 360 175 3.15 3.440 ## Valiant 225 105 2.76 3.460 Ngoài ra, khi lấy chi tiết các cột (liệt kê từng cột) khi lấy dữ liệu trên 1 bảng, bạn có thể dùng một số hàm sau để hỗ trợ việc lấy trường dữ liệu được nhanh hơn: starts_with(&quot;Ký tự là thông tin mong muốn&quot;): các cột dữ liệu ccó tên hứa các ký tự mong muốn đứng ở đầu của tên, ví dụ: iris %&gt;% select(starts_with(&quot;Petal&quot;)) %&gt;% head ## Petal.Length Petal.Width ## 1 1.4 0.2 ## 2 1.4 0.2 ## 3 1.3 0.2 ## 4 1.5 0.2 ## 5 1.4 0.2 ## 6 1.7 0.4 ends_with(&quot;Ký tự là thông tin mong muốn&quot;): các cột dữ liệu có tên chứa các ký tự mong muốn ở cuối của tên, ví dụ: iris %&gt;% select(ends_with(&quot;Length&quot;)) %&gt;% head ## Sepal.Length Petal.Length ## 1 5.1 1.4 ## 2 4.9 1.4 ## 3 4.7 1.3 ## 4 4.6 1.5 ## 5 5.0 1.4 ## 6 5.4 1.7 contains(&quot;Ký tự là thông tin mong muốn&quot;): các cột dữ liệu có tên chứa chính xác các ký tự mong muốn ở bất kỳ vị trí nào của tên, ví dụ: iris %&gt;% select(contains(&quot;etal&quot;)) %&gt;% head ## Petal.Length Petal.Width ## 1 1.4 0.2 ## 2 1.4 0.2 ## 3 1.3 0.2 ## 4 1.5 0.2 ## 5 1.4 0.2 ## 6 1.7 0.4 matches(“Dạng ký tự là thông tin mong muốn”): các cột dữ liệu có tên chứa các ký tự có dạng ký tự mong muốn ở bất kỳ vị trí nào của tên, ví dụ: iris %&gt;% select(matches(&quot;.t.&quot;)) %&gt;% head ## Sepal.Length Sepal.Width Petal.Length Petal.Width ## 1 5.1 3.5 1.4 0.2 ## 2 4.9 3.0 1.4 0.2 ## 3 4.7 3.2 1.3 0.2 ## 4 4.6 3.1 1.5 0.2 ## 5 5.0 3.6 1.4 0.2 ## 6 5.4 3.9 1.7 0.4 Trong ví dụ trên, R sẽ lấy tất cả các cột có tên chứa chữ t và có ký tự khác ở trước và sau (các ký tự chỉ chứa chữ t mà chữ t ở đâu hoặc cuối tên sẽ không được tính vào) Thêm vào đó, ta có thể đổi tên biến ngay trong khi lựa chọn các biến với select như sau: mtcars %&gt;% select(`miles per gallon` = mpg , cylinder = cyl , weight = wt) %&gt;% head ## miles per gallon cylinder weight ## Mazda RX4 21.0 6 2.620 ## Mazda RX4 Wag 21.0 6 2.875 ## Datsun 710 22.8 4 2.320 ## Hornet 4 Drive 21.4 6 3.215 ## Hornet Sportabout 18.7 8 3.440 ## Valiant 18.1 6 3.460 3.2.1.2 Lấy các dòng trong dataframe với slice data %&gt;% slice(observation) Tương tự như lấy theo cột, ta có thể lấy các dòng trong một dataframe. Tuy nhiên, lưu ý hàm slice chỉ cho phép điều kiện lấy quan sát là một véc-tơ. Xem ví dụ sau. # Lấy dòng đầu tiên mtcars %&gt;% slice(1) ## # A tibble: 1 x 11 ## mpg cyl disp hp drat wt qsec vs ## &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 21 6 160 110 3.9 2.62 16.5 0 ## # ... with 3 more variables: am &lt;dbl&gt;, gear &lt;dbl&gt;, ## # carb &lt;dbl&gt; # Lấy dòng từ 1:3 mtcars %&gt;% slice(1:3) ## # A tibble: 3 x 11 ## mpg cyl disp hp drat wt qsec vs ## &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 21 6 160 110 3.9 2.62 16.5 0 ## 2 21 6 160 110 3.9 2.88 17.0 0 ## 3 22.8 4 108 93 3.85 2.32 18.6 1 ## # ... with 3 more variables: am &lt;dbl&gt;, gear &lt;dbl&gt;, ## # carb &lt;dbl&gt; # Lấy dòng 1:3 và 5 mtcars %&gt;% slice(c(1:3,5)) ## # A tibble: 4 x 11 ## mpg cyl disp hp drat wt qsec vs ## &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 21 6 160 110 3.9 2.62 16.5 0 ## 2 21 6 160 110 3.9 2.88 17.0 0 ## 3 22.8 4 108 93 3.85 2.32 18.6 1 ## 4 18.7 8 360 175 3.15 3.44 17.0 0 ## # ... with 3 more variables: am &lt;dbl&gt;, gear &lt;dbl&gt;, ## # carb &lt;dbl&gt; 3.2.1.3 Lọc quan sát theo điều kiện với filter data %&gt;% filter(condition) Hàm filter cho phép ta sử dụng các điều kiện phức tạp để truy xuất dữ liệu từ dataframe. Các điều kiện thường dùng bao gồm. Dấu Ký hiệu Ví dụ Bằng == 7==8 Khác != 7!=8 Lớn hơn &gt; a &gt; b Lớn hơn hoặc bằng &gt;= a &gt;= b Nhỏ hơn &lt; a &lt; b Nhỏ hơn hoặc bằng &lt;= a &lt;= b Và &amp; a &gt; 7 &amp; b &lt; 9 Xem ví dụ sau. # Lọc điều kiện mpg &gt; 20 mtcars %&gt;% filter(mpg &gt; 20) %&gt;% dim ## [1] 14 11 # Lọc điều kiện mpg &gt;20 hoặc mpg &lt;18 mtcars %&gt;% filter(mpg &gt; 20 | mpg &lt; 18) %&gt;% dim ## [1] 27 11 # Lọc điều kiện mpg &gt;=20 và cyl = 6 mtcars %&gt;% filter(mpg &gt; 20 &amp; cyl == 6) ## mpg cyl disp hp drat wt qsec vs am gear carb ## 1 21.0 6 160 110 3.90 2.620 16.46 0 1 4 4 ## 2 21.0 6 160 110 3.90 2.875 17.02 0 1 4 4 ## 3 21.4 6 258 110 3.08 3.215 19.44 1 0 3 1 Lưu ý: Khi điều kiện hoặc là chuỗi các giá trị rời rạc áp dụng cho cùng một trường, chúng ta có thể làm ngắn gọn hơn với cấu trúc “%in%” thay vì cấu phải liệt kê tất cả các điều kiện đơn lẻ và ngăn cách nhau bởi dấu “|”: mtcars %&gt;% filter(carb == 4 | carb == 3 | carb == 1) ## mpg cyl disp hp drat wt qsec vs am gear carb ## 1 21.0 6 160.0 110 3.90 2.620 16.46 0 1 4 4 ## 2 21.0 6 160.0 110 3.90 2.875 17.02 0 1 4 4 ## 3 22.8 4 108.0 93 3.85 2.320 18.61 1 1 4 1 ## 4 21.4 6 258.0 110 3.08 3.215 19.44 1 0 3 1 ## 5 18.1 6 225.0 105 2.76 3.460 20.22 1 0 3 1 ## 6 14.3 8 360.0 245 3.21 3.570 15.84 0 0 3 4 ## 7 19.2 6 167.6 123 3.92 3.440 18.30 1 0 4 4 ## 8 17.8 6 167.6 123 3.92 3.440 18.90 1 0 4 4 ## 9 16.4 8 275.8 180 3.07 4.070 17.40 0 0 3 3 ## 10 17.3 8 275.8 180 3.07 3.730 17.60 0 0 3 3 ## 11 15.2 8 275.8 180 3.07 3.780 18.00 0 0 3 3 ## 12 10.4 8 472.0 205 2.93 5.250 17.98 0 0 3 4 ## 13 10.4 8 460.0 215 3.00 5.424 17.82 0 0 3 4 ## 14 14.7 8 440.0 230 3.23 5.345 17.42 0 0 3 4 ## 15 32.4 4 78.7 66 4.08 2.200 19.47 1 1 4 1 ## 16 33.9 4 71.1 65 4.22 1.835 19.90 1 1 4 1 ## 17 21.5 4 120.1 97 3.70 2.465 20.01 1 0 3 1 ## 18 13.3 8 350.0 245 3.73 3.840 15.41 0 0 3 4 ## 19 27.3 4 79.0 66 4.08 1.935 18.90 1 1 4 1 ## 20 15.8 8 351.0 264 4.22 3.170 14.50 0 1 5 4 Câu lệnh trên tương đương với: mtcars %&gt;% filter(carb %in% c(1, 3, 4)) 3.2.1.4 Sắp xếp dữ liệu với arrange data %&gt;% arrange(var1, var2) Ngoài việc lọc dữ liệu có điều kiện, chúng ta cũng thường xuyên thực hiện việc sắp xếp dữ liệu theo một trật tự nhất định nào đó khi xem dữ liệu. Hàm arrange() hỗ trợ công việc này. Cách thức sắp xếp dữ liệu mặc định là từ nhỏ đến lớn. mtcars %&gt;% arrange(mpg) %&gt;% head ## mpg cyl disp hp drat wt qsec vs am gear carb ## 1 10.4 8 472 205 2.93 5.250 17.98 0 0 3 4 ## 2 10.4 8 460 215 3.00 5.424 17.82 0 0 3 4 ## 3 13.3 8 350 245 3.73 3.840 15.41 0 0 3 4 ## 4 14.3 8 360 245 3.21 3.570 15.84 0 0 3 4 ## 5 14.7 8 440 230 3.23 5.345 17.42 0 0 3 4 ## 6 15.0 8 301 335 3.54 3.570 14.60 0 1 5 8 Khi có nhiều biến cần được sắp xếp, hàm arrange sẽ ưu tiên các biến theo thứ tự từ trái sang phải. mtcars %&gt;% arrange(mpg, cyl) ## mpg cyl disp hp drat wt qsec vs am gear carb ## 1 10.4 8 472.0 205 2.93 5.250 17.98 0 0 3 4 ## 2 10.4 8 460.0 215 3.00 5.424 17.82 0 0 3 4 ## 3 13.3 8 350.0 245 3.73 3.840 15.41 0 0 3 4 ## 4 14.3 8 360.0 245 3.21 3.570 15.84 0 0 3 4 ## 5 14.7 8 440.0 230 3.23 5.345 17.42 0 0 3 4 ## 6 15.0 8 301.0 335 3.54 3.570 14.60 0 1 5 8 ## 7 15.2 8 275.8 180 3.07 3.780 18.00 0 0 3 3 ## 8 15.2 8 304.0 150 3.15 3.435 17.30 0 0 3 2 ## 9 15.5 8 318.0 150 2.76 3.520 16.87 0 0 3 2 ## 10 15.8 8 351.0 264 4.22 3.170 14.50 0 1 5 4 ## 11 16.4 8 275.8 180 3.07 4.070 17.40 0 0 3 3 ## 12 17.3 8 275.8 180 3.07 3.730 17.60 0 0 3 3 ## 13 17.8 6 167.6 123 3.92 3.440 18.90 1 0 4 4 ## 14 18.1 6 225.0 105 2.76 3.460 20.22 1 0 3 1 ## 15 18.7 8 360.0 175 3.15 3.440 17.02 0 0 3 2 ## 16 19.2 6 167.6 123 3.92 3.440 18.30 1 0 4 4 ## 17 19.2 8 400.0 175 3.08 3.845 17.05 0 0 3 2 ## 18 19.7 6 145.0 175 3.62 2.770 15.50 0 1 5 6 ## 19 21.0 6 160.0 110 3.90 2.620 16.46 0 1 4 4 ## 20 21.0 6 160.0 110 3.90 2.875 17.02 0 1 4 4 ## 21 21.4 4 121.0 109 4.11 2.780 18.60 1 1 4 2 ## 22 21.4 6 258.0 110 3.08 3.215 19.44 1 0 3 1 ## 23 21.5 4 120.1 97 3.70 2.465 20.01 1 0 3 1 ## 24 22.8 4 108.0 93 3.85 2.320 18.61 1 1 4 1 ## 25 22.8 4 140.8 95 3.92 3.150 22.90 1 0 4 2 ## 26 24.4 4 146.7 62 3.69 3.190 20.00 1 0 4 2 ## 27 26.0 4 120.3 91 4.43 2.140 16.70 0 1 5 2 ## 28 27.3 4 79.0 66 4.08 1.935 18.90 1 1 4 1 ## 29 30.4 4 75.7 52 4.93 1.615 18.52 1 1 4 2 ## 30 30.4 4 95.1 113 3.77 1.513 16.90 1 1 5 2 ## 31 32.4 4 78.7 66 4.08 2.200 19.47 1 1 4 1 ## 32 33.9 4 71.1 65 4.22 1.835 19.90 1 1 4 1 3.2.1.5 Đổi tên biến với rename data %&gt;% rename(new_var = old_var) mtcars %&gt;% rename(displacement = disp, miles_per_gallon = mpg) %&gt;% names ## [1] &quot;miles_per_gallon&quot; &quot;cyl&quot; ## [3] &quot;displacement&quot; &quot;hp&quot; ## [5] &quot;drat&quot; &quot;wt&quot; ## [7] &quot;qsec&quot; &quot;vs&quot; ## [9] &quot;am&quot; &quot;gear&quot; ## [11] &quot;carb&quot; 3.2.2 Nhóm câu lệnh biến đổi dữ liệu 3.2.2.0.1 Tạo mới trường dữ liệu với mutate Trong quá trình xử lý dữ liệu, ta thường xuyên phải tạo thêm các trường dữ liệu mới (trường dữ liệu phát sinh). Hàm mutate() được sử dụng để làm công việc này. Cấu trúc của hàm rất đơn giản như sau. data %&gt;% mutate(new_var = statement) Xem ví dụ sau mtcars %&gt;% select(mpg) %&gt;% mutate(new_mpg = mpg * 2) %&gt;% head ## mpg new_mpg ## 1 21.0 42.0 ## 2 21.0 42.0 ## 3 22.8 45.6 ## 4 21.4 42.8 ## 5 18.7 37.4 ## 6 18.1 36.2 Trong một số trường hợp, khi ta không muốn lấy các trường thông tin cũ mà chỉ muốn lấy các trường thông tin mới tạo thì có thể sử dụng hàm transmute() với cấu trúc giống như hàm mutate. mtcars %&gt;% select(mpg) %&gt;% transmute(new_mpg = mpg * 1.61) %&gt;% head ## new_mpg ## 1 33.81 ## 2 33.81 ## 3 36.71 ## 4 34.45 ## 5 30.11 ## 6 29.14 3.2.2.0.2 Gộp nhiều bảng với nhóm hàm join Hàm inner_join(x, y, by = &quot;key&quot;): lấy tất cả dữ liệu có trên bảng hai bảng khi trùng key, ví dụ: x &lt;- data.frame(student_id = seq(1, 10, 1), maths = c(10, 8, 7, 6, 7.8, 4, 7.7, 9, 9.5, 6.5)) y &lt;- data.frame(student_id = seq(2, 20, 2), physics = c(8, 9.5, 7.5, 6, 5.5, 6.5, 7.8, 8.2, 8, 7.5)) x ## student_id maths ## 1 1 10.0 ## 2 2 8.0 ## 3 3 7.0 ## 4 4 6.0 ## 5 5 7.8 ## 6 6 4.0 ## 7 7 7.7 ## 8 8 9.0 ## 9 9 9.5 ## 10 10 6.5 y ## student_id physics ## 1 2 8.0 ## 2 4 9.5 ## 3 6 7.5 ## 4 8 6.0 ## 5 10 5.5 ## 6 12 6.5 ## 7 14 7.8 ## 8 16 8.2 ## 9 18 8.0 ## 10 20 7.5 # gộp 2 bảng dữ liệu x và y theo student_id x %&gt;% inner_join(y, by = &quot;student_id&quot;) ## student_id maths physics ## 1 2 8.0 8.0 ## 2 4 6.0 9.5 ## 3 6 4.0 7.5 ## 4 8 9.0 6.0 ## 5 10 6.5 5.5 full_join: lấy tất cả dữ liệu có cả trên bảng x, y. full_join(x, y, by = “key”) x %&gt;% full_join(y, by = &quot;student_id&quot;) ## student_id maths physics ## 1 1 10.0 NA ## 2 2 8.0 8.0 ## 3 3 7.0 NA ## 4 4 6.0 9.5 ## 5 5 7.8 NA ## 6 6 4.0 7.5 ## 7 7 7.7 NA ## 8 8 9.0 6.0 ## 9 9 9.5 NA ## 10 10 6.5 5.5 ## 11 12 NA 6.5 ## 12 14 NA 7.8 ## 13 16 NA 8.2 ## 14 18 NA 8.0 ## 15 20 NA 7.5 Trong ví dụ trên, các giá trị về điểm toán (maths) sẽ trả về NA cho các student_id không tồn tại trên bảng y và ngược lại cho bảng x với các giá trị điểm vật lý (physics) của các student_id không tồn tại trên bảng x. Hàm left_join: lấy dữ liệu chỉ có trên bảng x, ví dụ: left_join(x, y, by = “var”) x %&gt;% left_join(y, by = &quot;student_id&quot;) ## student_id maths physics ## 1 1 10.0 NA ## 2 2 8.0 8.0 ## 3 3 7.0 NA ## 4 4 6.0 9.5 ## 5 5 7.8 NA ## 6 6 4.0 7.5 ## 7 7 7.7 NA ## 8 8 9.0 6.0 ## 9 9 9.5 NA ## 10 10 6.5 5.5 Với các student_id không có giá trị trên bảng y, cột physics sẽ trả về giá trị NA Hàm right_join : lấy dữ liệu chỉ có trên bảng y, ví dụ: right_join(x, y, by = “var”) x %&gt;% right_join(y, by = &quot;student_id&quot;) ## student_id maths physics ## 1 2 8.0 8.0 ## 2 4 6.0 9.5 ## 3 6 4.0 7.5 ## 4 8 9.0 6.0 ## 5 10 6.5 5.5 ## 6 12 NA 6.5 ## 7 14 NA 7.8 ## 8 16 NA 8.2 ## 9 18 NA 8.0 ## 10 20 NA 7.5 Với các student_id không có giá trị trên bảng x, cột maths sẽ trả về giá trị NA Lưu ý: Trong trường hợp cột dữ liệu dùng để nối các bảng có tên khác nhau, ta có thể sử dụng cấu trúc sau: left_join(x, y, by = c(“key_x” = “key_y”)) Xem ví dụ sau: names(x)[1] &lt;- &quot;student_id1&quot; names(y)[1] &lt;- &quot;student_id2&quot; x %&gt;% inner_join(y, by = c(&quot;student_id1&quot; = &quot;student_id2&quot;)) ## student_id1 maths physics ## 1 2 8.0 8.0 ## 2 4 6.0 9.5 ## 3 6 4.0 7.5 ## 4 8 9.0 6.0 ## 5 10 6.5 5.5 3.2.2.0.3 Ghép nhiều bảng theo dòng hoặc cột với nhóm hàm bind Bên cạnh các hàm join, khi xử lý dữ liệu trong thực tiễn, ta có thể phải ghép các bảng dữ liệu theo hàng hoặc cột. Trong dplyr, có hai hàm rất hữu dụng trong hai trường hợp trên là bind_col và bind_rows bind_cols(data1, data2) bind_rows(data1, data2) Xem hai ví dụ sau. df1 &lt;- data.frame(id = 1:3, income = 8:10) df2 &lt;- data.frame(id = 4:9, income = 8:13) df3 &lt;- data.frame(id = 1:3, gender = c(&quot;F&quot;, &quot;F&quot;, &quot;M&quot;)) # Nối theo dòng df1 %&gt;% bind_rows(df2) ## id income ## 1 1 8 ## 2 2 9 ## 3 3 10 ## 4 4 8 ## 5 5 9 ## 6 6 10 ## 7 7 11 ## 8 8 12 ## 9 9 13 # Nối theo cột df1 %&gt;% bind_cols(df3) ## id income id1 gender ## 1 1 8 1 F ## 2 2 9 2 F ## 3 3 10 3 M 3.2.3 Nhóm hàm tổng hợp dữ liệu với summarise Trong quá trình xử lý dữ liệu, ta thường xuyên phải tổng hợp dữ liệu theo các cách như: tính tổng, tính số dư bình quân, phương sai, tổng số lượng quan sát… Với dplyr, ta có thể sử dụng hàm summarise() để thực hiện công việc này. data %&gt;% summarise(var_name = calculate_stats(var)) mtcars %&gt;% summarise(mean_mpg = mean(mpg), sd_mpg = sd(mpg)) ## mean_mpg sd_mpg ## 1 20.09 6.027 Đây là ví dụ đơn giản nhất với summarise mà ta có thể thay thế bằng summary() trên R base. Tuy nhiên, kết hợp giữa hàm summarise() và hàm group_by() trên dplyr sẽ cho chúng ta có cái nhìn về dữ liệu tổng hợp một cách đa chiều hơn. Hàm group_by() cho phép dữ liệu tổng hợp được gộp lại theo một hoặc nhiều trường thông tin khác nhau, giúp người phân tích có thể nhìn dữ liệu theo từ chiều riêng biệt hoặc gộp các chiều thông tin với nhau. mtcars %&gt;% group_by(cyl) %&gt;% summarise(mean_mpg = mean(mpg), mean_disp = mean(disp)) ## # A tibble: 3 x 3 ## cyl mean_mpg mean_disp ## &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 4 26.7 105. ## 2 6 19.7 183. ## 3 8 15.1 353. 3.3 Các hàm nâng cao trong dplyr Bên cạnh các nhóm hàm cơ bản đã trình bày ở phần trên, dplyr còn có một số hàm nâng cao khác đặc biệt hữu dụng trong quá trình biến đổi, tổng hợp dữ liệu, bao gồm case_when, mutate_at &amp; summarise_at 3.3.1 Điều kiện phân nhóm với case_when Trong quá trình phân tích và xử lý dữ liệu, chúng ta thường phải tạo thêm các trường mới hoặc tính toán dữ liệu dựa vào từng điều kiện khác nhau để đưa ra giá trị của trường hoặc cách tính cho dữ liệu. Ví dụ, khi ta muốn tính thưởng cho KH thì sẽ phải dùng nhiều công thức khác nhau như KH thuộc VIP sẽ nhân 1 tỷ lệ, KH thuộc nhóm trung bình sẽ có 1 tỷ lệ khác, hay KH thông thường thì sẽ 1 tỷ lệ khác…. Trong dplyr, hàm case_when() xử lý các trường hợp trên rất nhanh chóng. data %&gt;% mutate(new_var = case_when( condition_1 ~ “value_1”, condition_2 ~ “value_2”,…, TRUE ~ “value_n” )) Ta xem ví dụ sau: df &lt;- data.frame(number = 1:10) df %&gt;% mutate(nhom = case_when( number &lt;= 5 ~ &quot;nhom_1&quot;, # nhóm 1: số từ 1 đến 5 number &gt; 5 &amp; number &lt;= 8 ~ &quot;nhom_2&quot;, # nhóm 2: số từ 6 đến 8 TRUE ~ &quot;nhom_3&quot; # các số còn lại )) ## number nhom ## 1 1 nhom_1 ## 2 2 nhom_1 ## 3 3 nhom_1 ## 4 4 nhom_1 ## 5 5 nhom_1 ## 6 6 nhom_2 ## 7 7 nhom_2 ## 8 8 nhom_2 ## 9 9 nhom_3 ## 10 10 nhom_3 3.3.2 Tạo thêm biến mới theo điều kiện với mutate_if &amp; mutate_at Khi phân tích, ta có thể tạo thêm biến mới khi các biến trong dataframe thỏa mãn điều kiện nào đó. data %&gt;% mutate_if(condition, function) df &lt;- data.frame( id = 1:5, gender = c(&quot;F&quot;, &quot;M&quot;, &quot;M&quot;, &quot;F&quot;, &quot;F&quot;), income = c(4,5,3,6,7) ) df %&gt;% summary ## id gender income ## Min. :1 F:3 Min. :3 ## 1st Qu.:2 M:2 1st Qu.:4 ## Median :3 Median :5 ## Mean :3 Mean :5 ## 3rd Qu.:4 3rd Qu.:6 ## Max. :5 Max. :7 # Biến đổi các biến factor thành character df %&gt;% mutate_if(is.factor, as.character) %&gt;% summary ## id gender income ## Min. :1 Length:5 Min. :3 ## 1st Qu.:2 Class :character 1st Qu.:4 ## Median :3 Mode :character Median :5 ## Mean :3 Mean :5 ## 3rd Qu.:4 3rd Qu.:6 ## Max. :5 Max. :7 Ngoài ra, ta có thể tự tạo các hàm mới và áp dụng với mutate_if. Xem ví dụ sau. my_func &lt;- function(x){x*100} # Nhân các biến numeric lên 100 lần df %&gt;% mutate_if(is.numeric, my_func) ## id gender income ## 1 100 F 400 ## 2 200 M 500 ## 3 300 M 300 ## 4 400 F 600 ## 5 500 F 700 Đối với mutate_at, ta cũng có thể thực hiện tương tự. Cấu trúc tổng quát của mutate_at như sau. data %&gt;% mutate_at(vars(var1, var2, …), function) df ## id gender income ## 1 1 F 4 ## 2 2 M 5 ## 3 3 M 3 ## 4 4 F 6 ## 5 5 F 7 # Nhân biến income lên 100 lần df %&gt;% mutate_at(vars(income), my_func) ## id gender income ## 1 1 F 400 ## 2 2 M 500 ## 3 3 M 300 ## 4 4 F 600 ## 5 5 F 700 3.3.3 Tổng hợp dữ liệu theo điều kiện với summarise_at và summarise_if Tương tự như mutate_at và mutate_if, ta có thể tổng hợp nhanh dữ liệu theo điều kiện. Cấu trúc tổng quát của summarise_at data %&gt;% group_by(var) (không bắt buộc) summarise_at(vars(variables), funs(functions)) Xem ví dụ sau mtcars %&gt;% group_by(am) %&gt;% summarise_at(vars(mpg, disp), funs(mean, max, median)) ## # A tibble: 2 x 7 ## am mpg_mean disp_mean mpg_max disp_max mpg_median ## &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 0 17.1 290. 24.4 472 17.3 ## 2 1 24.4 144. 33.9 351 22.8 ## # ... with 1 more variable: disp_median &lt;dbl&gt; Tương tự, ta có cấu trúc tổng quát của summarise_if data %&gt;% group_by(var) (không bắt buộc) summarise_if(condition, funs(functions)) iris %&gt;% select(Species, Sepal.Length, Sepal.Width) %&gt;% group_by(Species) %&gt;% summarise_if(is.numeric, funs(mean, median)) ## # A tibble: 3 x 5 ## Species Sepal.Length_mean Sepal.Width_mean ## &lt;fct&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 setosa 5.01 3.43 ## 2 versicolor 5.94 2.77 ## 3 virginica 6.59 2.97 ## # ... with 2 more variables: ## # Sepal.Length_median &lt;dbl&gt;, ## # Sepal.Width_median &lt;dbl&gt; "],
["phan-ra-va-xoay-chiu-d-liu.html", "Chương 4 Phân rã và xoay chiều dữ liệu 4.1 Phân rã dữ liệu thành dạng dọc với gather 4.2 Xoay chiều dữ liệu với spread 4.3 Tách một biến thành nhiều biến với separate 4.4 Gộp nhiều biến thành một biến với unite", " Chương 4 Phân rã và xoay chiều dữ liệu Khi phân tích dữ liệu, dữ liệu sau khi được làm sạch thường cơ bản có hai dạng. Dạng ngang: Mỗi dòng ứng với 1 quan sát và nhiều biến Dạng dọc: Nhiều dòng có thể chứa cùng một quan sát nhưng với các biến khác nhau. Xem hai ví dụ về dữ liệu dạng ngang và dọc ở dưới đây. id Species Sepal.Length Sepal.Width Petal.Length Petal.Width 1 setosa 5.1 3.5 1.4 0.2 2 setosa 4.9 3.0 1.4 0.2 3 setosa 4.7 3.2 1.3 0.2 4 setosa 4.6 3.1 1.5 0.2 5 setosa 5.0 3.6 1.4 0.2 6 setosa 5.4 3.9 1.7 0.4 id Species Measurement Value 1 setosa Sepal.Length 5.1 1 setosa Sepal.Width 3.5 1 setosa Petal.Length 1.4 1 setosa Petal.Width 0.2 2 setosa Sepal.Length 4.9 2 setosa Sepal.Width 3.0 2 setosa Petal.Length 1.4 2 setosa Petal.Width 0.2 Trong thực tế, chúng ta phải sử dụng rất linh hoạt cả hai định dạng dữ liệu này (dữ liệu ngang và dữ liệu dọc). Trong chương này, chúng ta sẽ học cách sử dụng và biến đổi dữ liệu giữa hai định dạng với tidyr. 4.1 Phân rã dữ liệu thành dạng dọc với gather library(tidyr) data %&gt;% gather(key = name_of_key, value = name_of_value_variable, gather = c(list_of_var)) Xem ví dụ sau. library(dplyr) df &lt;- iris %&gt;% head(2) %&gt;% mutate(id = 1:nrow(.)) %&gt;% select(6, 5, 1:4) df ## id Species Sepal.Length Sepal.Width Petal.Length ## 1 1 setosa 5.1 3.5 1.4 ## 2 2 setosa 4.9 3.0 1.4 ## Petal.Width ## 1 0.2 ## 2 0.2 # Xoay dữ liệu sang dạng dọc df2 &lt;- df %&gt;% gather(key = Measurement, value = Value, c(3:6)) # Các biến được phân rã df2 ## id Species Measurement Value ## 1 1 setosa Sepal.Length 5.1 ## 2 2 setosa Sepal.Length 4.9 ## 3 1 setosa Sepal.Width 3.5 ## 4 2 setosa Sepal.Width 3.0 ## 5 1 setosa Petal.Length 1.4 ## 6 2 setosa Petal.Length 1.4 ## 7 1 setosa Petal.Width 0.2 ## 8 2 setosa Petal.Width 0.2 Ở ví dụ trên, khi phân rã dữ liệu sang dạng dọc, các biến được phân rã là 4 biến ở vị trí từ 3 đến 6. Do dữ liệu gốc df chỉ có 2 quan sát, nên dữ liệu mới sau khi phân rã sẽ có 8 quan sát. 4.2 Xoay chiều dữ liệu với spread Ngược lại với phân rã dữ liệu là xoay chiều dữ liệu. Trong tidyr, ta có thể sử dụng hàm spread. Công thức tổng quát để xoay chiều dữ liệu như sau. data %&gt;% #Biến được xoay thành cột spread(key = key_variable, # Biến giá trị value = value_variable) Ta quay trở lại ví dụ ở phần trước vói dữ liệu df2 đã được phân rã. df2 ## id Species Measurement Value ## 1 1 setosa Sepal.Length 5.1 ## 2 2 setosa Sepal.Length 4.9 ## 3 1 setosa Sepal.Width 3.5 ## 4 2 setosa Sepal.Width 3.0 ## 5 1 setosa Petal.Length 1.4 ## 6 2 setosa Petal.Length 1.4 ## 7 1 setosa Petal.Width 0.2 ## 8 2 setosa Petal.Width 0.2 Ta có thể xoay chiều dữ liệu lại như sau. df2 %&gt;% spread(key = Measurement, value = Value) ## id Species Petal.Length Petal.Width Sepal.Length ## 1 1 setosa 1.4 0.2 5.1 ## 2 2 setosa 1.4 0.2 4.9 ## Sepal.Width ## 1 3.5 ## 2 3.0 4.3 Tách một biến thành nhiều biến với separate Khi phân tích dữ liệu, ta thường xuyên phải tách một biến thành nhiều biến. Khi đó, việc tách biến sẽ trở nên rất đơn giản với hàm separate. Công thức tổng quát của separate như sau: data %&gt;% separate(var_to_spread, c(&quot;new_var1&quot;, &quot;new_var2&quot;, ...)) df &lt;- data.frame(date = c(NA, &quot;2018-07-01&quot;, &quot;2018-09-02&quot;)) df ## date ## 1 &lt;NA&gt; ## 2 2018-07-01 ## 3 2018-09-02 # Tách biến date thành 3 biến df %&gt;% separate(date, c(&quot;year&quot;, &quot;month&quot;, &quot;date&quot;)) ## year month date ## 1 &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; ## 2 2018 07 01 ## 3 2018 09 02 4.4 Gộp nhiều biến thành một biến với unite Ngược lại với spread, ta có thể gộp nhiều biến thành một với unite. Công thức tổng quát của unite như sau. data %&gt;% unite(new_var, var_1, var2,...) Trong đó, var_1, var_2 là tên các biến sẽ được gộp. new_var là tên biến mới được tạo thành. Quay trở lại ví dụ trên. df &lt;- data.frame(date = c(&quot;2018-07-01&quot;, &quot;2018-09-02&quot;)) %&gt;% separate(date, c(&quot;year&quot;, &quot;month&quot;, &quot;date&quot;)) df ## year month date ## 1 2018 07 01 ## 2 2018 09 02 # Gộp nhiều biến df %&gt;% unite(full_date, 1:3, sep = &quot;/&quot;, remove = F) ## full_date year month date ## 1 2018/07/01 2018 07 01 ## 2 2018/09/02 2018 09 02 "],
["lp-trinh-chc-nang-ham-vi-purrr.html", "Chương 5 Lập trình chức năng hàm với purrr 5.1 Nhóm hàm map 5.2 Sửa đổi giá trị với modify 5.3 Ứng dụng", " Chương 5 Lập trình chức năng hàm với purrr Khi phân tích dữ liệu phức tạp, ta thường xuyên phải thực hiện một nhóm các phân tích tương tự nhau cho các nhóm dữ liệu khác nhau. Việc sử dụng các hàm làm đơn vị thao tác cơ bản và phối hợp các hàm với nhau được gọi là lập trình chức năng hàm (functional programming). Để đơn giản, ta xét ví dụ sau. Sử dụng tập dữ liệu iris, với mỗi nhóm của Species, xây dựng mô hình hồi quy giữa Sepal.Length và Petal.Length, so sánh giá trị r.squared giữa các mô hình. Với cách làm thông thường, ta sẽ phải thức hiện theo thứ tự sau: Tạo các data.frame cho từng giá trị của Species Với mỗi data.frame vừa tạo, xây dựng mô hình lm Với mỗi mô hình vừa tạo, chiết xuất giá trị r.squared và lưu vào một data.frame Cách triển khai trên có thể sử dụng vòng lặp trong R với phương án như sau library(dplyr) category &lt;- iris$Species %&gt;% levels %&gt;% as.character() model_result &lt;- data.frame() for (i in category){ df &lt;- iris %&gt;% filter(Species == i) model &lt;- lm(Sepal.Length ~ Sepal.Width, data = df) model_summary &lt;- summary(model) df_temp &lt;- data.frame(species = i, r.square = model_summary$r.squared) model_result &lt;- bind_rows(model_result, df_temp) } Tuy nhiên, với lập trình chức năng hàm, ta có thể làm rất đơn giản như sau. library(purrr) iris %&gt;% split(.$Species) %&gt;% map(~lm(Sepal.Length ~ Sepal.Width, data = .)) %&gt;% map(summary) %&gt;% map_dbl(&quot;r.squared&quot;) ## setosa versicolor virginica ## 0.5514 0.2766 0.2091 Trong chương này, chúng ta sẽ tìm hiểu các cách thức cơ bản lập trình chức năng hàm với R qua package purrr. Việc nắm vững kiến thức và kỹ năng lập trình hàm có rất nhiều ứng dụng trong công việc phân tích, giúp giảm thiểu rất lớn thời gian phân tích, làm cho quá trình phân tích mạch lạc hơn rất nhiều trong các bài toán khám phá dữ liệu 5.1 Nhóm hàm map Công thức tổng quát của nhóm hàm map map(.x, .f, ...) Giải thích: Với mỗi giá trị của .x, thực hiện .f. Trong đó, x là một list. Hàm map làm hàm tổng quát, ngoài ra, map còn có các biến thể chính sau Câu lênh Kết quả map list map_dbl vector dạng double map_int vector dạng int map_chr vector dạng character map_df data.frame # Dạng list iris %&gt;% map(class) ## $Sepal.Length ## [1] &quot;numeric&quot; ## ## $Sepal.Width ## [1] &quot;numeric&quot; ## ## $Petal.Length ## [1] &quot;numeric&quot; ## ## $Petal.Width ## [1] &quot;numeric&quot; ## ## $Species ## [1] &quot;factor&quot; # Dạng char iris %&gt;% map_chr(class) ## Sepal.Length Sepal.Width Petal.Length Petal.Width ## &quot;numeric&quot; &quot;numeric&quot; &quot;numeric&quot; &quot;numeric&quot; ## Species ## &quot;factor&quot; # Dạng data.frame iris %&gt;% map_df(class) ## # A tibble: 1 x 5 ## Sepal.Length Sepal.Width Petal.Length Petal.Width ## &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; ## 1 numeric numeric numeric numeric ## # ... with 1 more variable: Species &lt;chr&gt; Map theo điều kiện với map_if và map_at Tương tự với map, nhóm map_if và map_at cho phép tính toán theo điều kiện hoặc vị trí của list. Xem ví dụ sau. # map_if iris %&gt;% map_if(is.numeric, as.character) %&gt;% as.data.frame %&gt;% str ## &#39;data.frame&#39;: 150 obs. of 5 variables: ## $ Sepal.Length: Factor w/ 35 levels &quot;4.3&quot;,&quot;4.4&quot;,&quot;4.5&quot;,..: 9 7 5 4 8 12 4 8 2 7 ... ## $ Sepal.Width : Factor w/ 23 levels &quot;2&quot;,&quot;2.2&quot;,&quot;2.3&quot;,..: 15 10 12 11 16 19 14 14 9 11 ... ## $ Petal.Length: Factor w/ 43 levels &quot;1&quot;,&quot;1.1&quot;,&quot;1.2&quot;,..: 5 5 4 6 5 8 5 6 5 6 ... ## $ Petal.Width : Factor w/ 22 levels &quot;0.1&quot;,&quot;0.2&quot;,&quot;0.3&quot;,..: 2 2 2 2 2 4 3 2 2 1 ... ## $ Species : Factor w/ 3 levels &quot;setosa&quot;,&quot;versicolor&quot;,..: 1 1 1 1 1 1 1 1 1 1 ... # map_at iris %&gt;% map_at(c(1,2), as.character) %&gt;% str ## List of 5 ## $ Sepal.Length: chr [1:150] &quot;5.1&quot; &quot;4.9&quot; &quot;4.7&quot; &quot;4.6&quot; ... ## $ Sepal.Width : chr [1:150] &quot;3.5&quot; &quot;3&quot; &quot;3.2&quot; &quot;3.1&quot; ... ## $ Petal.Length: num [1:150] 1.4 1.4 1.3 1.5 1.4 1.7 1.4 1.5 1.4 1.5 ... ## $ Petal.Width : num [1:150] 0.2 0.2 0.2 0.2 0.2 0.4 0.3 0.2 0.2 0.1 ... ## $ Species : Factor w/ 3 levels &quot;setosa&quot;,&quot;versicolor&quot;,..: 1 1 1 1 1 1 1 1 1 1 ... 5.2 Sửa đổi giá trị với modify Tương tự như map, modify cho áp dụng hàm vào một nhóm các list. Tuy nhiên, khác với map, modify cho ra kết quả với cấu trúc dữ liệu ban đâu. # map đổi cấu trúc của dataframe iris %&gt;% map_if(is.factor, as.character) %&gt;% str ## List of 5 ## $ Sepal.Length: num [1:150] 5.1 4.9 4.7 4.6 5 5.4 4.6 5 4.4 4.9 ... ## $ Sepal.Width : num [1:150] 3.5 3 3.2 3.1 3.6 3.9 3.4 3.4 2.9 3.1 ... ## $ Petal.Length: num [1:150] 1.4 1.4 1.3 1.5 1.4 1.7 1.4 1.5 1.4 1.5 ... ## $ Petal.Width : num [1:150] 0.2 0.2 0.2 0.2 0.2 0.4 0.3 0.2 0.2 0.1 ... ## $ Species : chr [1:150] &quot;setosa&quot; &quot;setosa&quot; &quot;setosa&quot; &quot;setosa&quot; ... # modify giữ nguyên cấu trúc iris %&gt;% modify_if(is.factor, as.character) %&gt;% str ## &#39;data.frame&#39;: 150 obs. of 5 variables: ## $ Sepal.Length: num 5.1 4.9 4.7 4.6 5 5.4 4.6 5 4.4 4.9 ... ## $ Sepal.Width : num 3.5 3 3.2 3.1 3.6 3.9 3.4 3.4 2.9 3.1 ... ## $ Petal.Length: num 1.4 1.4 1.3 1.5 1.4 1.7 1.4 1.5 1.4 1.5 ... ## $ Petal.Width : num 0.2 0.2 0.2 0.2 0.2 0.4 0.3 0.2 0.2 0.1 ... ## $ Species : chr &quot;setosa&quot; &quot;setosa&quot; &quot;setosa&quot; &quot;setosa&quot; ... 5.3 Ứng dụng 5.3.1 Biến đổi dữ liệu với modify và map_df Khi phân tích dữ liệu, đôi khi ta cần chuẩn hóa dữ liệu cho tất cả các biến numeric trong data.frame. Với nhóm hàm của purrr, ta có thể thực hiện như sau # Tạo hàm standardize_data &lt;- function(x){ x &lt;- (x - min(x))/(max(x) - min(x)) return(x) } # Sử dụng map_df df &lt;- iris df[, 1:4] &lt;- df[, 1:4] %&gt;% map_df(standardize_data) df %&gt;% head ## Sepal.Length Sepal.Width Petal.Length Petal.Width ## 1 0.22222 0.6250 0.06780 0.04167 ## 2 0.16667 0.4167 0.06780 0.04167 ## 3 0.11111 0.5000 0.05085 0.04167 ## 4 0.08333 0.4583 0.08475 0.04167 ## 5 0.19444 0.6667 0.06780 0.04167 ## 6 0.30556 0.7917 0.11864 0.12500 ## Species ## 1 setosa ## 2 setosa ## 3 setosa ## 4 setosa ## 5 setosa ## 6 setosa # Sử dụng modify # Sử dụng map_df df &lt;- iris df &lt;- df %&gt;% modify_if(is.numeric, standardize_data) df %&gt;% head ## Sepal.Length Sepal.Width Petal.Length Petal.Width ## 1 0.22222 0.6250 0.06780 0.04167 ## 2 0.16667 0.4167 0.06780 0.04167 ## 3 0.11111 0.5000 0.05085 0.04167 ## 4 0.08333 0.4583 0.08475 0.04167 ## 5 0.19444 0.6667 0.06780 0.04167 ## 6 0.30556 0.7917 0.11864 0.12500 ## Species ## 1 setosa ## 2 setosa ## 3 setosa ## 4 setosa ## 5 setosa ## 6 setosa 5.3.2 Phân tích nhiều nhóm khác nhau cùng lúc Khi phân tích dữ liệu, đôi khi ta muốn xây dựng chuẩn phân tích dữ liệu qua một số bước, bao gồm: Tính toán các chỉ số thống kê Vẽ đồ thị Xây dụng mô hình đơn giản Quy trình này sẽ không gặp vấn đề khi ta chỉ phải xử lý với một nhóm nhỏ dữ liệu. Khi số lượng nhóm tăng lên, việc phân tích dữ liệu trở nên khó khăn hơn rất nhiều và tốn thời gian. Tuy nhiên, với purrr, các vấn đề này trở nên rất đơn giản. Ví dụ: Với mỗi nhóm của Species trong tập dữ liệu iris: Tổng hơp dữ liệu Vẽ đồ thị điểm giữa Sepal.Length vs. Petal.Length Xây dựng mô hình hồi quy Sepal.Length ~ Petal.Length library(tidyverse) # Bước một: Xây dựng hàm my_stat &lt;- function(data){ print(&quot;Summary data&quot;) print(&quot;====================&quot;) summary(data) %&gt;% print p &lt;- data %&gt;% ggplot(aes(Sepal.Length, Petal.Length)) + geom_point() print(&quot;Summary model&quot;) print(&quot;====================&quot;) model &lt;- lm(Sepal.Length ~ Petal.Length, data = data) summary(model) %&gt;% print p %&gt;% print } # Test hàm my_stat(iris) ## [1] &quot;Summary data&quot; ## [1] &quot;====================&quot; ## Sepal.Length Sepal.Width Petal.Length ## Min. :4.30 Min. :2.00 Min. :1.00 ## 1st Qu.:5.10 1st Qu.:2.80 1st Qu.:1.60 ## Median :5.80 Median :3.00 Median :4.35 ## Mean :5.84 Mean :3.06 Mean :3.76 ## 3rd Qu.:6.40 3rd Qu.:3.30 3rd Qu.:5.10 ## Max. :7.90 Max. :4.40 Max. :6.90 ## Petal.Width Species ## Min. :0.1 setosa :50 ## 1st Qu.:0.3 versicolor:50 ## Median :1.3 virginica :50 ## Mean :1.2 ## 3rd Qu.:1.8 ## Max. :2.5 ## [1] &quot;Summary model&quot; ## [1] &quot;====================&quot; ## ## Call: ## lm(formula = Sepal.Length ~ Petal.Length, data = data) ## ## Residuals: ## Min 1Q Median 3Q Max ## -1.2468 -0.2966 -0.0152 0.2768 1.0027 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 4.3066 0.0784 54.9 &lt;2e-16 *** ## Petal.Length 0.4089 0.0189 21.6 &lt;2e-16 *** ## --- ## Signif. codes: ## 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 0.407 on 148 degrees of freedom ## Multiple R-squared: 0.76, Adjusted R-squared: 0.758 ## F-statistic: 469 on 1 and 148 DF, p-value: &lt;2e-16 # Bước 2: Xây dựng map iris$Species %&gt;% unique %&gt;% map(function(value){ print(paste0(&quot;Analysis of &quot;, value)) iris %&gt;% filter(Species == value) %&gt;% my_stat }) ## [1] &quot;Analysis of setosa&quot; ## [1] &quot;Summary data&quot; ## [1] &quot;====================&quot; ## Sepal.Length Sepal.Width Petal.Length ## Min. :4.30 Min. :2.30 Min. :1.00 ## 1st Qu.:4.80 1st Qu.:3.20 1st Qu.:1.40 ## Median :5.00 Median :3.40 Median :1.50 ## Mean :5.01 Mean :3.43 Mean :1.46 ## 3rd Qu.:5.20 3rd Qu.:3.67 3rd Qu.:1.57 ## Max. :5.80 Max. :4.40 Max. :1.90 ## Petal.Width Species ## Min. :0.100 setosa :50 ## 1st Qu.:0.200 versicolor: 0 ## Median :0.200 virginica : 0 ## Mean :0.246 ## 3rd Qu.:0.300 ## Max. :0.600 ## [1] &quot;Summary model&quot; ## [1] &quot;====================&quot; ## ## Call: ## lm(formula = Sepal.Length ~ Petal.Length, data = data) ## ## Residuals: ## Min 1Q Median 3Q Max ## -0.5724 -0.2067 -0.0308 0.1734 0.9361 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 4.213 0.416 10.14 1.6e-13 *** ## Petal.Length 0.542 0.282 1.92 0.061 . ## --- ## Signif. codes: ## 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 0.343 on 48 degrees of freedom ## Multiple R-squared: 0.0714, Adjusted R-squared: 0.052 ## F-statistic: 3.69 on 1 and 48 DF, p-value: 0.0607 ## [1] &quot;Analysis of versicolor&quot; ## [1] &quot;Summary data&quot; ## [1] &quot;====================&quot; ## Sepal.Length Sepal.Width Petal.Length ## Min. :4.90 Min. :2.00 Min. :3.00 ## 1st Qu.:5.60 1st Qu.:2.52 1st Qu.:4.00 ## Median :5.90 Median :2.80 Median :4.35 ## Mean :5.94 Mean :2.77 Mean :4.26 ## 3rd Qu.:6.30 3rd Qu.:3.00 3rd Qu.:4.60 ## Max. :7.00 Max. :3.40 Max. :5.10 ## Petal.Width Species ## Min. :1.00 setosa : 0 ## 1st Qu.:1.20 versicolor:50 ## Median :1.30 virginica : 0 ## Mean :1.33 ## 3rd Qu.:1.50 ## Max. :1.80 ## [1] &quot;Summary model&quot; ## [1] &quot;====================&quot; ## ## Call: ## lm(formula = Sepal.Length ~ Petal.Length, data = data) ## ## Residuals: ## Min 1Q Median 3Q Max ## -0.7348 -0.2027 -0.0206 0.2609 0.6996 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 2.408 0.446 5.39 2.1e-06 *** ## Petal.Length 0.828 0.104 7.95 2.6e-10 *** ## --- ## Signif. codes: ## 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 0.343 on 48 degrees of freedom ## Multiple R-squared: 0.569, Adjusted R-squared: 0.56 ## F-statistic: 63.3 on 1 and 48 DF, p-value: 2.59e-10 ## [1] &quot;Analysis of virginica&quot; ## [1] &quot;Summary data&quot; ## [1] &quot;====================&quot; ## Sepal.Length Sepal.Width Petal.Length ## Min. :4.90 Min. :2.20 Min. :4.50 ## 1st Qu.:6.22 1st Qu.:2.80 1st Qu.:5.10 ## Median :6.50 Median :3.00 Median :5.55 ## Mean :6.59 Mean :2.97 Mean :5.55 ## 3rd Qu.:6.90 3rd Qu.:3.17 3rd Qu.:5.88 ## Max. :7.90 Max. :3.80 Max. :6.90 ## Petal.Width Species ## Min. :1.40 setosa : 0 ## 1st Qu.:1.80 versicolor: 0 ## Median :2.00 virginica :50 ## Mean :2.03 ## 3rd Qu.:2.30 ## Max. :2.50 ## [1] &quot;Summary model&quot; ## [1] &quot;====================&quot; ## ## Call: ## lm(formula = Sepal.Length ~ Petal.Length, data = data) ## ## Residuals: ## Min 1Q Median 3Q Max ## -0.7341 -0.2364 -0.0313 0.2377 0.7621 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 1.0597 0.4668 2.27 0.028 * ## Petal.Length 0.9957 0.0837 11.90 6.3e-16 *** ## --- ## Signif. codes: ## 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 0.323 on 48 degrees of freedom ## Multiple R-squared: 0.747, Adjusted R-squared: 0.742 ## F-statistic: 142 on 1 and 48 DF, p-value: 6.3e-16 ## [[1]] ## ## [[2]] ## ## [[3]] 5.3.3 Phân tích nhiều biến trong dataframe cùng lúc Một biên thể khác của map là sử dụng trong phân tích cùng lúc nhiều biến số với một biến thuộc dạng nhóm (group). Khi lập trình với NSE, ta cần phải sử dụng hàm syms() trước khi map Ví dụ: Với mỗi biến số trong tập dữ liệu iris: So sánh giá trị trung bình của biến này với các nhóm khác nhau của Species Vẽ biểu đồ boxplot library(tidyverse) # Bước 1: Xây dựng hàm my_stat &lt;- function(x){ x &lt;- enquo(x) iris %&gt;% group_by(Species) %&gt;% summarise(mean = mean(!!x)) %&gt;% print iris %&gt;% ggplot(aes(Species, !!x)) + geom_boxplot(aes(fill = Species)) + theme_minimal() } my_stat(Sepal.Length) ## # A tibble: 3 x 2 ## Species mean ## &lt;fct&gt; &lt;dbl&gt; ## 1 setosa 5.01 ## 2 versicolor 5.94 ## 3 virginica 6.59 # Bước 2: Dùng map iris %&gt;% select_if(is.numeric) %&gt;% names %&gt;% syms %&gt;% map(my_stat) ## # A tibble: 3 x 2 ## Species mean ## &lt;fct&gt; &lt;dbl&gt; ## 1 setosa 5.01 ## 2 versicolor 5.94 ## 3 virginica 6.59 ## # A tibble: 3 x 2 ## Species mean ## &lt;fct&gt; &lt;dbl&gt; ## 1 setosa 3.43 ## 2 versicolor 2.77 ## 3 virginica 2.97 ## # A tibble: 3 x 2 ## Species mean ## &lt;fct&gt; &lt;dbl&gt; ## 1 setosa 1.46 ## 2 versicolor 4.26 ## 3 virginica 5.55 ## # A tibble: 3 x 2 ## Species mean ## &lt;fct&gt; &lt;dbl&gt; ## 1 setosa 0.246 ## 2 versicolor 1.33 ## 3 virginica 2.03 ## [[1]] ## ## [[2]] ## ## [[3]] ## ## [[4]] Lưu ý: Việc ứng dụng purrr với các hàm tính toán thống kê sẽ cho phép phân tích khám phá dữ liệu hàng loạt "],
["khai-quat-v-hoc-may.html", "Chương 6 Khái quát về học máy 6.1 Giới thiệu 6.2 Quy trình xây dựng mô hình dự báo 6.3 Các nhóm thuật ngữ cần nhớ 6.4 Lưu ý", " Chương 6 Khái quát về học máy Trong chương này, chúng ta sẽ tìm hiểu những khái niệm và nguyên lý cơ bản nhất của học máy (machine learning hay statistical learning). Các nguyên lý này sẽ giúp ta nắm vững để có thể phát triển nhanh chóng trong lĩnh vực dự báo. Khi đã nắm vững các nguyên lý này, việc xây dựng mô hình với các thuật toán khác nhau sẽ không còn quan trọng nữa bởi tất cả sẽ đều phải đi qua các nguyên lý giống nhau. 6.1 Giới thiệu Statistical Learning (SL) hay Machine Learning là nghành học sử dụng nhiều phương pháp và công cụ khác nhau để hiểu dữ liệu. SL có thể được chia thành 2 dạng: Định hướng &amp; không định hướng: Phân tích có định hướng trước (Supervised learning): Xây dựng các mô hình giữa biến phụ thuộc với một hoặc nhiều biến độc lập. Trong đó, kết quả đầu ra y đã được xác định trước. Ví dụ: Dự báo khách hàng vỡ nợ dựa vào các đặc trưng về nhân khẩu học và hành vi giao dịch của khách hàng. Các thuật toán như cây quyết định, logistics, mô hình hồi quy tuyến tính đều thuộc loại này. Tuy nhiên, tùy thuộc vào biến cần dự báo, ta lại có hai nhóm nhỏ sau: Bài toán phân loại (classification): Khi biến phụ thuộc là các biến định dạng nhóm (category). Ví dụ, khách hàng tốt hay xấu, khách hàng mua hay không mua sản phẩm,… Bài toán dự báo (regression): Khi biến phụ thuộc là biến số cần dự báo giá trị. Ví dụ, giá trị của tổng các giao dịch một khách hàng có trong 1 tháng… Phân tích không định hướng trước(Unsupervised learning): Biến phụ thuộc chưa biết trước và mục tiêu của mô hình là tìm ra các mối qua hệ ẩn giữa các nhóm. Ví dụ, phân nhóm khách hàng thành 5 nhóm các hành vi tương tự nhau. Các thuật toán nhưu apriori, k-means, PCA, FA thuộc nhóm này. Reducible vs. irreducible error Trong thực tế, mối quan hệ giữa X &amp; Y được biểu diễn qua hàm sau: \\[Y = f(X) + \\epsilon \\] Khi phân tích dữ liệu, ta tìm hàm \\(\\hat(Y)=\\hat{f}(X)\\) gần nhất với \\(f(X)\\). Sai số giữa thực tế và mô hình sẽ là \\[E(Y-\\hat{Y})^2 = E[f(X)+\\epsilon - \\hat{f}(X)]^2 = \\underbrace{[f(X) - \\hat{f}(X)]^2}_{reducible} + \\underbrace{Var(\\epsilon)}_{irreducible}\\] Khi xây dựng mô hình, ta chỉ có thể giảm bớt phần reducible error Khả năng giải thích và khả năng dự báo (Inference vs. Predict) Khi xây dựng mô hình, có hai khía cạnh cần phải xử lý: Khả năng giải thích hay khả năng rút ra kết luận từ mô hình (inference): Nhấn mạnh đến khả năng diễn đạt ý nghĩa các biến trong mô hình. VD: khách hàng trả nợ trễ hạn 3 lần sẽ làm tăng khả năng trốn nợ lên 20%, giá giảm 10% sẽ khiên doanh thu tăng thêm khoảng 6%. Các thuật toán như OLS, apriori, Logistics thuộc nhóm này. Khả năng dự báo của mô hình (Predictor): Ưu tiên hơn đến tính chính xác của mô hình dự báo, không quan tâm đến việc mô tả quan hệ giữa các biến. Ví dụ: Random Forest, Neuron Network, KNN Đánh đổi giữa độ chính xác của mô hình vs. khả năng diễn giải mô hình Không tồn tại một mô hình tốt nhất cho mọi trường hợp. Do đó, ta cần phải lựa chọn mô hình theo từng đối tượng. Mô hình có độ chính xác cao thường khó mô tả mối quan hệ giữa các biến (VD: decision tree) và ngược lại (VD: OLS) Parametric vs. Nonparametric methods Parametric: Đưa ra mô hình biểu diễn mối quan hệ trước rồi sau đó ước lượng mô hình đưa ra. VD: OLS, Logistic Regression Nonparametric: Không đưa ra mô hình, chỉ đưa ra phương pháp và để thuật toán tự động tìm kết quả. VD: Association rule, decision tree… Đánh giá độ chính xác của mô hình Trong Statistical Learning, ta cố gắng dự báo càng chính xác càng tốt. Một chỉ số quan trọng để đo độ chính xác là MSE. \\[MSE = \\frac{1}{n} \\sum^n_{i=1}(y_i - \\hat{f}(x_i))^2\\] Khi dự báo, ta quan tâm đến MSE của dữ liệu test. Trong mô hình đánh giá hiệu quả MSE, có quy luật như sau: Khi mô hình càng linh hoạt (flexibility), hay nói cách khác - số bậc tự do càng lớn (degrees of freedom - được đo bằng “số lượng quan sát - số biến phụ thuộc”) thì MSE của tập training càng giảm còn MSE của tập test sẽ có dạng chữ U Giải thích: Tại đường MSE cho tập train ở hình trên, ta có: Các kỹ thuật Data Mining dùng trong dự báo sẽ tìm điểm tối ưu, sao cho MSE của tập test sẽ là nhỏ nhất (vị trí min trên đường chữ U) Bias Variance Trade-Off Người ta chứng minh được rằng: \\[E(y_0 - \\hat{f}(x_0))^2 = Var(\\hat{f}(x_0)) + [Bias(\\hat{f}(x_0))]^2 + Var(\\epsilon)\\] Trong đó: \\(E(y_0 - \\hat{f}(x_0))^2\\): Kỳ vọng của MSE \\(Var(\\hat{f}(x_0))\\): Phương sai của MSE trong tập test khi ta thay đổi tập training \\([Bias(\\hat{f}(x_0))]^2\\): Sai số của mô hình ước lượng \\(\\hat{f}(x)\\) so với mô hình thực tế \\(f(x)\\). VD: Mô hình thực tế là \\(y=x^2\\), mô hình ước lượng là \\(y=0.9*x^2\\) \\(Var(\\epsilon)\\): Phương sai của nhiễu Quy luật: Variance của mô hình tăng thì Bias sẽ giảm và ngược lại. Mô hình tốt là mô hình giảm được của Bias và Variance Phương pháp tính toán phân loại - The Classification Setting \\[training\\_error\\_rate = \\frac{1}{n}\\sum_{i=1}^n I(y_i \\neq \\hat {y_i})\\] Trong đó \\(I(y_i \\neq \\hat{y_i}\\) là “indicator variable”, có giá trị bằng 1 nếu \\(y_i \\neq \\hat{y_i}\\), có giá trị bằng 0 nếu \\(y_i= \\hat{y_i}\\) 6.2 Quy trình xây dựng mô hình dự báo Quá trình dự báo: Trong thực tế, quá trình dự báo diễn ra như sau 6.3 Các nhóm thuật ngữ cần nhớ Độ biến động (variance) vs. độ chuẩn xác (bias): variance là độ biến động của mô hình so với dữ liệu, đo độ nhạy cảm của các tham số trong mô hình khi thay đổi dữ liệu. Nói cách khác, một mô hình được gọi là biến động lớn khi dữ liệu mô hình thay đổi sẽ dẫn đến một sự thay đổi lớn trong mô hình. Ví dụ, mô hình sử dụng trung vị làm biến dự báo sẽ ít biến động hơn khi sử dụng giá trị trung bình. bias đo độ chuẩn xác của mô hình. Một mô hình được gọi là bias thấp khi kết quả dự báo gần với kết quả thực tế và ngược lại. Các mô hình có bias thấp có khả năng thích ứng với dữ liệu tốt, các mô hình như cây quyết đinh, neural network thuộc dạng này. 6.4 Lưu ý Khi xây dựng mô hình, hiệu ứng của các biến dự báo (predictor) có thể lớn hơn thuật toán rất nhiều Với cùng một nhóm các biến dự báo đúng thực tế, các thuật toán khác nhau có thể đưa ra các kết quả tương tự nhau. "],
["phan-tich-gio-hang.html", "Chương 7 Phân tích giỏ hàng 7.1 Các khái niệm cơ bản 7.2 Cách thực hiện mô hình 7.3 Ba câu hỏi khi phân tích giỏ hàng 7.4 Ưu nhược điểm", " Chương 7 Phân tích giỏ hàng Phân tích giỏ hàng là 1 phương pháp phân tích với mục đích là để tìm ra được tổ hợp các sản phẩm hay được khách hàng mua cùng nhau. Phân tích giỏ hàng là một nhánh của Frequent Pattern Mining (FPM) là kỹ thuật được dùng trong việc phân tích các hành vi lặp đi lặp lại giữa các yêu tố có liên hệ với nhau. FPM được sử dụng đặc biệt rộng rãi trong các ngành như ecommerce, banking, retail… giúp người bán có thể phân tích hành vi mua sắm của khách hàng. Phương pháp phân tích thương được dùng nhất là sử dụng thuật toánapriori 7.1 Các khái niệm cơ bản Trong phân tích giỏ hàng, chỉ có 5 thuật ngữ đơn giản ta cần phải nhớ là transaction, rule, support, confidence và lift Item: Sản phẩm chứa trong giỏ hàng Transaction: Giao dịch là một hoặc một nhóm các sản phẩm được mua khi khách hàng thực hiện trong cùng một giao dịch Rule: Rule là một quy tắc thể hiện mối quan hệ giữa các sản phẩm có trong cùng một giỏ hàng, có dạng nếu A, thì B. \\[\\left \\{{i_{1},i_{2},...}\\right \\} =&gt; {i_{k}}\\] Ví dụ: {Bánh mỳ} =&gt; {Sữa}: Nếu khách hàng mua bánh mỳ thì khách hàng sẽ mua thêm sữa Support：Tần suất (dưới dạng phần trăm) xuất hiện của các quy tắc trong tổng số các giao dịch. Ví dụ: Một cửa hàng trong tháng 1 có 100 khách hàng, mỗi khách hàng thực hiện một giao dịch. Trong đó 50 khách hàng mua sản phẩm A, 75 khách hàng mua sản phẩm B, và 25 khách hàng mua cả sản phẩm A và B - Support(sản phẩm A) = 50% - Support(Sản phẩm A, sản phẩm B) = 25% Confidence: Cơ hội mua sản phẩm tiếp theo trong hành vi giao dịch của khác hàng. \\[Confidence (i_{m} =&gt; i_{n}) = \\frac{support(i_{m}\\frown i_{n})}{support (i_{m})}\\] &gt; Ví dụ: Confidence (sản phẩm A , sản phẩm B) = 25/50 = 50% - Tức là nếu 1 người mua sản phẩm A, thì xác suất họ cũng mua sản phẩm B là 50% Lift: Nếu khách hàng mua sản phẩm A thì khả năng họ mua sản phẩm B sẽ tăng lên bao nhiêu % \\[Lift(i_{m} =&gt; i_{n}) = \\frac{support(i_{m}\\frown i_{n})}{support(i_{m}) * support (i_{n})} = \\frac{Confidence (i_{m} =&gt; i_{n})}{support (i_{n})}\\] Lift có thể cho 3 loại giá trị Lift &gt; 1: tức là những sản phẩm ở vế trái của rule sẽ làm tăng khả năng xảy ra của những sản phẩm ở vế phải của rule (2 sản phẩm bổ trợ). Ví dụ, mua bia sẽ mua thêm lạc. Lift &lt; 1: tức là những sản phẩm ở vế trái của rule sẽ làm giảm khả năng xảy ra của những sản phẩm ở vế phải của rule ( 2 sản phẩm thay thế được cho nhau). Ví dụ, mua bia sẽ không mua thêm cafe. Lift = 1: Các sản phẩm ở vế trái và vế phải xuất hiện độc lập với nhau về mặt thống kê, ta không thể đưa ra kết luận về tương quan giữa các sản phẩm. 7.2 Cách thực hiện mô hình library(dplyr) library(arules) library(arulesViz) data(&quot;Groceries&quot;) Dữ liệu Groceries chứa gần 10,000 giao dịch với hơn 160 sản phẩm khác nhau. Ta có thể xem chi tiết dưới đây. # Thống kê giao dịch summary(Groceries) ## transactions as itemMatrix in sparse format with ## 9835 rows (elements/itemsets/transactions) and ## 169 columns (items) and a density of 0.02609 ## ## most frequent items: ## whole milk other vegetables rolls/buns ## 2513 1903 1809 ## soda yogurt (Other) ## 1715 1372 34055 ## ## element (itemset/transaction) length distribution: ## sizes ## 1 2 3 4 5 6 7 8 9 10 11 ## 2159 1643 1299 1005 855 645 545 438 350 246 182 ## 12 13 14 15 16 17 18 19 20 21 22 ## 117 78 77 55 46 29 14 14 9 11 4 ## 23 24 26 27 28 29 32 ## 6 1 1 1 1 3 1 ## ## Min. 1st Qu. Median Mean 3rd Qu. Max. ## 1.00 2.00 3.00 4.41 6.00 32.00 ## ## includes extended item information - examples: ## labels level2 level1 ## 1 frankfurter sausage meat and sausage ## 2 sausage sausage meat and sausage ## 3 liver loaf sausage meat and sausage Lưu ý: Dữ liệu phục vụ phân tích giỏ hàng không phải là dữ liệu dạng dataframe thông thường mà được cấu trúc định dạng transaction. # Sử dụng dữ liệu groceries class(Groceries) ## [1] &quot;transactions&quot; ## attr(,&quot;package&quot;) ## [1] &quot;arules&quot; str(Groceries) ## Formal class &#39;transactions&#39; [package &quot;arules&quot;] with 3 slots ## ..@ data :Formal class &#39;ngCMatrix&#39; [package &quot;Matrix&quot;] with 5 slots ## .. .. ..@ i : int [1:43367] 13 60 69 78 14 29 98 24 15 29 ... ## .. .. ..@ p : int [1:9836] 0 4 7 8 12 16 21 22 27 28 ... ## .. .. ..@ Dim : int [1:2] 169 9835 ## .. .. ..@ Dimnames:List of 2 ## .. .. .. ..$ : NULL ## .. .. .. ..$ : NULL ## .. .. ..@ factors : list() ## ..@ itemInfo :&#39;data.frame&#39;: 169 obs. of 3 variables: ## .. ..$ labels: chr [1:169] &quot;frankfurter&quot; &quot;sausage&quot; &quot;liver loaf&quot; &quot;ham&quot; ... ## .. ..$ level2: Factor w/ 55 levels &quot;baby food&quot;,&quot;bags&quot;,..: 44 44 44 44 44 44 44 42 42 41 ... ## .. ..$ level1: Factor w/ 10 levels &quot;canned food&quot;,..: 6 6 6 6 6 6 6 6 6 6 ... ## ..@ itemsetInfo:&#39;data.frame&#39;: 0 obs. of 0 variables # Xem 5 giao dịch đầu tiên Groceries[1:5] %&gt;% inspect ## items ## 1 {citrus fruit, ## semi-finished bread, ## margarine, ## ready soups} ## 2 {tropical fruit, ## yogurt, ## coffee} ## 3 {whole milk} ## 4 {pip fruit, ## yogurt, ## cream cheese , ## meat spreads} ## 5 {other vegetables, ## whole milk, ## condensed milk, ## long life bakery product} Trong thực tế, khi triển khai phân tích, việc đầu tiên ta cần làm là biến đổi từ định dạng dataframe sang định dạng transactions. Ta có thể biến đổi định dạng của dataframe về transactions với hàm as. df &lt;- data.frame( prod_1 = c(1,0, 1) %&gt;% as.factor, prod_2 = c(0,0, 1) %&gt;% as.factor, prod_3 = c(1, 0, 0) %&gt;% as.factor ) df ## prod_1 prod_2 prod_3 ## 1 1 0 1 ## 2 0 0 0 ## 3 1 1 0 as(df %&gt;% select(-1), &quot;transactions&quot;) ## transactions in sparse format with ## 3 transactions (rows) and ## 4 items (columns) Phân tích khám phá nhanh các sản phẩm được mua nhiều nhất. # Vẽ barchart đơn giản về các item phổ biến nhất itemFrequencyPlot(Groceries, type = &quot;absolute&quot;, topN = 20, decreasing = T) 7.3 Ba câu hỏi khi phân tích giỏ hàng Khi sử dụng kỹ thuật phân tích giỏ hàng, có 3 câu hỏi thường gặp về mặt kinh doanh cần phải trả lời là: Các sản phẩm nào hay được mua cùng nhau? Nếu khách hàng mua sản phẩm A rồi thì sẽ hay mua tiếp sản phẩm nào? Khách hàng nếu mua sản phẩm B thì trước đấy hay mua sản phẩm nào? 7.3.1 Các sản phẩm nào hay được mua cùng nhau rules &lt;- apriori(Groceries, parameter = list(supp = 0.001, # support &gt;= 0.1% conf = 0.5)) ## Apriori ## ## Parameter specification: ## confidence minval smax arem aval originalSupport ## 0.5 0.1 1 none FALSE TRUE ## support minlen maxlen target ext ## 0.001 1 10 rules FALSE ## ## Algorithmic control: ## filter tree heap memopt load sort verbose ## 0.1 TRUE TRUE FALSE TRUE 2 TRUE ## ## Absolute minimum support count: 9 ## ## set item appearances ...[0 item(s)] done [0.00s]. ## set transactions ...[169 item(s), 9835 transaction(s)] done [0.00s]. ## sorting and recoding items ... [157 item(s)] done [0.00s]. ## creating transaction tree ... done [0.00s]. ## checking subsets of size 1 2 3 4 5 6 done [0.01s]. ## writing ... [5668 rule(s)] done [0.00s]. ## creating S4 object ... done [0.00s]. # confidence &gt;= 50% # Tổng hợp các rules rules %&gt;% summary ## set of 5668 rules ## ## rule length distribution (lhs + rhs):sizes ## 2 3 4 5 6 ## 11 1461 3211 939 46 ## ## Min. 1st Qu. Median Mean 3rd Qu. Max. ## 2.00 3.00 4.00 3.92 4.00 6.00 ## ## summary of quality measures: ## support confidence lift ## Min. :0.00102 Min. :0.500 Min. : 1.96 ## 1st Qu.:0.00112 1st Qu.:0.545 1st Qu.: 2.46 ## Median :0.00132 Median :0.600 Median : 2.90 ## Mean :0.00167 Mean :0.625 Mean : 3.26 ## 3rd Qu.:0.00173 3rd Qu.:0.684 3rd Qu.: 3.69 ## Max. :0.02227 Max. :1.000 Max. :19.00 ## ## mining info: ## data ntransactions support confidence ## Groceries 9835 0.001 0.5 Như vậy, khi khai phá dữ liệu của tập Groceries, ta có 5668 rules thỏa mãn hai điều kiện: Tần suất xuất hiện đạt ít nhất 1% Confidence của rule đạt ít nhất 50% # Top 5 rules có lift cao nhất rules %&gt;% sort(by = &quot;lift&quot;) %&gt;% head(5) %&gt;% as(&quot;data.frame&quot;) %&gt;% knitr::kable(booktabs = T) rules support confidence lift 53 {Instant food products,soda} =&gt; {hamburger meat} 0.0012 0.6316 19.00 37 {soda,popcorn} =&gt; {salty snack} 0.0012 0.6316 16.70 444 {flour,baking powder} =&gt; {sugar} 0.0010 0.5556 16.41 327 {ham,processed cheese} =&gt; {white bread} 0.0019 0.6333 15.05 55 {whole milk,Instant food products} =&gt; {hamburger meat} 0.0015 0.5000 15.04 Giải thích: Với rule đầu tiên là {Instant food products, soda}, ta có thể diễn giải như sau: support = 0.00122 - Tần xuất xuất hiện rules trong tổng số các transaction là 1.22% confidence = 0.63157 - Nếu khách hàng mua đồ ăn nhanh (instant food products), 63.16% khách hàng sẽ mua thêm soda lift = 18.99 - Mối quan hệ giữa hai sản phẩm đồ ăn nhanh và soda cao gần 19 lần so với thông thường (khi hai sản phẩm hoàn toàn độc lập với nhau) Lưu ý: Khi phân tích, ta cần loại bỏ các rule thừa (redundant) khi phân tích dữ liệu. Một rule A được gọi là một rule thừa nếu tồn tại một rule con có confidence lớn hơn hoặc bằng rule A này. Rule B được gọi là rule con của rule A nếu có cùng RHS nhưng các sản phẩm trong rule B ít hơn rule A Ví dụ: Với 2 rule Rule A với {a,b,c} → {d} Rule B với {a,b} → {d} Rule B với {a,b} → {d} được gọi là rule thừa nếu \\(conf(A) &gt;= conf(B)\\) Cách loại bỏ rule thừa trong R subset.matrix &lt;- is.subset(rules, rules) subset.matrix[lower.tri(subset.matrix, diag = T)] &lt;- NA redundant &lt;- colSums(subset.matrix, na.rm = T) &gt;= 1 rules.pruned &lt;- rules[!redundant] rules &lt;- rules.pruned rules %&gt;% summary ## set of 1904 rules ## ## rule length distribution (lhs + rhs):sizes ## 2 3 4 5 ## 11 1381 509 3 ## ## Min. 1st Qu. Median Mean 3rd Qu. Max. ## 2.00 3.00 3.00 3.27 4.00 5.00 ## ## summary of quality measures: ## support confidence lift ## Min. :0.00102 Min. :0.500 Min. : 1.96 ## 1st Qu.:0.00112 1st Qu.:0.524 1st Qu.: 2.21 ## Median :0.00153 Median :0.556 Median : 2.68 ## Mean :0.00207 Mean :0.578 Mean : 2.91 ## 3rd Qu.:0.00224 3rd Qu.:0.613 3rd Qu.: 3.18 ## Max. :0.02227 Max. :1.000 Max. :19.00 ## ## mining info: ## data ntransactions support confidence ## Groceries 9835 0.001 0.5 Như vậy, sau khi lọc bỏ các rule thừa, số lượng rule trong dữ liệu giảm đi gần 3 lần xuống còn 1904 rules khác nhau. Ta có thể vẽ biểu đồ cho nhóm 10 rules có lift cao nhất như sau. rules %&gt;% head(10) %&gt;% plot(method = &quot;graph&quot;) 7.3.2 Khách hàng mua sản phẩm A thì sẽ mua sản phẩm nào tiếp theo? rules &lt;- apriori(data = Groceries, parameter = list(supp = 0.001, conf = 0.15, minlen = 2), appearance = list(default = &quot;rhs&quot;, lhs = &quot;whole milk&quot;), control = list(verbose = F)) rules %&gt;% sort(by = &quot;lift&quot;) %&gt;% head %&gt;% as(&quot;data.frame&quot;) %&gt;% knitr::kable(booktabs = TRUE) rules support confidence lift 2 {whole milk} =&gt; {root vegetables} 0.0489 0.1914 1.7560 1 {whole milk} =&gt; {tropical fruit} 0.0423 0.1655 1.5776 4 {whole milk} =&gt; {yogurt} 0.0560 0.2193 1.5717 6 {whole milk} =&gt; {other vegetables} 0.0748 0.2929 1.5136 5 {whole milk} =&gt; {rolls/buns} 0.0566 0.2216 1.2050 3 {whole milk} =&gt; {soda} 0.0401 0.1568 0.8991 Tham số lhs cho phép chúng ta lựa chọn điều kiện về sản phẩm được mua đầu tiên. Trong trường hợp này, ta thấy khách hàng mua sữa sẽ có xu hương mua thêm rau củ quả (root vegetables) 7.3.3 Khách hàng mua sản phẩm gì thì sẽ mua tiếp sản phẩm A? Tương tự với lhs, ta có thể sử dụng tham số rhs để tìm kiếm các khách hàng tiềm năng cho một sản phẩm đã xác định trước. rules &lt;- apriori(data = Groceries, parameter = list(supp = 0.001, conf = 0.08), appearance = list(default = &quot;lhs&quot;, rhs = &quot;whole milk&quot;), control = list(verbose = F)) rules %&gt;% sort(by = &quot;lift&quot;) %&gt;% head %&gt;% as(&quot;data.frame&quot;) %&gt;% knitr::kable(booktabs = TRUE) rules support confidence lift 196 {rice,sugar} =&gt; {whole milk} 0.0012 1 3.914 323 {canned fish,hygiene articles} =&gt; {whole milk} 0.0011 1 3.914 1643 {root vegetables,butter,rice} =&gt; {whole milk} 0.0010 1 3.914 1705 {root vegetables,whipped/sour cream,flour} =&gt; {whole milk} 0.0017 1 3.914 1716 {butter,soft cheese,domestic eggs} =&gt; {whole milk} 0.0010 1 3.914 1985 {pip fruit,butter,hygiene articles} =&gt; {whole milk} 0.0010 1 3.914 7.4 Ưu nhược điểm Như vậy, ta vừa thực hiện xong việc ứng dụng phân tích giỏ hàng. Đây là phương pháp cực kỳ hữu hiệu trong việc tìm kiếm các mối quan hệ ẩn, chưa được khám phá giữa các biến. Phương pháp này cũng có các ưu và nhược điểm như sau. Ưu điểm: Thực hiện nhanh Tính giải thích cao, trưc quan hóa Có thể nhanh chóng tìm ra tập khách hàng tiềm năng theo điều kiện và không cần phải chia ra thành các tập train/test như các thuật toán machine learning khác. Nhược điểm: Chỉ dùng cho các biến factor, không dùng được cho các biến dạng số như thu nhập, độ tuổi,… Nếu muốn đưa các biến này vào cần phải biến đổi thành dạng factor Khi có quá nhiều nhóm, tốc độ tính toán có thể rất chậm và tràn bộ nhớ "],
["feature-engineering.html", "Chương 8 Feature Engineering 8.1 Feature engineering cho các biến nhóm 8.2 Các biến liên tục", " Chương 8 Feature Engineering Khi xây dựng mô hình dự báo, ta phải cân bằng giữa độ chính xác và khả năng giải thích của mô hình. Trong nhiều trường hợp, khả năng giải thích được ưu tiên hơn, thể hiện rất rõ trong các mô hình score card của rủi ro trong hệ thống ngân hàng. Tuy nhiên, ta không thể hy sinh độ chính xác để lấy khả năng giải thích nếu độ chính xác của mô hình không đạt đến một ngưỡng nhất định. Khi đó, có hai cách tiếp cận: Cho thêm biến vào mô hình. Thay đổi các biến có sẵn bằng các nhóm biến phái sinh để mô hình tốt hơn - cách tiếp cận này gọi là feature engineering. Feature Engineering là quá trình thể hiện các biến đầu vào (variables) với những cách thức khác nhau để giúp cho tăng độ chính xác của mô hình dự báo. Ví dụ: Địa điểm của khách hàng có thể được thể hiện bằng ZIP code hoặc cùng lúc 2 biến, kinh độ và vĩ độ. Biến dự báo age có thể cho ra kết quả tốt hơn trong mô hình khi ta dùng biến \\(\\frac{1}{age}\\) Với mỗi mô hình, thuật toán khác nhau sẽ yêu cầu những cách thức triển khai và thay đổi biến đầu vào khác nhau. Do đó, khi lựa chọn cách thức biến đổi biến đầu vào, ta phải nắm rất rõ thuật toán và cách thức biến đổi dữ liệu theo từng trường hợp khác nhau. Đối với feature engineering, ta có thể chia làm 2 nhóm chính. Biến đổi các biến định dạng nhóm (categorical) Biến đổi các biến định dạng số (numeric) 8.1 Feature engineering cho các biến nhóm 8.1.1 Tạo dữ liệu giả (dummy data) cho biến không phân biệt thứ tự Trong phương pháp này, toàn bộ các dữ liệu gốc được chuyển sang dạng 0-1. Tuy nhiên, dữ liệu mới được tạo ra sẽ ít hơn dữ liệu gốc 1 trường hợp. Bởi lẽ khi biết giá trị của 6 biến, ta có thể biết được giá trị của biến cuối cùng. Biến gốc Mon Tues Wed Thurs Fri Sat Sun 0 0 0 0 0 0 Mon 1 0 0 0 0 0 Tues 0 1 0 0 0 0 Wed 0 0 1 0 0 0 Thurs 0 0 0 1 0 0 Fri 0 0 0 0 1 0 Sat 0 0 0 0 0 1 zero-variance predictor: là biến chỉ có một giá trị. Khi xây dựng mô hình, ta cần loại biến này. 8.1.2 Dữ liệu có rất nhiều nhóm Đối với các biến có rất nhiều nhóm (ví dụ: 200 chi nhánh trong ngân hàng), ta có 2 cách tiếp cận. Cách một, dựa vào kiến thức nghiệp vụ tự nhóm. Ví dụ, các chi nhánh ở Hà Nội sẽ đánh dấu là HN, ở Hồ Chí Minh là HCM, các chi nhánh còn lại là Others. Cách hai, sử dụng hash function. Trong trường hợp này, các biến category sẽ được tạo thành một biến hoàn toàn mới có giá trị số. Xem ví dụ dưới đây. Giá trị Hash belvedere tiburon 582753783 berkeley 1166288024 Lưu ý: Nhiều thí nghiệm đã được sử dụng để so sánh sự khác biệt giữa việc dùng factor và encoding 0-1 trong dữ liệu. Kết quả cho thấy không có nhiều sự khác biệt giữa hai cách. 8.2 Các biến liên tục Đối với các biến liên tục, khi xây dựng mô hình, ta sẽ gặp phải các vấn đề sau. Các biến có các đơn vị khác nhau. Ví dụ, tuổi có giá trị từ 15-75, thu nhập có giá trị từ 2 triệu VND đến 200 triệu VND Các biến bị lệch sang phải (skewness) Các biến có xuất hiện giá trị ngoại lai (outliers) Các biến có thể bị chặn trai hoặc chặn phải. Ví dụ, độ tuổi có giá trị không quá 80 Đối với các biến số, có ba nhóm kỹ thuật lớn biến đổi dữ liệu. Biến đổi 1:1 - một biến được biến đổi thành một biến khác Biến đổi 1:n - một biến được biến đổi thành nhiều biến khác nhau Biến đổi n:n - n biến gốc được biến đổi cùng lúc thành n biến khác 8.2.1 Biến đổi 1:1 Trong biến đổi 1:1, có rất nhiều cách khác nhau. Biến đổi theo scale của dữ liệu: log, Box-Cox \\[x^{*} = \\left\\{ \\begin{array}{l l} \\frac{x^{\\lambda}-1}{\\lambda\\: \\tilde{x}^{\\lambda-1}}, &amp; \\lambda \\neq 0 \\\\ \\tilde{x} \\: \\log x, &amp; \\lambda = 0 \\\\ \\end{array} \\right.\\] "],
["gii-thiu-v-chui-thi-gian.html", "Chương 9 Giới thiệu về chuỗi thời gian 9.1 Thành phần của chuỗi thời gian", " Chương 9 Giới thiệu về chuỗi thời gian Khi phân tích dữ liệu, có hai khái niệm đều được gọi là dự báo khi được dịch sang tiếng Việt, đó là forcast và predict. Tuy nhiên, hai khái niệm này rất khác nhau. Predict: Thường được dùng để chỉ việc dự báo xác suất xảy ra các sự kiện. Ví dụ, xác suất vỡ nợ, xác suất khách hàng churn,… Forecast: Thường dùng trong việc dự báo chuỗi thời gian. Ví dụ, dựa vào lịch sử biến động của tổng khách hàng uống cafe theo tuần, ta có thể dự báo được số lượng khách hàng uống cafe của 1 tuần tới, hai tuần tới,… Trong phần trước, chúng ta đã bàn nhiều về nhóm predict, trong phần này, ta sẽ bàn thêm về nhóm forecast, về các cấu phần và cách thức dự báo đối với chuỗi thời gian. Do đó, trong phần này, khi nói về dự báo, chúng ta đang bàn về vấn đề forecast. Phân biệt dự báo, mục tiêu và kế hoạch Dự báo: là quá trình sử dụng các thông tin hiện hữu đưa ra các nhận định chính xác nhất có thể có trong tương lai trong một khoảng thời gian xác định về một chỉ số nào đó. Mục tiêu: là thứ mà cá nhân, tổ chức mong muốn đạt được trong một khoảng thời gian xác định trong tương lai. Thường thì mục tiêu được đặt mà không quan tâm đến bất kỳ đến việc dự báo nào cả. Ví dụ, mục tiêu tăng trưởng của doanh nghiệp thường là năm sau cao gấp đôi năm trước trong khi dự báo chỉ có thể tăng được 30%. Kế hoạch: Là phản ứng của tổ chức, cá nhân đối với dự báo và mục tiêu. Việc lập kế hoạch đòi hỏi nhiều hành động cụ thể để điều hướng dự báo sát sát với mục tiêu (hoặc vượt mục tiêu) Xét về yếu tố thời gian, việc dự báo có thể chia thành dự báo ngắn hạn, trung hạn và dài hạn. Các điểm khi dự báo Khi dự báo, có hai điểm quan trọng chúng ta phải trả lời. Thứ nhất, ta cần dự báo điều gì? Dự báo với từng sản phẩm hay với cả nhóm sản phẩm? Dự báo doanh số bán hàng của từng cửa hàng hay của toàn hệ thống? Thứ hai, yếu tố thời gian xét trong vấn đề dự báo này là gì? Ta cần dự báo trong bao lâu? Tần xuất như thế nào? Ví dụ, dư báo doanh số bán hàng mỗi tháng/tuần 1 lần trong 1 năm tới? Lưu ý: Các bạn phân tích dữ liệu cần phải tìm hiểu các đơn vị nghiệp vụ sẽ sử dụng kết quả dự báo như thế nào, để tránh việc bỏ quá nhiều thời gian và công sức dự báo nhưng không ai sử dụng. 9.1 Thành phần của chuỗi thời gian Trong bất cứ chuỗi thời gian nào, cũng có 3 thành phần sau. Xu hướng (trend) thể hiện chiều hướng tăng hay giảm dài hạn của chuỗi thời gian Mùa vụ (seasonal) thể hiện sự biến đổi của chuỗi thời gian theo chu kỳ biết trước. Ví dụ, vào cuối tuần, khách hàng có xu hướng đi ăn nhà hàng nhiều hơn. Chu kỳ kinh doanh (cyclic) thể hiện xu hướng biến đổi dài hạn của chuỗi thời gian, thường ít nhất hai năm. Chu kỳ kinh doanh khác với yếu tố mùa vụ ở chỗ, chu kỳ biến đổi của yếu tố mùa vụ thường là đã được biết trước và mang tính ngắn hạn. "],
["trc-quan-hoa-d-liu.html", "Chương 10 Trực quan hóa dữ liệu 10.1 Xây dựng phễu bán hàng theo từng nhóm 10.2 Vẽ biểu đồ warterfall cho acive/inactive users", " Chương 10 Trực quan hóa dữ liệu 10.1 Xây dựng phễu bán hàng theo từng nhóm Trong quá trình phân tích bán hàng, phếu bán hàng (sale funnel) là một kỹ thuật rất hữu dụng để trực quan hóa kết quả kinh doanh theo từng nhóm. Tuy nhiên, hiện ít có biểu đồ nào thể hiện được phễu bán hàng một cách hiệu quả trên R. Trong mục này, tác giả sẽ hướng dẫn một ví dụ thực tiễn trực quan hóa phễu bán hàng một cách hiệu quả. Xem ví dụ điển hình về phễu bán hàng dưới đây data &lt;- read.table(textConnection( c(&quot;step;segment1;segment2;segment3;total 1_visit;1806;11663;12641;26110 2_register;1143;6476;5372;12991 3_login;1806;11663;2694;16163 4_subscribe;21;3322;2694;6037 5_paid;259;422;41;722&quot;)), header = T, sep = &quot;;&quot;) # Dữ liệu data ## step segment1 segment2 segment3 ## 1 1_visit 1806 11663 12641 ## 2 2_register 1143 6476 5372 ## 3 3_login 1806 11663 2694 ## 4 4_subscribe 21 3322 2694 ## 5 5_paid 259 422 41 ## total ## 1 26110 ## 2 12991 ## 3 16163 ## 4 6037 ## 5 722 Trong tập dữ liệu trên, ta sẽ mô phỏng dữ liệu phếu bán hàng của 3 phân khúc khách hàng trên một trang thương mại điện tử mà trong đó, khách hàng sẽ đi qua năm bước khác nhau: Ghé thăm website (visit) Đăng ký (register) Đăng nhập (login) Đăng ký cập nhật các thông tin sản phẩm (subscribe) Mua hàng và trả tiền thành công (paid) Để tạo một biểu đồ phễu bán hàng, ta sẽ thực hiện 3 bước lớn sau. Tạo theme cho biểu đồ Tạo các biểu đồ con cho phễu bán hàng Kết hợp các biểu đồ để tạo thành phễu bán hàng hoàn chỉnh # Gọi library library(tidyverse) library(reshape2) library(forcats) library(ggthemes) # Tạo theme trông cho chart funnel_theme &lt;- theme(axis.title = element_blank(), axis.ticks.x = element_blank(), axis.text.x = element_blank(), legend.position = &quot;none&quot;, panel.grid = element_blank() ) # Phân rã dữ liệu df &lt;- data %&gt;% melt(id.vars = &quot;step&quot;) # Tạo biểu đồ chính p1 &lt;- df %&gt;% mutate(step = fct_rev(step)) %&gt;% filter(variable != &quot;total&quot;) %&gt;% ggplot(aes(step, value)) + geom_bar(aes(fill = variable), stat = &quot;identity&quot;) + facet_grid(~variable, scale = &quot;free&quot;) + coord_flip() + geom_text(aes(label = value), position = position_stack(vjust = .5)) + scale_fill_tableau() + theme_minimal() + scale_y_sqrt() + funnel_theme + theme(plot.margin=grid::unit(c(0,0,0,0), &quot;mm&quot;)) + theme( axis.text.y = element_blank(), strip.text = element_text(size = 14, face = &quot;bold&quot;)) + theme( panel.spacing = unit(0, &quot;mm&quot;)) + annotate(&quot;rect&quot;, xmin = 0.5, xmax = 1.5, ymin = 0, ymax = Inf, alpha = .2) + annotate(&quot;rect&quot;, xmin = 2.5, xmax = 3.5, ymin = 0, ymax = Inf, alpha = .2) + annotate(&quot;rect&quot;, xmin = 4.5, xmax = 5.5, ymin = 0, ymax = Inf, alpha = .2) + theme(axis.text.y = element_blank()) p1 Tạo thêm phần label tổng theo từng segment df %&gt;% mutate(step = fct_rev(step)) %&gt;% filter(variable == &quot;total&quot;) %&gt;% ggplot(aes(step, 0)) + geom_label(aes(label = value), col = &quot;white&quot;, fill = &quot;darkred&quot;, size = 4) + coord_flip() + facet_wrap(~variable) + theme_minimal() + theme(axis.text = element_blank()) + funnel_theme + theme( strip.text.x = element_blank() ) -&gt; p2 p2 Tạo thêm thứ tự các bước trong phễu bán hàng để dễ theo dõi hơn df2 &lt;- data.frame(step = data$step, value = 1:5) df2 %&gt;% mutate(step = fct_rev(step)) %&gt;% ggplot(aes(step, 1)) + geom_hline(yintercept = 1) + geom_point(size = 10, col = &quot;darkgreen&quot;) + geom_text(aes(label = value), col = &quot;white&quot;) + coord_flip() + theme_minimal() + funnel_theme + theme( axis.text = element_text(size = 14) ) -&gt; p3 p3 Cuối cùng, ta có thể tạo ghép các biểu đồ rời rạc để tạo thành phễu bán hàng hoàn chỉnh. Việc kết hợp các biểu đồ trên ggplot2 có thể hoàn thành một cách đơn giản với ggplot2 #devtools::install_github(&quot;thomasp85/patchwork&quot;) library(patchwork) p3 + labs(title = &quot;Sale funnel for 3 segments&quot;) + p1 + p2 + plot_layout(nrow = 1, widths = c(1, 8, 1)) Như vậy, chúng ta đã hoàn thành phễu bán hàng rất chuyên nghiệp với ggplot2. Phễu bán hàng này đặc biệt hiệu quả khi cùng lúc phải so sánh nhiều phân khúc khách hàng khác nhau trên toàn bộ chuỗi bán hàng. 10.2 Vẽ biểu đồ warterfall cho acive/inactive users Trong kỷ nguyên số, chỉ số active user (tạm dịch: người dùng thường xuyên hoạt động) là chỉ số đặc biệt quan trọng với bất kỳ website/ app nào. Công thức tính chỉ số người dùng thường xuyên hoạt động tại khoảng thời gian t được tính như sau: \\[active_{t} = active_{t-1} + new_{t} - churn_{t}\\] Ví dụ về waterfall chart được lấy từ ví dụ của Tableau tại đường link: [https://public.tableau.com/views/CH24_BBOD_ChurnTurnover/SubscriberChurnAnalysis] Trong case study này, chúng ta sẽ tìm cách xây dựng một biểu đồ waterfall chart tương tự # Load library library(tidyverse) library(ggplot2) library(reshape2) library(lubridate) library(grid) library(gridExtra) # Tạo dữ liệu giả lập set.seed(123) data &lt;- data.frame(date = seq(1, 372, by = 31) %&gt;% as_date) data &lt;- data %&gt;% mutate(new = abs(rnorm(12, 100, 10)) %&gt;% round(0)) %&gt;% mutate(churn = abs(rnorm(12, 50, 30)) %&gt;% round(0)) %&gt;% mutate(net = new - churn) %&gt;% mutate(eop = cumsum(net)) %&gt;% select(-net) data ## date new churn eop ## 1 1970-01-02 94 62 32 ## 2 1970-02-02 98 53 77 ## 3 1970-03-05 116 33 160 ## 4 1970-04-05 101 104 157 ## 5 1970-05-06 101 65 193 ## 6 1970-06-06 117 9 301 ## 7 1970-07-07 105 71 335 ## 8 1970-08-07 87 36 386 ## 9 1970-09-07 93 18 461 ## 10 1970-10-08 96 43 514 ## 11 1970-11-08 112 19 607 ## 12 1970-12-09 104 28 683 Trong ví dụ này, dữ liệu được tạo ngẫu nhiên sao cho số lượng active user cuối kỳ (eop - end of period) bằng với số cuối kỳ trước, thêm số lượng mới và trừ đi lượng khách hàng rời bỏ (churn). Để tạo waterfall chart, ta có thể sử dụng geom_segment trong ggplot2 # Xác định độ rộng của segment step &lt;- 0.4*(max(data$date) - min(data$date))/(nrow(data) - 1) # Xác định ymax data &lt;- data %&gt;% mutate(ymax = eop + churn) # Xác định ymin df &lt;- data %&gt;% melt(id.vars = c(&quot;date&quot;, &quot;eop&quot;, &quot;ymax&quot;)) %&gt;% mutate(ymin = ymax - value) %&gt;% rename(group = variable) # Xác định xmin và xmax df &lt;- df %&gt;% mutate(xmin = case_when( group == &quot;new&quot; ~ date - step, TRUE ~ date )) %&gt;% mutate(xmax = case_when( group == &quot;new&quot; ~ date, TRUE ~ date + step )) # Create waterfall chart p1 &lt;- df %&gt;% arrange(date) %&gt;% ggplot() + geom_rect(aes(xmin = xmin, xmax = xmax, ymin = ymin, ymax = ymax, fill = group)) p1 Như vậy, ta đã tạo xong biểu đồ water-fall đơn giản. Ở bước tiếp theo, chúng ta cần điều chỉnh lại các thành phần cho biểu đồ. # Tạo dữ liệu cho biểu đồ đường df2 &lt;- df %&gt;% select(date, eop) %&gt;% distinct() # Điều chỉnh theme p2 &lt;- p1 + geom_line(aes(date, eop), col = &quot;dodgerblue4&quot;, size = 1) + geom_point(aes(date, eop), col = &quot;dodgerblue4&quot;, size = 2.5) + geom_text(aes(date, eop, label = eop), vjust = 1.2, hjust = -0.1) + scale_fill_manual(values = c(&quot;grey60&quot;, &quot;coral2&quot;)) + theme_minimal() + theme( axis.line = element_line(color = &quot;gray40&quot;, size = 0.5), legend.position = &quot;top&quot;) + scale_x_date(breaks = data$date, date_labels = &quot;%b&quot;) + theme(panel.grid.minor.x = element_blank(), legend.title = element_blank()) + ggtitle(&quot;Overview of active users&quot;) + xlab(&quot;Date&quot;) + ylab(&quot;Number of active users&quot;) p2 Bước tiếp theo, ta cần xây dựng biểu đồ bar đơn giản để có thể đưa vào góc phần tư bên trái của biểu đồ vừa tạo. p3 &lt;- df %&gt;% mutate(value = case_when( group == &quot;churn&quot; ~ -1 * value, TRUE ~ value )) %&gt;% ggplot(aes(date, value)) + geom_bar(aes(fill = group), stat = &quot;identity&quot;) + scale_fill_manual(values = c(&quot;grey60&quot;, &quot;coral2&quot;)) + theme_minimal() + theme( legend.position = &quot;none&quot;, axis.title.x = element_blank(), axis.title.y = element_blank(), axis.ticks.y = element_blank(), axis.text.y = element_blank(), panel.grid.minor = element_blank(), panel.grid.major = element_blank(), axis.text.x = element_text(angle = 90) ) + scale_x_date(breaks = data$date, date_labels = &quot;%b&quot;) p3 Cuối cùng, ta có thể nhóm hai biểu đồ trên với grid &amp; gridExtra. grid.newpage() # Xác định vị trí cho biểu đồ chính position_1 &lt;- viewport(width = 1, height = 1, x = 0.5, y = 0.5) # Vị trí cho biểu đồ phụ position_2 &lt;- viewport(width = 0.35, height = 0.25, x = 0.25, y = 0.75) print(p2, vp = position_1) print(p3, vp = position_2) "],
["more-to-say.html", "A More to Say", " A More to Say Yeah! I have finished my book, but I have more to say about some topics. Let me explain them in this appendix. To know more about bookdown, see https://bookdown.org. "],
["tai-liu-tham-khao.html", "Tài liệu tham khảo", " Tài liệu tham khảo "]
]
