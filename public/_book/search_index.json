[
["li-m-u.html", "Phân tích dữ liệu thực tế với R Lời mở đầu", " Phân tích dữ liệu thực tế với R Hoàng Đức Anh 2019-07-11 Lời mở đầu Trong quá trình triển khai công việc thực tế tại nhiều tổ chức khác nhau, tôi nhận thấy một hiện thực là mặt bằng kiến thức thực tế về phân tích dữ liệu trên thị trường Việt Nam còn rất yếu. Phần lớn, các bạn làm trong ngành phân tích dữ liệu rơi vào một trong hai nhóm sau: Nhóm một, chưa có nền tảng về phân tích thống kê. Ở nhóm này, các bạn thường không được đào tạo bài bản hoặc không có đủ điều kiện (phần lớn là về thời gian) để học các kiến thức về phân tích thống kê. Do nhu cầu của công việc, các bạn có thành thạo các kỹ năng về xây dựng báo cáo và phân tích khám phá dữ liệu đơn giản với SQL và Excel. Các bạn này thường thuộc các nhóm phân tích báo cáo (Business Intelligence) tại các tổ chức lớn hoặc làm trong startup. Điểm mạnh của nhóm này là làm việc sát với các bộ phận kinh doanh, hiểu rõ nghiệp vụ và nhu cầu nghiệp vụ. Tuy nhiên, điểm yếu của các bạn lại là không thể ứng dụng hoặc tự học và không biết cách triển khai các ứng dụng của khoa học dữ liệu ở mức độ cao vào công việc thực tế. Nhóm hai, có nền tảng vững vàng về kiến thức thống kê, dự báo nhưng lại quá chú trọng vào các yếu tố kỹ thuật. Ở nhóm này, các bạn đều có nền tảng kiến thức về toán, thống kê rất tốt. Một số bạn được học và đào tạo cơ bản về khoa học dữ liệu, học máy và các ứng dụng của khoa học dữ liệu. Các bạn này có thiên hướng thích xây dựng mô hình dự báo, thích làm các bài toán lớn trong kinh doanh. Điểm mạnh của nhóm này là rất thông minh, chịu khó học hỏi và có thể áp dụng những kỹ thuật phân tích mới vào thực tế một cách nhanh chóng. Nhưng ngược lại, nhóm này lại có nhược điểm chết người là có thói quen chỉ tập trung vào việc phân tích dữ liệu mà thiếu đi cái nhìn tổng quát trong việc giải quyết bài toán thực tế. Không chỉ thế, nhóm này không có thế mạnh trong việc trình bày và giao tiếp, dẫn đến các kết quả thực tế không được các đơn vị kinh doanh nghiệp vụ đón nhận và sử dụng. Đối với một tổ chức muốn phát triển dựa vào dữ liệu và muốn biến các quyết định của tổ chức dựa vào phân tích dữ liệu, cả hai nhóm trên đều là các trạng thái nên tránh và phải cân bằng được cả hai. Cuốn sách này sẽ phân tích và giúp các bạn làm trong lĩnh vực phân tích dữ liệu hiểu rõ hơn các ưu nhược điểm của chính mình. "],
["intro.html", "Chương 1 Khoa học dữ liệu và nghề phân tích dữ liệu 1.1 Khoa học dữ liệu là gì 1.2 Tại sao phân tích dữ liệu là nghề khó? 1.3 Phân loại hoạt động phân tích dữ liệu 1.4 Cách xây dựng nhóm phân tích dữ liệu 1.5 Lưu ý khi đọc tài liệu", " Chương 1 Khoa học dữ liệu và nghề phân tích dữ liệu 1.1 Khoa học dữ liệu là gì Khoa học dữ liệu là nhóm ngành ứng dụng việc tổ chức và khai thác dữ liệu để hỗ trợ hoạt động ra quyết định. Phạm vi của khoa học dữ liệu rất rộng, từ việc tổ chức, quản lý, quản trị dữ liệu cho đến khai thác dữ liệu dưới dạng các báo cáo đơn giản cho đến các ứng dụng của Machine Learning, AI. 1.2 Tại sao phân tích dữ liệu là nghề khó? Phân tích dữ liệu đòi hỏi cùng lúc thực hiện ba nhóm công việc sau: Hiểu biết về vấn đề kinh doanh. Nghe có vẻ đơn giản nhưng qua quá trình làm việc thực tế, kinh nghiệm của tác giả cho thấy đây có lẽ là phần dễ bị bỏ qua nhất bỏi lẽ mấy nguyên nhân sau. Sự khác biệt của vận hành kinh doanh so với hoạt động phân tích dữ liệu. Tư duy và thái độ của nhóm phân tích dữ liệu. Các bạn phân tích kinh doanh (hoặc đôi khi được gọi là phân tích kinh doanh) thường tự coi mình là đơn vị hỗ trợ và cung cấp dữ liệu theo yêu cầu. Với lối tư duy thụ động này, các bạn sẽ không có nhu cầu tìm hiểu cặn kẽ các hoạt động kinh doanh, dẫn đến không hiểu hoạt động kinh doanh đủ sâu để có thể tư vấn và thuyết phục các bên kinh doanh trong việc triển khai các dự án phân tích dữ liệu mới. Nghiệp vụ kinh doanh rất phức tạp và thiếu hệ thống tài liệu ghi chép dưới góc độ khái quát cho hoạt động phân tích dữ liệu. Đây là vấn đề phần lớn các nhóm phân tích dữ liệu gặp phải. Để giải quyết vấn đề này, trong phần sau tác giả sẽ đưa ra phương pháp tìm hiểu hoạt động kinh doanh theo 6 nhóm vấn đề. Khai thác và phân tích dữ liệu Trình bày, thuyết phục và tư vấn cho các bên kinh doanh về kết quả phân tích dữ liệu. Ba cấp độ của viết code: Khi sử dụng các công cụ phân tích dữ liệu (viết code), ta sẽ trải qua 3 cấp độ như sau: Viết thứ đơn giản. Ở cấp độ này, các bạn thường mới nhập môn phân tích dữ liệu, đầy lo lắng và thiếu tự tin ở bản thân và bắt đầu với những bài phân tích đơn giản để áp dụng các kiến thức mới học. Viết càng nguy hiểm càng tốt. Ở giai đoạn này, các bạn đã có một lượng kiến thức nền tương đối vững và bắt đầu đi vào các phương pháp nâng cao. Do đó, các bạn thường có xu hướng khiến mọi thứ trở nên nguy hiểm, tô vẽ và đưa ra nhiều yếu tố không thực sự cần thiết. Với các bạn có xu hướng trực quan hóa, sẽ là đưa ra các biểu đồ đầy màu sắc và cực kỳ nguy hiểm. Với các bạn có xu hướng xây dựng mô hình dự báo, sẽ là dùng mô hình dự báo, học máy hoặc deep-learning mọi lúc, mọi nơi. Kết quả sẽ khiến người đọc ấn tượng nhưng có thể chưa thực sự có tính ứng dụng cao. Chỉ viết những thứ có khả năng tái sử dụng, giải quyết vấn đề bằng phương pháp đơn giản nhất có thể có. Ở giai đoạn này, các bạn đã dung hòa được rất nhiều kiến thức của nghành phân tích dữ liệu với nhau và tiếp cận các vấn đề một cách mạch lạc, logic và rất chặt chẽ. Các bạn nắm rất vững khi nào nên dùng các phương pháp phân tích khám phá dữ liệu đơn giản thay cho các thuật toán phức tạp trong việc giải quyết các vấn đề kinh doanh. Công thức tổng quát khi viết code: Khi phân tích dữ liệu (hay bất cứ hoạt động nào cần phải viết code), công thức tổng quát cần phải ghi nhớ là: \\[e = mc^2 \\Leftrightarrow error = ({more\\;code})^2\\] Do đó, càng viết đơn giản, khả năng chúng ta giảm thiểu được các sai sót trong quá trình phân tích dữ liệu càng nhiều. 1.3 Phân loại hoạt động phân tích dữ liệu 1.3.1 Dựa theo cách thức tác động đến khách hàng Khi làm việc thực tế, có nhiều nhánh trong hoạt động phân tích dữ liệu dẫn tới việc gây hiểu nhầm giữa các khía cạnh khác nhau. Khái quát hóa có thể chia làm hai nhánh lớn sau. Phân tích dữ liệu để tác động lên khách hàng một cách trực tiếp. Với cách tiếp cận này, để có thể tác động trực tiếp lên khách hàng, kết quả của hoạt động phân tích dữ liệu phải được đưa vào thực tế dưới dạng API và tác động trực tiếp lên UI mà người dùng sử dụng. Các hệ thống recommendation engine trên các trang thương mại điện tử thuộc loại này Phân tích dữ liệu để tác động lên khách hàng thông qua những người ra quyết định. Đây là nhóm được sử dụng đặc biệt nhiều trong thực tế nhưng lại bị đánh giá thấp hơn so với nhóm đầu tiên. Nhánh ứng dụng phân tích dữ liệu này được dùng đặc biệt nhiều trong các tổ chức lớn như ngân hàng, bảo hiểm… Trong nhóm này lại tách thành hai nhóm nhỏ: Kết quả phân tích được thể hiện dưới dạng trình chiếu - công cụ thường hay được sử dụng nhất là powerpoint Kết quả được sử dụng dưới dạng phần mềm (data tools). Với dạng này, người ra quyết định sẽ sử dụng các kết quả từ các công cụ này để đánh giá và ra quyết định thay đổi chính sách, sản phẩm cho phù hợp với khách hàng. Các công cụ như Google Analytics, Tableau… thuộc dạng này. Lưu ý: Đối với mô hình nâng cao hơn, kết quả phân loại, dự báo có thể được hiển thị trên các công cụ này và khiến người sử dụng dễ dàng hơn trong việc ra quyết định. Nguồn dữ liệu sử dụng để phân tích có thể tách ra thành hai nhóm lớn: Dữ liệu trong nội bộ doanh nghiệp. Nguồn dữ liệu này thể hiện phần lớn hành vi, thói quen của khách hàng đối với sản phẩm, dịch vụ do doanh nghiệp nắm giữ. Tổ chức càng lớn, nguồn dữ liệu này càng khó được khai thác hết nếu không có hệ thống Data Warehouse tốt do gặp phải vấn đề phân tán và chất lượng dữ liệu. Các hệ thống như T24, W4, EBank, LOS của ngân hàng, Google Analytics trên website/apps… thuộc vào nhóm này Dữ liệu bên ngoài doanh nghiệp. Nguồn dữ liệu không do doanh nghiệp quản lý, phản ánh hành vi của khách hàng nói chung. Ví dụ - social network, blog, giao dịch trên các trang thương mại điện tử, etc. Mục tiêu toàn bộ của hoạt động phân tích dữ liệu là sử dụng dữ liệu, thông qua phân tích, dự báo để tác động lên hành vi khách hàng nhằm gia tăng giá trị của sản phẩm, dịch vụ của doanh nghiệp đối với khách hàng. Khái quát hóa hoạt động phân tích dữ liệu có thể thể hiện qua sơ đồ sau. 1.3.2 Dựa theo đặc tính của nghề phân tích dữ liệu Nếu bám sát vào hoạt động của sơ đồ phía trên, ta có thể chia các hoạt động có liên quan đến khoa học dữ liệu thành 4 nhóm lớn. Data Governance - Quản trị dữ liệu: Quản trị các hoạt động ghi nhận dữ liệu vào hệ thống của tổ chức, đưa ra các quy định, quy trình để đảm bảo dữ liệu được đưa vào hệ thống một cách chính xác và đảm bảo yêu cầu của hoạt động kinh doanh. Ví dụ, dữ liệu địa chỉ của khách hàng phải để dạng dropdown theo các đơn vị hành chính để tránh sai sót (quận/huyện, phường/xã, tỉnh, thành phố) Data Management - Quản lý dữ liệu: Quản lý dữ liệu sau khi dữ liệu được đưa vào hệ thống của tổ chức, bao gổm làm sạch, tái cấu trúc, sắp xếp dữ liệu theo hệ thống để đảm bảo sẵn sàng sử dụng dữ liệu Business Intelligence: Khai thác dữ liệu để hỗ trợ hoạt động kinh doanh và quá trình ra quyết định. Đối với hoạt động BI, có thể chia thành ba cấp độ. Data provider - cung cấp dữ liệu: Đơn vị kinh doanh/ nghiệp vụ yêu cầu như thế nào, cung cấp chính xác như vậy Information &amp; basic insight provider - cung cấp thông tin &amp; các hiểu biết đơn giản về khách hàng và hoạt động kinh doanh: Đưa ra các nhận định, thông tin ở mức độ đơn giản nhưng rất hữu dụng cho hoạt động kinh doanh. Công cụ sử dụng cho cấp độ này phần lớn là Excel với pivot table đi kèm với các chỉ số thống kê cơ bản. Ví dụ, top 10% khách hàng chiếm đến 90% doanh thu của cả công ty. Advanced insights provider - cung cấp các hiểu biết sâu sắc về khách hàng và hoạt động kinh doanh. Để đưa ra các hiểu biết này, phải sử dụng nhiều đến các công cụ và mô hình thuộc nhóm inference nhằm tác động đến hoạt động kinh doanh. Ví dụ, khách hàng nếu có ít nhất 3 loại giao dịch trong 2 tháng đầu tiên có khả năng ở lại với ngân hàng cao hơn 20% so với các khách hàng khác. Predictive Modelling/ Optimization - Phân tích dự báo, tối ưu hóa. Ở mức độ này, công việc phân tích dữ liệu sẽ tập trung nhiều vào việc dự báo và tối ưu hóa. Ví dụ: Khách hàng nào là khách hàng có nhiều khả năng bán chéo được sản phẩm thấu chi? Các công cụ sử dụng trong cấp độ này là các phần mềm chuyên biệt về phân tích thống kê, dự báo như R/Python/SAS. 1.3.3 Dựa theo chu kỳ của khách hàng đối với sản phẩm Xét về khía cạnh vòng đời sản phẩm, chu kỳ khách hàng, bất cứ hoạt động kinh doanh nào cũng có thể chia thành chu kỳ như sau. Tim kiếm khách hàng - acquisition: Giai đoạn thu hút khách hàng mới, mục tiêu của giai đoạn này là đưa khách hàng mới về doanh nghiệp. Giai đoạn này, khách hàng chưa đem lại doanh số mà phần lớn chỉ là ở mức đăng ký dịch vụ, sản phẩm và trải nghiệm. Kích hoạt khách hàng - activation: Sau khi một người đã trở thành khách hàng của doanh nghiệp, không có nghĩa người đó sẽ sử dụng sản phẩm dịch vụ mà thường phải có quá trình on-boarding. Mục tiêu của giai đoạn này không chỉ là để khách hàng có những trải nghiệm đầu tiên với sản phẩm dịch vụ mà còn khiến các sản phẩm đó trở nên quen thuộc và khách hàng bắt đầu thực sự sử dụng sản phẩm dịch vụ. Đây là giai đoạn rất nhiều doanh nghiệp lớn quên không sử dụng và đưa vào vận hành, khiến khách hàng vừa ở giai đoạn acquisition đã chuyển sang giai đoạn churn Khai thác khách hàng - deep farming: Chỉ sau khi khách hàng thực sự sử dụng sản phẩm, dịch vụ, lúc này doanh nghiệp mới có thể tiếp tục khai thác khách hàng. Thông thường có hai nhóm là bán chéo (cross-selling) và bán thêm (up-selling). Giữ chân khách hàng - retaining customers: Sau một thời gian sử dụng sản phẩm, dịch vụ, khách hàng lúc này sẽ có xu hướng rời bỏ doanh nghiệp để sử dụng các sản phẩm khác cạnh tranh hơn. Do đó, mục tiêu phân tích trong giai đoạn này là dự báo, tìm kiếm và hỗ trợ các hoạt động kinh doanh giúp giảm churn Thu hồi nợ - collection/bad bank: Giai đoạn này thường chỉ sử dụng đối với các doanh nghiệp có hoạt động cho vay như ngân hàng. Mục tiêu phân tích là giảm tỷ lệ nợ, tỷ lệ nợ xấu Ngoài các giai đoạn lớn trên, hoạt động phân tích dữ liệu còn tập trung vào 2 mảng lớn: Giữ chân khách hàng chủ động - proactively retaining customers: Phân tích để giữ chân khách hàng chủ động, ngay từ khi khách hàng chuyển qua giai đoạn có thể khai thác được. Nhiều doanh nghiệp lớn chưa chú trọng đến vấn đề này mà thường để đến khi khách hàng sắp rời bỏ doanh nghiệp mới tìm cách giữ chân. Khi đó, phần lớn mọi việc đã trở nên quá muộn. Phân nhóm khách hàng - customers segmentation: Phân nhóm khách hàng theo các đặc trưng về hành vi, tính chất để có thể đưa ra các sản phẩm phù hợp 1.4 Cách xây dựng nhóm phân tích dữ liệu Để xây dựng nhóm phân tích dữ liệu, cần đảm bảo 3 nguyên tắc lớn sau: Nắm vững kỹ năng phân tích khám phá dữ liệu Bám sát hoạt động kinh doanh Đẩy mạnh tương tác nhóm &amp; phân nhóm kỹ năng dựa trên sở thích và tính cách cá nhân. Thứ nhất, nền tảng của toàn bộ hoạt động phân tích dữ liệu (Analytics) phải giải quyết được bài toán kinh doanh dưới góc độ phân tích dữ liệu. Do đó, không nhất thiết phải xây dựng các mô hình phức tạp để giải quyết bài toán. Ngược lại, trong nhiều tình huống, ta nên sử dụng các phương pháp đơn giản bởi hai lý do: thứ nhất - có thể giải thích cho kinh doanh và thứ hai - rất nhiều vấn đề không cần xây mô hình cũng có thể giải quyết ngay được bởi các nguyên nhân từ vận hành, con người và sản phẩm. Chỉ khi nắm vững được kỹ năng phân tích khám phá dữ liệu ta mới có thể đưa ra các nhận định và kết quả một cách nhanh chóng và chính xác hỗ trợ hoạt động kinh doanh. Thứ hai, hoạt động phân tích được sinh ra nhằm phục vụ kinh doanh. Do đó, nếu không nắm vững đến một mức độ nhất định hoạt động kinh doanh, ta không thể đưa ra các kết luận thực tế và có thể triển khai được. Rất nhiều trường hợp, nhóm phân tích dữ liệu quá chú trọng đến mặt kỹ thuật và thuật toán, đưa ra những kết luận phi thực tế. Điều này cần tránh dối với các tổ chức hướng đến việc ứng dụng phân tích dữ liệu thưc tiễn hỗ trợ kinh doanh. Thứ ba, các nhà phân tích dữ liệu thường khá thông minh và thích làm việc tương đối độc lập. Không chỉ thế, có rất nhiều các khía cạnh khác nhau trong hoạt động phân tích dữ liệu. Có người ưa thích thuật toán và kiên trì và có thể tự học tốt nhưng lại yếu trong khâu giao tiếp và tương tác. Nhóm này nên khuyến khích phát triển các bài toán và vấn đề dự báo. Ngược lại, có nhóm rất thông minh, nhanh nhạy, thích thể hiện trước đông người nhưng lại thường xuyên cả thèm chóng chán. Nhóm này nên phát triển các kỹ năng khám phá dữ liệu, tìm kiếm insights và nên giao cho nhiệm vụ tương tác chính với kinh doanh. Ngoài ra, còn có nhóm trunh bình, việc gì cũng có thể làm ở mức độ khá nhưng không có việc gì thực sự nổi trội. Với nhóm này nên cố gắng bồi dưỡng để họ phát huy kiến thức tổng thể và hướng họ có thể back-up hai nhóm trên. Thêm vào đó, việc phân rã kỹ năng còn giúp cho kiến thức không bị quá tập trung vào 1-2 thành viên, điều này có thể khiến cho hoạt động phân tích dữ liệu bị chậm lại do gặp phải vấn đề bottle-neck của những thành viên đó. 1.5 Lưu ý khi đọc tài liệu Việc viết cuốn sách này xuất phát thuần túy từ nhu cầu cá nhân của tác giả. Trong quá trình làm việc thực tế, bản thân tác giả luôn có mong muốn tổng hợp và đúc rút các kiến thức phân tích thực tế. Tuy nhiên, khi bắt đầu bắt tay vào xây dựng cuốn sách, tác giả cũng có mong muốn có thể đúc kết và giúp cho các tổ chức, bộ phận muốn tập trung vào và khai thác sức mạnh của phân tích dữ liệu trong thực tế có thể có thêm các nguồn tài liệu và kinh nghiệm, vốn rất ít được chia sẻ thực tế, để có thể thành công hơn trong công việc triển khai. Thêm vào đó, trong quá trình làm việc và giảng dạy cho các học viên, tác giả luôn nhận được câu hỏi: Thưa thày, em không được đào tạo bài bản về phân tích thống kê cũng như khoa học dữ liệu, liệu em có thể trở thành chuyên gia phân tích dữ liệu được không? Tác giả luôn trăn trở với câu hỏi trên cũng như với kinh nghiệm đào tạo thực tế và xây dựng năng lực phân tích dữ liệu ở VPBank, quyển sách này được viết ra theo cách tiếp cận ứng dụng của khoa học dữ liệu trong hoạt động kinh doanh. Do đó, ngôn ngữ cũng như cách tiếp cận trong cuốn sách này được cố gắng viết một cách đơn giản, dễ hiểu để trình bày các kiến thức, thuật ngữ khó hiểu của khoa học dữ liệu thành các ngôn ngữ bình dân phù hợp với nhiều đối tượng. Cuốn sách này sẽ không đi sâu vào lý thuyết của các thuật toán, mô hình thống kê mà sẽ cố gắng trả lời các khía cạnh sau. Thuật toán, mô hình đó là gì? Mô hình đó được sử dụng như thế nào? Khi giải thích cho các đơn vị kinh doanh, ta cần phải giải thích điều gì? Ứng dụng của mô hình trong thực tế "],
["ng-phap-cua-bin-i-d-liu-vi-dplyr.html", "Chương 2 Ngữ pháp của biến đổi dữ liệu với DPLYR 2.1 Giới thiệu về pipe operator 2.2 Các hàm cơ bản trong dplyr 2.3 Các hàm nâng cao trong dplyr", " Chương 2 Ngữ pháp của biến đổi dữ liệu với DPLYR Khi bắt tay vào công việc phân tích số liệu, việc đầu tiên ta cần phải làm là thu thập dữ liệu từ nhiều nguồn khác nhau. Sau khi hoàn thành xong bước này, ta sẽ phải dành phần lớn thời gian để làm sạch, biến đổi và tổng hợp dữ liệu nhằm tìm kiếm các insights hoặc chuẩn bị dữ liệu cho các bước xây dựng mô hình, dự báo. R rất mạnh trong việc biến đối dữ liệu và có rất nhiều package hỗ trợ cho công việc này. Tuy nhiên, thư viện nổi tiếng nhất trong R trong việc làm sạch và biến đổi dữ liệu là dplyr, một thư viện nổi tiếng với những tính năng chuyên cho việc xử lý, tổng hợp dữ liệu trước khi xây dựng mô hình phân tích dữ liệu. Chương này sẽ tập trung vào giới thiệu về những hàm cơ bản nhất của dplyr. Trước khi bắt đầu nội dung bài giảng, chúng ta có thể download và gọi gói dplyr. #install.packages(&quot;dplyr&quot;) library(dplyr) 2.1 Giới thiệu về pipe operator Khi viết các câu lệnh, thông thường ta có 2 cách viết phổ biến sau. Cách 1: Viết với các câu lệnh lồng vào nhau (nested). Với cách viết này, các hàm sẽ được viết lồng vào nhau và kết quả của hàm sẽ được tính toán theo thứ tự từ trong ra ngoài. Cách 2: Viết lưu dưới dạng các đối tượng trung gian. Với cách viết này, từng đối tượng sẽ được tính toán từng phần và kết quả sẽ được hiển thị một cách mạch lạc hơn. Tuy nhiên, nhược điểm của phương pháp này là sẽ tạo ra rất nhiều đối tượng trung gian, gây ra khó khăn trong việc theo dõi và quản lỹ. Giả sử ta cần tính toán độ lệch chuẩn của véc-tơ x, công thức tính độ lệch chuẩn sẽ là \\[\\sigma = \\frac{\\sum_{i=1}^n(x_i-\\overline{x})^2}{n-1}\\] Với hai cách viết code khác nhau, ta có thể tính độ lệch chuẩn theo hai cách. # Cách 1 # Tạo vector x x &lt;- seq(2, 100, 2) # Tính độ lệch chuẩn sqrt(sum((x-mean(x))^2)/(length(x)-1)) ## [1] 29.15476 sd(x) ## [1] 29.15476 # Cách 2 # Tạo vector x x &lt;- seq(2, 100, 2) # Tính tổng bình phương sum_sqr &lt;- sum((x-mean(x))^2) len_x &lt;- length(x) var &lt;- sum_sqr/(len_x - 1) sd &lt;- var^(1/2) sd ## [1] 29.15476 Ta thấy kết quả ở hai cách tính là như nhau. Tuy nhiên, cách viết hai sẽ tạo ra nhiều đối tượng trung gian hơn cách viết 1 rất nhiều. Khi phân tích dữ liệu thực tế, ta sẽ phải áp dụng cả 2 cách viết code để có thể vận dụng linh hoạt trong từng trường hợp cụ thể. Trong R, có cách viết code thứ ba, được gọi là cách suwr dụng pipe operator (%&gt;%). Toán tử Pipe cho phép viết code theo cách đơn giản và dễ theo dõi giúp cho người đọc và người viết code trên R có thể theo dõi được code một cách dễ dàng nhất. Câu trúc của pipe như sau f(x, y) = x %&gt;% f(., y) Ví dụ của pipe. # Cách 1 mean(x) # Cách 2 x %&gt;% mean Ta có thể xem xét ví dụ phức tạp hơn. # Cách 1 - dùng cách viết thường summary(head(iris)) ## Sepal.Length Sepal.Width Petal.Length Petal.Width ## Min. :4.600 Min. :3.000 Min. :1.300 Min. :0.2000 ## 1st Qu.:4.750 1st Qu.:3.125 1st Qu.:1.400 1st Qu.:0.2000 ## Median :4.950 Median :3.350 Median :1.400 Median :0.2000 ## Mean :4.950 Mean :3.383 Mean :1.450 Mean :0.2333 ## 3rd Qu.:5.075 3rd Qu.:3.575 3rd Qu.:1.475 3rd Qu.:0.2000 ## Max. :5.400 Max. :3.900 Max. :1.700 Max. :0.4000 ## Species ## setosa :6 ## versicolor:0 ## virginica :0 ## ## ## # Cách 2 - dùng pipe iris %&gt;% head %&gt;% summary ## Sepal.Length Sepal.Width Petal.Length Petal.Width ## Min. :4.600 Min. :3.000 Min. :1.300 Min. :0.2000 ## 1st Qu.:4.750 1st Qu.:3.125 1st Qu.:1.400 1st Qu.:0.2000 ## Median :4.950 Median :3.350 Median :1.400 Median :0.2000 ## Mean :4.950 Mean :3.383 Mean :1.450 Mean :0.2333 ## 3rd Qu.:5.075 3rd Qu.:3.575 3rd Qu.:1.475 3rd Qu.:0.2000 ## Max. :5.400 Max. :3.900 Max. :1.700 Max. :0.4000 ## Species ## setosa :6 ## versicolor:0 ## virginica :0 ## ## ## Cả hai cách đều cho ra kết quả giống nhau. Tuy nhiên, cách hai sẽ dễ theo dõi, dễ đọc hơn cách 1 rất nhiều. Cách đọc hiểu quá trình thực hiện pipe như sau: Gọi tập dữ liệu iris để phân tích Thực hiện hàm head trên tập dữ liệu này, được kết quả bao nhiêu… … tiếp tục thực hiện hàm summary Như ta thấy, cách viết theo phong cách của pipe operator (%&gt;%) cho phép ta thực hiện các phép tính theo đúng mạch tư duy logic của bản thân. Điều này là một điểm rất mạnh mà hiện tại, mới chỉ ở R có toán tử %&gt;% áp dụng được cho mọi hàm. Một số đặc tính cơ bản của toán tử pipe: Theo mặc định, Phía tay trái (LHS) sẽ được chuyển tiếp thành yếu tố đầu tiên của hàm được sử dụng phía tay phải (RHS), ví dụ: mean(x) ## [1] 51 # Tương đương với: x %&gt;% mean ## [1] 51 Khi LHS không còn là yếu tố đầu tiên của một hàm RHS, thì dấu “.” được sử dụng để định vị cho LHS, ví dụ: library(dplyr) # Cách 1 summary(lm(mpg ~ cyl, data = mtcars)) # Cách 2 mtcars %&gt;% lm(mpg ~ cyl, data = .) %&gt;% summary Trong tình huống trên, tham số về dữ liệu trong hàm lm không phải là ở đầu, mà sau phần công thức, nên chúng ta sẽ dùng dấu “.” như là đại diện của thực thể mtcars ở bên ngoài (LHS) của hàm lm. 2.2 Các hàm cơ bản trong dplyr Trong công việc biến đổi dữ liệu, bất kỳ ngôn ngữ phân tích nào cũng có 3 nhóm hàm lớn. Nhóm 1 - các hàm truy vấn dữ liệu: Lấy dữ liệu theo dòng, theo cột và theo điều kiện. Trong dplyr sẽ là các hàm select, filter và slice Nhóm 2 - Các hàm tổng hợp dữ liệu: Tính toán tổng hợp dữ liệu theo chiều. Trong dplyr sẽ là các hàm group_by, summarise Nhóm 3 - Các hàm biến đổi dữ liệu: Tạo mới, biến đổi các dữ liệu cũ thành các dữ liệu mới. Trong dplyr sẽ là các hàm thuộc nhóm mutate, join, bind Trong phần này, chúng ta sẽ giới thiệu nhanh các nhóm câu lệnh cơ bản trên. 2.2.1 Nhóm câu lệnh truy vấn dữ liệu Khi truy vấn dữ liệu, ta thường phải thực hiện 3 nhóm công việc sau. Lấy theo cột Lấy theo dòng Lấy theo điều kiện Xét về mặt bản chất, lấy theo điều kiện là một trường hợp đặc biệt của việc lấy theo dòng. Đối với các ngôn ngữ như SQL, sẽ không phân biệt hai loại này. Tuy nhiên, vì R lưu thứ tự của từng quan sát trong dataframe, nên việc phân biệt được hai loại truy vấn trên là cần thiết. 2.2.1.1 Lấy các cột trong dataframe với select data %&gt;% select(var1, var2, ...) Trong đó, var1, var2 là tên các cột cần truy vấn. Đặc biệt, R rất linh hoạt trong việc lọc theo cột. Ta có thể truy vấn theo tên, theo thứ tự hoặc thậm chí theo các khoảng thứ tự các biến. Xem ví dụ sau. library(dplyr) # Xêm tên các biến trong mtcars mtcars %&gt;% names ## [1] &quot;mpg&quot; &quot;cyl&quot; &quot;disp&quot; &quot;hp&quot; &quot;drat&quot; &quot;wt&quot; &quot;qsec&quot; &quot;vs&quot; &quot;am&quot; &quot;gear&quot; ## [11] &quot;carb&quot; # Chọn cột mpg và cyl mtcars %&gt;% select(mpg, cyl) %&gt;% head ## mpg cyl ## Mazda RX4 21.0 6 ## Mazda RX4 Wag 21.0 6 ## Datsun 710 22.8 4 ## Hornet 4 Drive 21.4 6 ## Hornet Sportabout 18.7 8 ## Valiant 18.1 6 # Chọn cột thứ nhất và thứ hai mtcars %&gt;% select(1,2) %&gt;% head ## mpg cyl ## Mazda RX4 21.0 6 ## Mazda RX4 Wag 21.0 6 ## Datsun 710 22.8 4 ## Hornet 4 Drive 21.4 6 ## Hornet Sportabout 18.7 8 ## Valiant 18.1 6 # Chọn cột thứ 3 đến cột thứ 6 mtcars %&gt;% select(3:6) %&gt;% head ## disp hp drat wt ## Mazda RX4 160 110 3.90 2.620 ## Mazda RX4 Wag 160 110 3.90 2.875 ## Datsun 710 108 93 3.85 2.320 ## Hornet 4 Drive 258 110 3.08 3.215 ## Hornet Sportabout 360 175 3.15 3.440 ## Valiant 225 105 2.76 3.460 Ngoài ra, khi lấy chi tiết các cột (liệt kê từng cột) khi lấy dữ liệu trên 1 bảng, bạn có thể dùng một số hàm sau để hỗ trợ việc lấy trường dữ liệu được nhanh hơn: starts_with(\"Ký tự là thông tin mong muốn\"): các cột dữ liệu ccó tên hứa các ký tự mong muốn đứng ở đầu của tên, ví dụ: iris %&gt;% select(starts_with(&quot;Petal&quot;)) %&gt;% head ## Petal.Length Petal.Width ## 1 1.4 0.2 ## 2 1.4 0.2 ## 3 1.3 0.2 ## 4 1.5 0.2 ## 5 1.4 0.2 ## 6 1.7 0.4 ends_with(\"Ký tự là thông tin mong muốn\"): các cột dữ liệu có tên chứa các ký tự mong muốn ở cuối của tên, ví dụ: iris %&gt;% select(ends_with(&quot;Length&quot;)) %&gt;% head ## Sepal.Length Petal.Length ## 1 5.1 1.4 ## 2 4.9 1.4 ## 3 4.7 1.3 ## 4 4.6 1.5 ## 5 5.0 1.4 ## 6 5.4 1.7 contains(\"Ký tự là thông tin mong muốn\"): các cột dữ liệu có tên chứa chính xác các ký tự mong muốn ở bất kỳ vị trí nào của tên, ví dụ: iris %&gt;% select(contains(&quot;etal&quot;)) %&gt;% head ## Petal.Length Petal.Width ## 1 1.4 0.2 ## 2 1.4 0.2 ## 3 1.3 0.2 ## 4 1.5 0.2 ## 5 1.4 0.2 ## 6 1.7 0.4 matches(“Dạng ký tự là thông tin mong muốn”): các cột dữ liệu có tên chứa các ký tự có dạng ký tự mong muốn ở bất kỳ vị trí nào của tên, ví dụ: iris %&gt;% select(matches(&quot;.t.&quot;)) %&gt;% head ## Sepal.Length Sepal.Width Petal.Length Petal.Width ## 1 5.1 3.5 1.4 0.2 ## 2 4.9 3.0 1.4 0.2 ## 3 4.7 3.2 1.3 0.2 ## 4 4.6 3.1 1.5 0.2 ## 5 5.0 3.6 1.4 0.2 ## 6 5.4 3.9 1.7 0.4 Trong ví dụ trên, R sẽ lấy tất cả các cột có tên chứa chữ t và có ký tự khác ở trước và sau (các ký tự chỉ chứa chữ t mà chữ t ở đâu hoặc cuối tên sẽ không được tính vào) Thêm vào đó, ta có thể đổi tên biến ngay trong khi lựa chọn các biến với select như sau: mtcars %&gt;% select(`miles per gallon` = mpg , cylinder = cyl , weight = wt) %&gt;% head ## miles per gallon cylinder weight ## Mazda RX4 21.0 6 2.620 ## Mazda RX4 Wag 21.0 6 2.875 ## Datsun 710 22.8 4 2.320 ## Hornet 4 Drive 21.4 6 3.215 ## Hornet Sportabout 18.7 8 3.440 ## Valiant 18.1 6 3.460 2.2.1.2 Lấy các dòng trong dataframe với slice data %&gt;% slice(observation) Tương tự như lấy theo cột, ta có thể lấy các dòng trong một dataframe. Tuy nhiên, lưu ý hàm slice chỉ cho phép điều kiện lấy quan sát là một véc-tơ. Xem ví dụ sau. # Lấy dòng đầu tiên mtcars %&gt;% slice(1) ## mpg cyl disp hp drat wt qsec vs am gear carb ## 1 21 6 160 110 3.9 2.62 16.46 0 1 4 4 # Lấy dòng từ 1:3 mtcars %&gt;% slice(1:3) ## mpg cyl disp hp drat wt qsec vs am gear carb ## 1 21.0 6 160 110 3.90 2.620 16.46 0 1 4 4 ## 2 21.0 6 160 110 3.90 2.875 17.02 0 1 4 4 ## 3 22.8 4 108 93 3.85 2.320 18.61 1 1 4 1 # Lấy dòng 1:3 và 5 mtcars %&gt;% slice(c(1:3,5)) ## mpg cyl disp hp drat wt qsec vs am gear carb ## 1 21.0 6 160 110 3.90 2.620 16.46 0 1 4 4 ## 2 21.0 6 160 110 3.90 2.875 17.02 0 1 4 4 ## 3 22.8 4 108 93 3.85 2.320 18.61 1 1 4 1 ## 4 18.7 8 360 175 3.15 3.440 17.02 0 0 3 2 2.2.1.3 Lọc quan sát theo điều kiện với filter data %&gt;% filter(condition) Hàm filter cho phép ta sử dụng các điều kiện phức tạp để truy xuất dữ liệu từ dataframe. Các điều kiện thường dùng bao gồm. Dấu Ký hiệu Ví dụ Bằng == 7==8 Khác != 7!=8 Lớn hơn &gt; a &gt; b Lớn hơn hoặc bằng &gt;= a &gt;= b Nhỏ hơn &lt; a &lt; b Nhỏ hơn hoặc bằng &lt;= a &lt;= b Và &amp; a &gt; 7 &amp; b &lt; 9 Xem ví dụ sau. # Lọc điều kiện mpg &gt; 20 mtcars %&gt;% filter(mpg &gt; 20) %&gt;% dim ## [1] 14 11 # Lọc điều kiện mpg &gt;20 hoặc mpg &lt;18 mtcars %&gt;% filter(mpg &gt; 20 | mpg &lt; 18) %&gt;% dim ## [1] 27 11 # Lọc điều kiện mpg &gt;=20 và cyl = 6 mtcars %&gt;% filter(mpg &gt; 20 &amp; cyl == 6) ## mpg cyl disp hp drat wt qsec vs am gear carb ## 1 21.0 6 160 110 3.90 2.620 16.46 0 1 4 4 ## 2 21.0 6 160 110 3.90 2.875 17.02 0 1 4 4 ## 3 21.4 6 258 110 3.08 3.215 19.44 1 0 3 1 Lưu ý: Khi điều kiện hoặc là chuỗi các giá trị rời rạc áp dụng cho cùng một trường, chúng ta có thể làm ngắn gọn hơn với cấu trúc “%in%” thay vì cấu phải liệt kê tất cả các điều kiện đơn lẻ và ngăn cách nhau bởi dấu “|”: mtcars %&gt;% filter(carb == 4 | carb == 3 | carb == 1) ## mpg cyl disp hp drat wt qsec vs am gear carb ## 1 21.0 6 160.0 110 3.90 2.620 16.46 0 1 4 4 ## 2 21.0 6 160.0 110 3.90 2.875 17.02 0 1 4 4 ## 3 22.8 4 108.0 93 3.85 2.320 18.61 1 1 4 1 ## 4 21.4 6 258.0 110 3.08 3.215 19.44 1 0 3 1 ## 5 18.1 6 225.0 105 2.76 3.460 20.22 1 0 3 1 ## 6 14.3 8 360.0 245 3.21 3.570 15.84 0 0 3 4 ## 7 19.2 6 167.6 123 3.92 3.440 18.30 1 0 4 4 ## 8 17.8 6 167.6 123 3.92 3.440 18.90 1 0 4 4 ## 9 16.4 8 275.8 180 3.07 4.070 17.40 0 0 3 3 ## 10 17.3 8 275.8 180 3.07 3.730 17.60 0 0 3 3 ## 11 15.2 8 275.8 180 3.07 3.780 18.00 0 0 3 3 ## 12 10.4 8 472.0 205 2.93 5.250 17.98 0 0 3 4 ## 13 10.4 8 460.0 215 3.00 5.424 17.82 0 0 3 4 ## 14 14.7 8 440.0 230 3.23 5.345 17.42 0 0 3 4 ## 15 32.4 4 78.7 66 4.08 2.200 19.47 1 1 4 1 ## 16 33.9 4 71.1 65 4.22 1.835 19.90 1 1 4 1 ## 17 21.5 4 120.1 97 3.70 2.465 20.01 1 0 3 1 ## 18 13.3 8 350.0 245 3.73 3.840 15.41 0 0 3 4 ## 19 27.3 4 79.0 66 4.08 1.935 18.90 1 1 4 1 ## 20 15.8 8 351.0 264 4.22 3.170 14.50 0 1 5 4 Câu lệnh trên tương đương với: mtcars %&gt;% filter(carb %in% c(1, 3, 4)) 2.2.1.4 Sắp xếp dữ liệu với arrange data %&gt;% arrange(var1, var2) Ngoài việc lọc dữ liệu có điều kiện, chúng ta cũng thường xuyên thực hiện việc sắp xếp dữ liệu theo một trật tự nhất định nào đó khi xem dữ liệu. Hàm arrange() hỗ trợ công việc này. Cách thức sắp xếp dữ liệu mặc định là từ nhỏ đến lớn. mtcars %&gt;% arrange(mpg) %&gt;% head ## mpg cyl disp hp drat wt qsec vs am gear carb ## 1 10.4 8 472 205 2.93 5.250 17.98 0 0 3 4 ## 2 10.4 8 460 215 3.00 5.424 17.82 0 0 3 4 ## 3 13.3 8 350 245 3.73 3.840 15.41 0 0 3 4 ## 4 14.3 8 360 245 3.21 3.570 15.84 0 0 3 4 ## 5 14.7 8 440 230 3.23 5.345 17.42 0 0 3 4 ## 6 15.0 8 301 335 3.54 3.570 14.60 0 1 5 8 Khi có nhiều biến cần được sắp xếp, hàm arrange sẽ ưu tiên các biến theo thứ tự từ trái sang phải. mtcars %&gt;% arrange(mpg, cyl) ## mpg cyl disp hp drat wt qsec vs am gear carb ## 1 10.4 8 472.0 205 2.93 5.250 17.98 0 0 3 4 ## 2 10.4 8 460.0 215 3.00 5.424 17.82 0 0 3 4 ## 3 13.3 8 350.0 245 3.73 3.840 15.41 0 0 3 4 ## 4 14.3 8 360.0 245 3.21 3.570 15.84 0 0 3 4 ## 5 14.7 8 440.0 230 3.23 5.345 17.42 0 0 3 4 ## 6 15.0 8 301.0 335 3.54 3.570 14.60 0 1 5 8 ## 7 15.2 8 275.8 180 3.07 3.780 18.00 0 0 3 3 ## 8 15.2 8 304.0 150 3.15 3.435 17.30 0 0 3 2 ## 9 15.5 8 318.0 150 2.76 3.520 16.87 0 0 3 2 ## 10 15.8 8 351.0 264 4.22 3.170 14.50 0 1 5 4 ## 11 16.4 8 275.8 180 3.07 4.070 17.40 0 0 3 3 ## 12 17.3 8 275.8 180 3.07 3.730 17.60 0 0 3 3 ## 13 17.8 6 167.6 123 3.92 3.440 18.90 1 0 4 4 ## 14 18.1 6 225.0 105 2.76 3.460 20.22 1 0 3 1 ## 15 18.7 8 360.0 175 3.15 3.440 17.02 0 0 3 2 ## 16 19.2 6 167.6 123 3.92 3.440 18.30 1 0 4 4 ## 17 19.2 8 400.0 175 3.08 3.845 17.05 0 0 3 2 ## 18 19.7 6 145.0 175 3.62 2.770 15.50 0 1 5 6 ## 19 21.0 6 160.0 110 3.90 2.620 16.46 0 1 4 4 ## 20 21.0 6 160.0 110 3.90 2.875 17.02 0 1 4 4 ## 21 21.4 4 121.0 109 4.11 2.780 18.60 1 1 4 2 ## 22 21.4 6 258.0 110 3.08 3.215 19.44 1 0 3 1 ## 23 21.5 4 120.1 97 3.70 2.465 20.01 1 0 3 1 ## 24 22.8 4 108.0 93 3.85 2.320 18.61 1 1 4 1 ## 25 22.8 4 140.8 95 3.92 3.150 22.90 1 0 4 2 ## 26 24.4 4 146.7 62 3.69 3.190 20.00 1 0 4 2 ## 27 26.0 4 120.3 91 4.43 2.140 16.70 0 1 5 2 ## 28 27.3 4 79.0 66 4.08 1.935 18.90 1 1 4 1 ## 29 30.4 4 75.7 52 4.93 1.615 18.52 1 1 4 2 ## 30 30.4 4 95.1 113 3.77 1.513 16.90 1 1 5 2 ## 31 32.4 4 78.7 66 4.08 2.200 19.47 1 1 4 1 ## 32 33.9 4 71.1 65 4.22 1.835 19.90 1 1 4 1 2.2.1.5 Đổi tên biến với rename data %&gt;% rename(new_var = old_var) mtcars %&gt;% rename(displacement = disp, miles_per_gallon = mpg) %&gt;% names ## [1] &quot;miles_per_gallon&quot; &quot;cyl&quot; &quot;displacement&quot; ## [4] &quot;hp&quot; &quot;drat&quot; &quot;wt&quot; ## [7] &quot;qsec&quot; &quot;vs&quot; &quot;am&quot; ## [10] &quot;gear&quot; &quot;carb&quot; 2.2.2 Nhóm câu lệnh biến đổi dữ liệu 2.2.2.0.1 Tạo mới trường dữ liệu với mutate Trong quá trình xử lý dữ liệu, ta thường xuyên phải tạo thêm các trường dữ liệu mới (trường dữ liệu phát sinh). Hàm mutate() được sử dụng để làm công việc này. Cấu trúc của hàm rất đơn giản như sau. data %&gt;% mutate(new_var = statement) Xem ví dụ sau: mtcars %&gt;% select(mpg) %&gt;% mutate(new_mpg = mpg * 2) %&gt;% head ## mpg new_mpg ## 1 21.0 42.0 ## 2 21.0 42.0 ## 3 22.8 45.6 ## 4 21.4 42.8 ## 5 18.7 37.4 ## 6 18.1 36.2 Trong một số trường hợp, khi ta không muốn lấy các trường thông tin cũ mà chỉ muốn lấy các trường thông tin mới tạo thì có thể sử dụng hàm transmute() với cấu trúc giống như hàm mutate. mtcars %&gt;% select(mpg) %&gt;% transmute(new_mpg = mpg * 1.61) %&gt;% head ## new_mpg ## 1 33.810 ## 2 33.810 ## 3 36.708 ## 4 34.454 ## 5 30.107 ## 6 29.141 2.2.2.0.2 Gộp nhiều bảng với nhóm hàm join Hàm inner_join(x, y, by = \"key\"): lấy tất cả dữ liệu có trên bảng hai bảng khi trùng key, ví dụ: x &lt;- data.frame(student_id = seq(1, 10, 1), maths = c(10, 8, 7, 6, 7.8, 4, 7.7, 9, 9.5, 6.5)) y &lt;- data.frame(student_id = seq(2, 20, 2), physics = c(8, 9.5, 7.5, 6, 5.5, 6.5, 7.8, 8.2, 8, 7.5)) x ## student_id maths ## 1 1 10.0 ## 2 2 8.0 ## 3 3 7.0 ## 4 4 6.0 ## 5 5 7.8 ## 6 6 4.0 ## 7 7 7.7 ## 8 8 9.0 ## 9 9 9.5 ## 10 10 6.5 y ## student_id physics ## 1 2 8.0 ## 2 4 9.5 ## 3 6 7.5 ## 4 8 6.0 ## 5 10 5.5 ## 6 12 6.5 ## 7 14 7.8 ## 8 16 8.2 ## 9 18 8.0 ## 10 20 7.5 # gộp 2 bảng dữ liệu x và y theo student_id x %&gt;% inner_join(y, by = &quot;student_id&quot;) ## student_id maths physics ## 1 2 8.0 8.0 ## 2 4 6.0 9.5 ## 3 6 4.0 7.5 ## 4 8 9.0 6.0 ## 5 10 6.5 5.5 full_join: lấy tất cả dữ liệu có cả trên bảng x, y. full_join(x, y, by = &quot;key&quot;) x %&gt;% full_join(y, by = &quot;student_id&quot;) ## student_id maths physics ## 1 1 10.0 NA ## 2 2 8.0 8.0 ## 3 3 7.0 NA ## 4 4 6.0 9.5 ## 5 5 7.8 NA ## 6 6 4.0 7.5 ## 7 7 7.7 NA ## 8 8 9.0 6.0 ## 9 9 9.5 NA ## 10 10 6.5 5.5 ## 11 12 NA 6.5 ## 12 14 NA 7.8 ## 13 16 NA 8.2 ## 14 18 NA 8.0 ## 15 20 NA 7.5 Trong ví dụ trên, các giá trị về điểm toán (maths) sẽ trả về NA cho các student_id không tồn tại trên bảng y và ngược lại cho bảng x với các giá trị điểm vật lý (physics) của các student_id không tồn tại trên bảng x. Hàm left_join: lấy dữ liệu chỉ có trên bảng x, ví dụ: left_join(x, y, by = &quot;var&quot;) x %&gt;% left_join(y, by = &quot;student_id&quot;) ## student_id maths physics ## 1 1 10.0 NA ## 2 2 8.0 8.0 ## 3 3 7.0 NA ## 4 4 6.0 9.5 ## 5 5 7.8 NA ## 6 6 4.0 7.5 ## 7 7 7.7 NA ## 8 8 9.0 6.0 ## 9 9 9.5 NA ## 10 10 6.5 5.5 Với các student_id không có giá trị trên bảng y, cột physics sẽ trả về giá trị NA Hàm right_join : lấy dữ liệu chỉ có trên bảng y, ví dụ: right_join(x, y, by = &quot;var&quot;) x %&gt;% right_join(y, by = &quot;student_id&quot;) ## student_id maths physics ## 1 2 8.0 8.0 ## 2 4 6.0 9.5 ## 3 6 4.0 7.5 ## 4 8 9.0 6.0 ## 5 10 6.5 5.5 ## 6 12 NA 6.5 ## 7 14 NA 7.8 ## 8 16 NA 8.2 ## 9 18 NA 8.0 ## 10 20 NA 7.5 Với các student_id không có giá trị trên bảng x, cột maths sẽ trả về giá trị NA Lưu ý: Trong trường hợp cột dữ liệu dùng để nối các bảng có tên khác nhau, ta có thể sử dụng cấu trúc sau: left_join(x, y, by = c(&quot;key_x&quot; = &quot;key_y&quot;)) Xem ví dụ sau: names(x)[1] &lt;- &quot;student_id1&quot; names(y)[1] &lt;- &quot;student_id2&quot; x %&gt;% inner_join(y, by = c(&quot;student_id1&quot; = &quot;student_id2&quot;)) ## student_id1 maths physics ## 1 2 8.0 8.0 ## 2 4 6.0 9.5 ## 3 6 4.0 7.5 ## 4 8 9.0 6.0 ## 5 10 6.5 5.5 2.2.2.0.3 Ghép nhiều bảng theo dòng hoặc cột với nhóm hàm bind Bên cạnh các hàm join, khi xử lý dữ liệu trong thực tiễn, ta có thể phải ghép các bảng dữ liệu theo hàng hoặc cột. Trong dplyr, có hai hàm rất hữu dụng trong hai trường hợp trên là bind_col và bind_rows bind_cols(data1, data2) bind_rows(data1, data2) Xem hai ví dụ sau. df1 &lt;- data.frame(id = 1:3, income = 8:10) df2 &lt;- data.frame(id = 4:9, income = 8:13) df3 &lt;- data.frame(id = 1:3, gender = c(&quot;F&quot;, &quot;F&quot;, &quot;M&quot;)) # Nối theo dòng df1 %&gt;% bind_rows(df2) ## id income ## 1 1 8 ## 2 2 9 ## 3 3 10 ## 4 4 8 ## 5 5 9 ## 6 6 10 ## 7 7 11 ## 8 8 12 ## 9 9 13 # Nối theo cột df1 %&gt;% bind_cols(df3) ## id income id1 gender ## 1 1 8 1 F ## 2 2 9 2 F ## 3 3 10 3 M 2.2.3 Nhóm hàm tổng hợp dữ liệu với summarise Trong quá trình xử lý dữ liệu, ta thường xuyên phải tổng hợp dữ liệu theo các cách như: tính tổng, tính số dư bình quân, phương sai, tổng số lượng quan sát… Với dplyr, ta có thể sử dụng hàm summarise() để thực hiện công việc này. data %&gt;% summarise(var_name = calculate_stats(var)) mtcars %&gt;% summarise(mean_mpg = mean(mpg), sd_mpg = sd(mpg)) ## mean_mpg sd_mpg ## 1 20.09062 6.026948 Đây là ví dụ đơn giản nhất với summarise mà ta có thể thay thế bằng summary() trên R base. Tuy nhiên, kết hợp giữa hàm summarise() và hàm group_by() trên dplyr sẽ cho chúng ta có cái nhìn về dữ liệu tổng hợp một cách đa chiều hơn. Hàm group_by() cho phép dữ liệu tổng hợp được gộp lại theo một hoặc nhiều trường thông tin khác nhau, giúp người phân tích có thể nhìn dữ liệu theo từ chiều riêng biệt hoặc gộp các chiều thông tin với nhau. mtcars %&gt;% group_by(cyl) %&gt;% summarise(mean_mpg = mean(mpg), mean_disp = mean(disp)) ## # A tibble: 3 x 3 ## cyl mean_mpg mean_disp ## &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 4 26.7 105. ## 2 6 19.7 183. ## 3 8 15.1 353. 2.3 Các hàm nâng cao trong dplyr Bên cạnh các nhóm hàm cơ bản đã trình bày ở phần trên, dplyr còn có một số hàm nâng cao khác đặc biệt hữu dụng trong quá trình biến đổi, tổng hợp dữ liệu, bao gồm case_when, mutate_at &amp; summarise_at 2.3.1 Điều kiện phân nhóm với case_when Trong quá trình phân tích và xử lý dữ liệu, chúng ta thường phải tạo thêm các trường mới hoặc tính toán dữ liệu dựa vào từng điều kiện khác nhau để đưa ra giá trị của trường hoặc cách tính cho dữ liệu. Ví dụ, khi ta muốn tính thưởng cho KH thì sẽ phải dùng nhiều công thức khác nhau như KH thuộc VIP sẽ nhân 1 tỷ lệ, KH thuộc nhóm trung bình sẽ có 1 tỷ lệ khác, hay KH thông thường thì sẽ 1 tỷ lệ khác…. Trong dplyr, hàm case_when() xử lý các trường hợp trên rất nhanh chóng. data %&gt;% mutate(new_var = case_when( condition_1 ~ &quot;value_1&quot;, condition_2 ~ &quot;value_2&quot;,..., TRUE ~ &quot;value_n&quot; )) Ta xem ví dụ sau: df &lt;- data.frame(number = 1:10) df %&gt;% mutate(nhom = case_when( number &lt;= 5 ~ &quot;nhom_1&quot;, # nhóm 1: số từ 1 đến 5 number &gt; 5 &amp; number &lt;= 8 ~ &quot;nhom_2&quot;, # nhóm 2: số từ 6 đến 8 TRUE ~ &quot;nhom_3&quot; # các số còn lại )) ## number nhom ## 1 1 nhom_1 ## 2 2 nhom_1 ## 3 3 nhom_1 ## 4 4 nhom_1 ## 5 5 nhom_1 ## 6 6 nhom_2 ## 7 7 nhom_2 ## 8 8 nhom_2 ## 9 9 nhom_3 ## 10 10 nhom_3 2.3.2 Tạo thêm biến mới theo điều kiện với mutate_if &amp; mutate_at Khi phân tích, ta có thể tạo thêm biến mới khi các biến trong dataframe thỏa mãn điều kiện nào đó. data %&gt;% mutate_if(condition, function) df &lt;- data.frame( id = 1:5, gender = c(&quot;F&quot;, &quot;M&quot;, &quot;M&quot;, &quot;F&quot;, &quot;F&quot;), income = c(4,5,3,6,7) ) df %&gt;% summary ## id gender income ## Min. :1 F:3 Min. :3 ## 1st Qu.:2 M:2 1st Qu.:4 ## Median :3 Median :5 ## Mean :3 Mean :5 ## 3rd Qu.:4 3rd Qu.:6 ## Max. :5 Max. :7 # Biến đổi các biến factor thành character df %&gt;% mutate_if(is.factor, as.character) %&gt;% summary ## id gender income ## Min. :1 Length:5 Min. :3 ## 1st Qu.:2 Class :character 1st Qu.:4 ## Median :3 Mode :character Median :5 ## Mean :3 Mean :5 ## 3rd Qu.:4 3rd Qu.:6 ## Max. :5 Max. :7 Ngoài ra, ta có thể tự tạo các hàm mới và áp dụng với mutate_if. Xem ví dụ sau. my_func &lt;- function(x){x*100} # Nhân các biến numeric lên 100 lần df %&gt;% mutate_if(is.numeric, my_func) ## id gender income ## 1 100 F 400 ## 2 200 M 500 ## 3 300 M 300 ## 4 400 F 600 ## 5 500 F 700 Đối với mutate_at, ta cũng có thể thực hiện tương tự. Cấu trúc tổng quát của mutate_at như sau. data %&gt;% mutate_at(vars(var1, var2, ...), function()) df ## id gender income ## 1 1 F 4 ## 2 2 M 5 ## 3 3 M 3 ## 4 4 F 6 ## 5 5 F 7 # Nhân biến income lên 100 lần df %&gt;% mutate_at(vars(income), my_func) ## id gender income ## 1 1 F 400 ## 2 2 M 500 ## 3 3 M 300 ## 4 4 F 600 ## 5 5 F 700 2.3.3 Tổng hợp dữ liệu theo điều kiện với summarise_at và summarise_if Tương tự như mutate_at và mutate_if, ta có thể tổng hợp nhanh dữ liệu theo điều kiện. Cấu trúc tổng quát của summarise_at data %&gt;% group_by(var) (không bắt buộc) summarise_at(vars(variables), funs(functions)) Xem ví dụ sau mtcars %&gt;% group_by(am) %&gt;% summarise_at(vars(mpg, disp), funs(mean, max, median)) ## # A tibble: 2 x 7 ## am mpg_mean disp_mean mpg_max disp_max mpg_median disp_median ## &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 0 17.1 290. 24.4 472 17.3 276. ## 2 1 24.4 144. 33.9 351 22.8 120. Tương tự, ta có cấu trúc tổng quát của summarise_if data %&gt;% group_by(var) (không bắt buộc) summarise_if(condition, funs(functions)) iris %&gt;% select(Species, Sepal.Length, Sepal.Width) %&gt;% group_by(Species) %&gt;% summarise_if(is.numeric, funs(mean, median)) ## # A tibble: 3 x 5 ## Species Sepal.Length_me~ Sepal.Width_mean Sepal.Length_me~ ## &lt;fct&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 setosa 5.01 3.43 5 ## 2 versic~ 5.94 2.77 5.9 ## 3 virgin~ 6.59 2.97 6.5 ## # ... with 1 more variable: Sepal.Width_median &lt;dbl&gt; "],
["phan-ra-va-xoay-chiu-d-liu.html", "Chương 3 Phân rã và xoay chiều dữ liệu 3.1 Phân rã dữ liệu thành dạng dọc với gather 3.2 Xoay chiều dữ liệu với spread 3.3 Tách một biến thành nhiều biến với separate 3.4 Gộp nhiều biến thành một biến với unite", " Chương 3 Phân rã và xoay chiều dữ liệu Khi phân tích dữ liệu, dữ liệu sau khi được làm sạch thường cơ bản có hai dạng. Dạng ngang: Mỗi dòng ứng với 1 quan sát và nhiều biến Dạng dọc: Nhiều dòng có thể chứa cùng một quan sát nhưng với các biến khác nhau. Xem hai ví dụ về dữ liệu dạng ngang và dọc ở dưới đây. ## id Species Sepal.Length Sepal.Width Petal.Length Petal.Width ## 1 1 setosa 5.1 3.5 1.4 0.2 ## 2 2 setosa 4.9 3.0 1.4 0.2 ## 3 3 setosa 4.7 3.2 1.3 0.2 ## 4 4 setosa 4.6 3.1 1.5 0.2 ## 5 5 setosa 5.0 3.6 1.4 0.2 ## 6 6 setosa 5.4 3.9 1.7 0.4 ## id Species Measurement Value ## 1 1 setosa Sepal.Length 5.1 ## 2 1 setosa Sepal.Width 3.5 ## 3 1 setosa Petal.Length 1.4 ## 4 1 setosa Petal.Width 0.2 ## 5 2 setosa Sepal.Length 4.9 ## 6 2 setosa Sepal.Width 3.0 ## 7 2 setosa Petal.Length 1.4 ## 8 2 setosa Petal.Width 0.2 Trong thực tế, chúng ta phải sử dụng rất linh hoạt cả hai định dạng dữ liệu này (dữ liệu ngang và dữ liệu dọc). Trong chương này, chúng ta sẽ học cách sử dụng và biến đổi dữ liệu giữa hai định dạng với tidyr. 3.1 Phân rã dữ liệu thành dạng dọc với gather library(tidyr) data %&gt;% gather(key = name_of_key, value = name_of_value_variable, gather = c(list_of_var)) Xem ví dụ sau. library(dplyr) df &lt;- iris %&gt;% head(2) %&gt;% mutate(id = 1:nrow(.)) %&gt;% select(6, 5, 1:4) df ## id Species Sepal.Length Sepal.Width Petal.Length Petal.Width ## 1 1 setosa 5.1 3.5 1.4 0.2 ## 2 2 setosa 4.9 3.0 1.4 0.2 # Xoay dữ liệu sang dạng dọc df2 &lt;- df %&gt;% gather(key = Measurement, value = Value, c(3:6)) # Các biến được phân rã df2 ## id Species Measurement Value ## 1 1 setosa Sepal.Length 5.1 ## 2 2 setosa Sepal.Length 4.9 ## 3 1 setosa Sepal.Width 3.5 ## 4 2 setosa Sepal.Width 3.0 ## 5 1 setosa Petal.Length 1.4 ## 6 2 setosa Petal.Length 1.4 ## 7 1 setosa Petal.Width 0.2 ## 8 2 setosa Petal.Width 0.2 Ở ví dụ trên, khi phân rã dữ liệu sang dạng dọc, các biến được phân rã là 4 biến ở vị trí từ 3 đến 6. Do dữ liệu gốc df chỉ có 2 quan sát, nên dữ liệu mới sau khi phân rã sẽ có 8 quan sát. 3.2 Xoay chiều dữ liệu với spread Ngược lại với phân rã dữ liệu là xoay chiều dữ liệu. Trong tidyr, ta có thể sử dụng hàm spread. Công thức tổng quát để xoay chiều dữ liệu như sau. data %&gt;% #Biến được xoay thành cột spread(key = key_variable, # Biến giá trị value = value_variable) Ta quay trở lại ví dụ ở phần trước vói dữ liệu df2 đã được phân rã. df2 ## id Species Measurement Value ## 1 1 setosa Sepal.Length 5.1 ## 2 2 setosa Sepal.Length 4.9 ## 3 1 setosa Sepal.Width 3.5 ## 4 2 setosa Sepal.Width 3.0 ## 5 1 setosa Petal.Length 1.4 ## 6 2 setosa Petal.Length 1.4 ## 7 1 setosa Petal.Width 0.2 ## 8 2 setosa Petal.Width 0.2 Ta có thể xoay chiều dữ liệu lại như sau. df2 %&gt;% spread(key = Measurement, value = Value) ## id Species Petal.Length Petal.Width Sepal.Length Sepal.Width ## 1 1 setosa 1.4 0.2 5.1 3.5 ## 2 2 setosa 1.4 0.2 4.9 3.0 3.3 Tách một biến thành nhiều biến với separate Khi phân tích dữ liệu, ta thường xuyên phải tách một biến thành nhiều biến. Khi đó, việc tách biến sẽ trở nên rất đơn giản với hàm separate. Công thức tổng quát của separate như sau: data %&gt;% separate(var_to_spread, c(&quot;new_var1&quot;, &quot;new_var2&quot;, ...)) df &lt;- data.frame(date = c(NA, &quot;2018-07-01&quot;, &quot;2018-09-02&quot;)) df ## date ## 1 &lt;NA&gt; ## 2 2018-07-01 ## 3 2018-09-02 # Tách biến date thành 3 biến df %&gt;% separate(date, c(&quot;year&quot;, &quot;month&quot;, &quot;date&quot;)) ## year month date ## 1 &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; ## 2 2018 07 01 ## 3 2018 09 02 3.4 Gộp nhiều biến thành một biến với unite Ngược lại với spread, ta có thể gộp nhiều biến thành một với unite. Công thức tổng quát của unite như sau. data %&gt;% unite(new_var, var_1, var2,...) Trong đó, var_1, var_2 là tên các biến sẽ được gộp. new_var là tên biến mới được tạo thành. Quay trở lại ví dụ trên. df &lt;- data.frame(date = c(&quot;2018-07-01&quot;, &quot;2018-09-02&quot;)) %&gt;% separate(date, c(&quot;year&quot;, &quot;month&quot;, &quot;date&quot;)) df ## year month date ## 1 2018 07 01 ## 2 2018 09 02 # Gộp nhiều biến df %&gt;% unite(full_date, 1:3, sep = &quot;/&quot;, remove = F) ## full_date year month date ## 1 2018/07/01 2018 07 01 ## 2 2018/09/02 2018 09 02 "],
["cac-chi-s-thng-ke-mo-ta-co-ban.html", "Chương 4 Các chỉ số thống kê mô tả cơ bản 4.1 Correlation 4.2 ANOVA 4.3 Kiểm định quan hệ Chi-bình phương", " Chương 4 Các chỉ số thống kê mô tả cơ bản Trong quá trình phân tích dữ liệu, việc làm phân tích khám phá dữ liệu (exploratory data analysis) rất quan trọng, vì nó giúp chúng ta có cái nhìn tổng quan về tập dữ liệu mà chúng ta đang có. Để làm được điều đó thì chúng ta cần phải biết đọc hiểu những chỉ số thống kê cơ bản như: min, max, median, mean, quantile 25%, quantile 75%. Ngoài ra, chúng ta cũng cần phải biết những phương pháp kiểm định phổ biến thường hay sử dụng trong thực tế như: Chi-square, correlation, ANOVA… Trong chương này, tác giả muốn giới thiệu với các bạn lần lượt những chỉ số thống kê cơ bản cũng như các phương pháp kiểm định quan trọng nói trên. Trước hết, chúng ta sẽ cùng nhau tìm hiểu cách đọc hiểu những chỉ số thống kê cơ bản như: min, max, median, mean, quantile 25%, quantile 75%. Trong chương này, chúng ta sẽ sử dụng dữ liệu iris có sẵn trong R, dữ liệu bao gồm kích thước chiều dài, chiều rộng cánh hoa và đài hoa của 3 loài hoa khác nhau: setosa, versicolor, virginica (đơn vị đo: cm). Mỗi loài hoa có 50 bông hoa với kích thước khác nhau. library(dplyr) data(&quot;iris&quot;) iris %&gt;% summary ## Sepal.Length Sepal.Width Petal.Length Petal.Width ## Min. :4.300 Min. :2.000 Min. :1.000 Min. :0.100 ## 1st Qu.:5.100 1st Qu.:2.800 1st Qu.:1.600 1st Qu.:0.300 ## Median :5.800 Median :3.000 Median :4.350 Median :1.300 ## Mean :5.843 Mean :3.057 Mean :3.758 Mean :1.199 ## 3rd Qu.:6.400 3rd Qu.:3.300 3rd Qu.:5.100 3rd Qu.:1.800 ## Max. :7.900 Max. :4.400 Max. :6.900 Max. :2.500 ## Species ## setosa :50 ## versicolor:50 ## virginica :50 ## ## ## Câu lệnh summary() cho ta biết các chỉ số thống kê cơ bản của một hoặc nhiều biến số trong tập dữ liệu như: Min: giá trị nhỏ nhất Max: giá trị lớn nhất First quantile: quantile 25% Median: giá trị trung vị Third quantile: quantile 75% Mean: giá trị trung bình Để giải thích cách đọc hiểu các chỉ số thống kê nói trên, chúng ta sẽ cùng xem tổng quan kích thước chiều dài cánh hoa (biến Petal.Length) của các loài hoa trên. iris %&gt;% select(Petal.Length) %&gt;% summary ## Petal.Length ## Min. :1.000 ## 1st Qu.:1.600 ## Median :4.350 ## Mean :3.758 ## 3rd Qu.:5.100 ## Max. :6.900 Thông qua kết quả trên, chúng ta có thể biết một số thông tin sau: Cánh hoa ngắn nhất có chiều dài 1cm 25% trên tổng số 150 bông hoa có chiều dài cánh hoa nhỏ hơn 1.6 cm 50% trên tổng số 150 bông hoa có chiều dài cánh hoa nhỏ hơn 4.35 cm Chiều dài trung bình của tất cả 150 bông hoa là 3.758 cm 75% trên tổng số 150 bông hoa có chiều dài cánh hoa nhỏ hơn 5.1 cm Cánh hoa dài nhất có chiều dài 6.9 cm Như vậy, chúng ta đã có thể đọc hiểu những chỉ số thống kê cơ bản như: giá trị nhỏ nhất/lớn nhất, giá trị trung bình/trung vị, giá trị quantile 25%, 75% với việc sử dụng một câu lệnh summary() đơn giản trong R. 4.1 Correlation Hệ số tương quan Hệ số tương quan (correlation coefficient) là một chỉ số thống kê, được sử dụng để đo lường mối liên hệ tương quan tuyến tính giữa 2 biến định lượng hay còn gọi là biến liên tục (quantitative/continuous variables). Hay nói một cách dễ hiểu hơn, hệ số tương quan cho ta biết rằng giữa 2 biến liên tục có mối liên hệ nào hay không. Hệ số tương quan có giá trị từ -1 đến 1. Dấu “+” hoặc “-” của hệ số tương quan cho ta biết 2 biến có mối liên hệ tương quan tuyến tính cùng chiều hoặc ngược chiều tương ứng. Nếu hệ số tương quan của 2 biến mang dấu “+” có nghĩa là khi biến này tăng thì biến kia cũng tăng, còn nếu hệ số tương quan của 2 biến mang dấu “-” thì có nghĩa là khi biến này tăng thì biến kia sẽ giảm. Hệ số tương quan của 2 biến bằng 0 có nghĩa là 2 biến không có mối liên hệ tương quan tuyến tính (no relationships), hệ số tương quan bằng -1 hoặc 1 có nghĩa là 2 biến có mối liên hệ tương quan tuyến tính tuyệt đối. Tiếp tục sử dụng dữ liệu iris, chúng ta tính toán hệ số tương quan của từng cặp biến chiều dài, chiều rộng cánh hoa và đài hoa của các loài hoa để xem 4 biến trên có mối liên hệ với nhau như thế nào. Để tính hệ số tương quan trong R, chúng ta có thể sử dụng hàm cor(). cor(iris %&gt;% select(1:4)) ## Sepal.Length Sepal.Width Petal.Length Petal.Width ## Sepal.Length 1.0000000 -0.1175698 0.8717538 0.8179411 ## Sepal.Width -0.1175698 1.0000000 -0.4284401 -0.3661259 ## Petal.Length 0.8717538 -0.4284401 1.0000000 0.9628654 ## Petal.Width 0.8179411 -0.3661259 0.9628654 1.0000000 Kết quả cho ta thấy Sepal.Length và Petal.Length có mối liên hệ tương quan tuyến tính cùng chiều (hệ số tương quan 0.87), trong khi đó, Sepal.Length và Sepal.Width có mối liên hệ tương quan tuyến tính ngược chiều (hệ số tương quan -0.11). Kiểm định hệ số tương quan Như vậy, chúng ta đã hiểu bản chất của hệ số tương quan và biết cách tính hệ số tương quan trong R. Câu hỏi đặt ra lúc này là làm thế nào để biết được thực sự hệ số tương quan được tính toán có ý nghĩa về mặt thống kê hay không, hay nói cách khác, giữa 2 biến có thực sự có mối liên hệ hay không. Vì vậy, chúng ta cần kiểm định hệ số tương quan. Giả sử, chúng ta muốn biết xem 2 biến Sepal.Length và Petal.Length có mối liên hệ hay không. Như đã tính toán ở phần trước, hệ số tương quan của 2 biến này là 0.87, chúng ta sẽ kiểm định xem hệ số tương quan này có ý nghĩa về mặt thống kê hay không, tức chúng ta cần kiểm định xem hệ số tương quan của 2 biến này có khác 0 hay không. Chúng ta có cặp giả thuyết sau: Ho: 2 biến không có mối liên hệ (hệ số tương quan = 0) H1: 2 biến có mối liên hệ (hệ số tương quan khác 0) cor.test(iris$Sepal.Length,iris$Petal.Length) ## ## Pearson&#39;s product-moment correlation ## ## data: iris$Sepal.Length and iris$Petal.Length ## t = 21.646, df = 148, p-value &lt; 2.2e-16 ## alternative hypothesis: true correlation is not equal to 0 ## 95 percent confidence interval: ## 0.8270363 0.9055080 ## sample estimates: ## cor ## 0.8717538 Kết quả cho ta thấy p-value &lt; 2.2e-16, như vậy, chúng ta bác bỏ giả thuyết Ho, thừa nhận H1, tức 2 biến trên có mối liên hệ với nhau, cụ thể hơn là tương quan tuyến tính cùng chiều (hệ số tương quan 0.87). 4.2 ANOVA ANOVA viết tắt của từ analysis of variance, tức phân tích phương sai. Phương pháp ANOVA được sử dụng khi chúng ta muốn so sánh giữa 2 hoặc nhiều nhóm đối tượng khác nhau dựa vào 1 tiêu chí nhất định nào đó, ví dụ như so sánh độ tuổi của nhóm khách hàng thường và khách VIP, hay so sánh thu nhập của nhóm khách hàng nam và khách hàng nữ… Để giải thích một cách dễ hiểu hơn, chúng ta sẽ làm ví dụ sau. Giả sử chúng ta muốn so sánh chiều dài đài hoa của 3 loài hoa trong tập dữ liệu iris. Chúng ta sẽ vẽ biểu đồ boxplot so sánh Sepal.Length giữa các loài hoa (Species). library(ggplot2) iris %&gt;% ggplot(aes(Species, Sepal.Length, fill = Species)) + geom_boxplot() + labs( title = &quot;Overview of Iris Sepal Length by Species&quot;, x = &quot;Species&quot;, y = &quot;Sepal Length (cm)&quot;) + scale_y_continuous(breaks = seq(0,10, by = 0.5)) + theme_bw() + # Background đen trắng theme(legend.position = &quot;none&quot;) # Bỏ legend Nhìn vào biểu đồ boxplot trên, ta thấy chiều dài đài hoa của loài hoa virginica lớn hơn so với 2 loài hoa còn lại ở cả giá trị median, quantile 25% và quantile 75%. Trong khi đó, setosa có chiều dài đài hoa ngắn nhất trong 3 loài hoa. Tuy nhiên, để chắc chắn khẳng định rằng thực sự có sự khác biệt chiều dài đài hoa giữa các loài hoa hay không thì chúng ta cần phải sử dụng kiểm định ANOVA. Kiểm định ANOVA sẽ giúp chung ta so sánh chiều dài đài hoa trung bình giữa các loài hoa, sau đó tính toán chênh lệch chiều dài đài hoa trung bình giữa các loài hoa, và cuối cùng sẽ kiểm định xem những sự chênh lệch đó thực sự có ý nghĩa về mặt thống kê hay không. Hay nói cách khác, phương pháp ANOVA kiểm định cặp giả thuyết sau: Ho: Không có sự khác biệt giữa chiều dài đài hoa của các loài hoa (tức chênh lệch chiều dài đài hoa trung bình = 0) H1: Có sự khác biệt giữa chiều dài đài hoa của các loài hoa (chênh lệch chiều dài đài hoa trung bình khác 0) Trước tiên chúng ta sẽ so sánh chiều dài đài hoa trung bình của các loài hoa. library(dplyr) iris %&gt;% group_by(Species) %&gt;% summarise(mean = mean(Sepal.Length), sd = sd(Sepal.Length)) ## # A tibble: 3 x 3 ## Species mean sd ## &lt;fct&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 setosa 5.01 0.352 ## 2 versicolor 5.94 0.516 ## 3 virginica 6.59 0.636 Kết quả cho ta thấy rằng chiều dài đài hoa trung bình của loài hoa sentosa là 5.01 cm, của versicolor là 5.94 cm, của virginica là 6.59 cm. Tiếp theo chúng ta sẽ sử dụng phương pháp ANOVA để kiểm định xem thực sự có sự khác biệt giữa chiều dài đài hoa trung bình giữa các loài hoa hay không bằng việc sử dụng hàm aov() #ANOVA model &lt;- aov(Sepal.Length ~ Species, data = iris) model %&gt;% summary ## Df Sum Sq Mean Sq F value Pr(&gt;F) ## Species 2 63.21 31.606 119.3 &lt;2e-16 *** ## Residuals 147 38.96 0.265 ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 Kết quả trên mô hình cho thấy chỉ số p value rất nhỏ, điều này cho phép chúng ta bác bỏ \\(H_0\\) và chấp nhận \\(H_1\\), tức là có sự khác biệt của giá trị trung bình của Sepal.Length giữa các loài hoa. Tuy nhiên, trong thực tế, câu hỏi có khác biệt hay không chưa đủ, mà ta còn phải trả lời câu hỏi: “Sự khác biệt là bao nhiêu?” Để trả lời cho câu hỏi trên, ta có thể sử dụng kiểm định TukeyHSD để tìm ra sự khác biệt giữa các nhóm. model %&gt;% TukeyHSD ## Tukey multiple comparisons of means ## 95% family-wise confidence level ## ## Fit: aov(formula = Sepal.Length ~ Species, data = iris) ## ## $Species ## diff lwr upr p adj ## versicolor-setosa 0.930 0.6862273 1.1737727 0 ## virginica-setosa 1.582 1.3382273 1.8257727 0 ## virginica-versicolor 0.652 0.4082273 0.8957727 0 Giải thích ý nghĩa của bảng kết quả: Cột diff: chênh lệch giá trị trung bình chiều dài đài hoa giữa các loài hoa lwr và upr: khoảng tin cậy 95% (lwr &lt; diff &lt; upr) p adj: giá trị p-value, nếu p-value &lt; 0.05 thì chúng ta đủ cơ sở để bác bỏ giả thuyết Ho, chấp nhận H1. Kết quả trong trường hợp này cho ta thấy thực sự có sự khác biệt giữa chiều dài đài hoa của các loài hoa (các giá trị p-value = 0). Như vậy, trong chương này, chúng ta đã cùng nhau tìm hiểu cách đọc hiểu những chỉ số thống kê cơ bản như: min/max (giá trị nhỏ nhất/lớn nhất), median (giá trị trung vị), mean (giá trị trung bình), quantile 25% và quantile 75%. Ngoài ra, chúng ta cũng đã biết những kiểm định quan trọng và phổ biến như: Chi-square test: sử dụng để xem giữa các biến rời rạc (discrete/categorical variables) có mối quan hệ độc lập hay phụ thuộc lẫn nhau Correlation test: sử dụng để xem giữa các biến liên tục (continuous/numeric variables) có mối liên hệ hay không (tương quan tuyến tính) ANOVA test: sử dụng để so sánh giữa các nhóm đối tượng khác nhau dựa vào giá trị trung bình của 1 biến nhất định nào đó 4.3 Kiểm định quan hệ Chi-bình phương Một trong những câu hỏi ta thường xuyên phải giải quyết trong quá trình phân tích dữ liệu là tìm kiếm mối quan hệ giữa các biến rời rạc. Một trong những kỹ thuật phổ biến để tìm kiếm mối quan hệ này là sử dụng kiểm định Chi-square (Khi bình phương). Để thực hiện phân tích mối quan hệ này, ta cần thực hiện ba bước: Bước một, xây dựng bảng phân phối tần xuất hai chiều. Bước hai, tính toán chỉ số \\(\\chi^2\\) để kiểm định giả thuyết độc lập giữa hai biến Bước ba, đưa ra kết luận về mối quan hệ vừa được kiểm định Để hiểu hơn về kiểm định \\(\\chi^2\\), ta xem xét ví dụ dưới đây. Ví dụ: Một trang web quảng cáo muốn phân tích về mối quan hệ giữa phương thức quảng cáo và thiết bị sử dụng. Dữ liệu trong một tháng về số lượt người dùng truy cập website được thể hiện như bảng dưới đây (đơn vị nghìn user) order_dc &lt;- c(&quot;Total&quot;, &quot;Phone&quot;, &quot;Tablet&quot;, &quot;Desktop&quot;) order_ch &lt;- c(&quot;Organic Search&quot;, &quot;Paid Search&quot;, &quot;Email&quot;, &quot;Display&quot;, &quot;Total&quot;) df &lt;- data.frame(device_category = c(&quot;Desktop&quot;, &quot;Tablet&quot;, &quot;Phone&quot;, &quot;Total&quot;), organic_search = c(25,20,35,80), paid_search = c(20,30,15,65), email = c(35,25,10,80), display = c(20,25,40,85), total = c(100,100,100,300)) df ## device_category organic_search paid_search email display total ## 1 Desktop 25 20 35 20 100 ## 2 Tablet 20 30 25 25 100 ## 3 Phone 35 15 10 40 100 ## 4 Total 80 65 80 85 300 Để phân tích mối quan hệ giữa hai biến, ta có giả thuyết như sau: \\(H_0\\): Phương thức quảng cáo và thiết bị sử dụng không có mối liên hệ với nhau (hai biến độc lập) \\(H_1\\): Phương thức quảng cáo và thiết bị sử dụng có mối liên hệ với nhau (tồn tại mối quan hệ giữa hai biến) Chỉ số \\(\\chi^2\\) được tính như sau: \\[\\chi^2 = \\sum\\frac{(O_i - E_i)^2}{E_i}\\] Trong đó: \\(O\\) là giá trị quan sát được thực tế (ví dụ: Desktop/Organic Search có 25 ngàn user) \\(E\\) là giá trị kỳ vọng của mỗi cặp giá trị. Ta tính giá trị kỳ vọng như sau: Tính tổng từng cột và từng dòng (giá trị \" Total \") Nhân từng dòng với từng hàng và chia cho tổng số quan sát Ví dụ: Giá trị kỳ vọng của Desktop và Organic Search có thể được tính như sau: \\[ \\begin{aligned} Users_{expected} &amp;= \\frac{r_1}{T} \\times \\frac{c_1}{T} \\times T \\\\ &amp;= \\frac{100}{300} \\times \\frac{80}{300} \\times 300 \\\\ &amp;= .33 \\times .27 \\times 300 \\\\ &amp;= 26.67 \\\\ &amp;= 27 \\end{aligned} \\] Giá trị kỳ vọng của tất cả các giá trị có kết quả như sau ## device_category organic_search paid_search email display ## 1 Desktop 26.67 21.67 23.33 28.33 ## 2 Tablet 26.67 21.67 23.33 28.33 ## 3 Phone 26.67 21.67 23.33 28.33 Tiếp đó, ta tính toán sự khác biệt giữa giá trị kỳ vọng và giá trị thực tế. Sau đó tính \\(\\chi^2\\) như sau: Tính độ sai lệch \\[ \\begin{aligned} &amp;= Users_o - Users_e \\\\ &amp;= 25 - 26.67 \\\\ &amp;= -1.67 \\end{aligned} \\] - Tính giá trị bình phương \\[-1.67^2=2.78\\] Chia giá trị vừa tính được cho giá trị kỳ vọng: \\[\\frac{2.78}{27} = 0.10\\] Tương tự, ta tính được cho tất cả các biến như sau: df3 &lt;- data.frame(device_category = c(&quot;Desktop&quot;, &quot;Tablet&quot;, &quot;Phone&quot;), organic_search = c(0.1, 1.67, 2.6), paid_search = c(0.13, 3.2, 2.05), email = c(5.83, 0.12, 7.62), display = c(2.45, 0.39, 4.8)) df3 ## device_category organic_search paid_search email display ## 1 Desktop 0.10 0.13 5.83 2.45 ## 2 Tablet 1.67 3.20 0.12 0.39 ## 3 Phone 2.60 2.05 7.62 4.80 \\(\\chi^2\\) thực tế được tính bằng tổng các giá trị trên \\(\\chi^2 = 30.96\\) Với dữ liệu trên, ta có 4 cột và 3 hàng. Như vậy, bậc tự do trong phân phôi \\(\\chi^2\\) là \\((4-1)*(3-1) = 6\\). Ta có thể so sánh giá trị vừa tìm được với giá trị \\(\\chi^2\\) tại đây. Với \\(\\alpha\\) bằng .1 và 6 bậc tự do, giá trị của \\(\\chi\\)2 là 10.64. Với giá trị thực tế lớn hơn giá trị lý thuyết, ta bác bỏ \\(H_0\\). Như vậy, về mặt thống kê, tồn tại mối quan hệ giữa phương thức marketing và thiết bị sử dụng. Ví dụ với R Trong thực tế, khi phân tích dữ liệu, ta có thể ra quyết định nhanh chóng về mối quan hệ giữa hai biến rời rạc thông qua p-value. df %&gt;% select(-1) %&gt;% as.matrix() %&gt;% chisq.test() ## ## Pearson&#39;s Chi-squared test ## ## data: . ## X-squared = 30.908, df = 12, p-value = 0.002035 Với \\(p-value = 0.002035\\), ta bác bỏ \\(H_0\\), chấp nhận \\(H_1\\). Nghĩa là tồn tại mối quan hệ giữa phương thức quảng cáo và thiết bị sử dụng. "],
["lp-trinh-ham.html", "Chương 5 Lập trình hàm 5.1 Quotes vs. Unquotes 5.2 Trường hợp dùng quo_name 5.3 Trường họp nhiều biến 5.4 Ứng dụng", " Chương 5 Lập trình hàm Khi phân tích dữ liệu, một kỹ năng nâng cao giúp tăng năng suất phân tích là kỹ năng viết hàm. Về lý thuyết, bất cứ đoạn code nào cũng có thể chuyển thành hàm. Một hàm viết tốt phải đảm bảo 3 yếu tố chính: Linh động: Có thể sử dụng trong nhiều tập dữ liệu khác nhau Ngắn gọn: Chỉ giải quyết 1 vấn đề cụ thể, không nên cho quá nhiều vấn đề trong một hàm. Mở rộng: Có thể mở rộng khi tăng số biến và giải quyết được vấn đề tổng quát Việc viết hàm là cả một nghệ thuật và đòi hỏi có sự hiểu biết sâu sắc về R. Tuy nhiên, khi đứng dưới góc độ ứng dụng, ta có thể viết hàm một cách nhanh chóng dựa trên khái niệm về NSE (Non Standard Evaluation). df &lt;- data.frame(x = 1:3, y = 3:1) filter(df, x == 3) ## x y ## 1 3 1 Tuy nhiên, câu lệnh sau sẽ không hoạt động # Cách 1: Không hoạt động my_var &lt;- x ## Error in eval(expr, envir, enclos): object &#39;x&#39; not found df %&gt;% filter(my_var == 1) ## Error in filter_impl(.data, quo): Evaluation error: object &#39;my_var&#39; not found. # Cách 2: Không hoạt động my_var &lt;- &quot;x&quot; df %&gt;% filter(my_var == 1) ## [1] x y ## &lt;0 rows&gt; (or 0-length row.names) 5.1 Quotes vs. Unquotes Quotes là cách thức biến chỉ được lưu dưới dạng string mà chưa đề cập đến giá trị mà biến đó chứa. Unquote là việc tính toán giá trị mà biến thực sự lưu trữ. Ví dụ, x &lt;- 5 thì \"x\" là quote vì chỉ lưu tên của biến dưới dạng string. x = 5 là unquote vì đã tính đến giá trị thực sự mà x lưu trữ là 5. Xem thêm ví dụ sau: greet &lt;- function(name){ print(&quot;Hello, name!&quot;) } greet(&quot;duc anh&quot;) ## [1] &quot;Hello, name!&quot; Hàm trên không hoạt động như chúng ta mong muốn vì name đang được quotes và chỉ lưu trữ dưới dạng string. greet &lt;- function(name){ glue::glue(&quot;Hello, {name}!&quot;) } greet(&quot;duc anh&quot;) ## Hello, duc anh! Ở hàm thứ hai, name được tính toán đến giá trị thực sự mà biến này đang lưu trữ (giá trị duc anh). Để quotes, ta dùng hàm quo(). Kết quả của quo() là quosure, một dạng của biểu thức (formula). quo(x) ## &lt;quosure&gt; ## expr: ^x ## env: global quo(a + b + c) ## &lt;quosure&gt; ## expr: ^a + b + c ## env: global quo(&quot;group_var&quot;) ## &lt;quosure&gt; ## expr: ^&quot;group_var&quot; ## env: empty Để sử dụng quosure trong hàm, ta sử dụng enquo. Sự khác biệt giữa quo và enquo có thể phân cấp như sau: quo: Tạo quosure với biến enquo: Tạo quosure với giá trị của biến. x &lt;- 5 # Ví dụ 1 quo(x) ## &lt;quosure&gt; ## expr: ^x ## env: global enquo(x) ## &lt;quosure&gt; ## expr: ^5 ## env: empty # Ví dụ 2 quo(x + 2) ## &lt;quosure&gt; ## expr: ^x + 2 ## env: global Để unquote, ta dùng hàm !! để tính các giá trị mà biến đang lưu trữ. my_summarise &lt;- function(df, group_var) { group_var &lt;- quo(group_var) print(group_var) df %&gt;% group_by(!! group_var) %&gt;% summarise(mean = mean(mpg)) } my_summarise(mtcars, cyl) ## &lt;quosure&gt; ## expr: ^group_var ## env: 0000000007E9A490 ## Error in grouped_df_impl(data, unname(vars), drop): Column `group_var` is unknown Câu lệnh trên không thực hiện được vì quo(group_var) sẽ trả ra kết quả là ^group_var, giá trị này không tồn tại. Cái chúng ta cần là biểu thức (expression) dạng ^cyl. Lúc này, ta phải dùng enquo my_summarise &lt;- function(df, group_var) { group_var &lt;- enquo(group_var) print(group_var) df %&gt;% group_by(!! group_var) %&gt;% summarise(mean = mean(mpg)) } my_summarise(mtcars, cyl) ## &lt;quosure&gt; ## expr: ^cyl ## env: global ## # A tibble: 3 x 2 ## cyl mean ## &lt;dbl&gt; &lt;dbl&gt; ## 1 4 26.7 ## 2 6 19.7 ## 3 8 15.1 my_var &lt;- quo(mpg) mtcars %&gt;% summarise(mean = mean(!!my_var)) ## mean ## 1 20.09062 Ta cũng có thể sử dụng quo, enquo với một nhóm các biểu thức như sau exp &lt;- quo(summarise( mtcars, mean(mpg) )) exp ## &lt;quosure&gt; ## expr: ^summarise(mtcars, mean(mpg)) ## env: global 5.2 Trường hợp dùng quo_name quo_name cho phép convert biểu thức thành dạng string. x &lt;- 5 quo(x) ## &lt;quosure&gt; ## expr: ^x ## env: global quo(x) %&gt;% quo_name() ## [1] &quot;x&quot; enquo(x) ## &lt;quosure&gt; ## expr: ^5 ## env: empty enquo(x) %&gt;% quo_name ## [1] &quot;5&quot; Lưu ý: Khi tạo các biến mới hoặc sử dụng dấu gán trong hàm khi lập trình, cần dùng dấu := Giá trị được gán (bên trái dấu gán) phải có !! Ví dụ khi sử dụng với rename my_rename &lt;- function(data, var){ var &lt;- enquo(var) new_var &lt;- paste0(&quot;new_&quot;, quo_name(var)) data &lt;- data %&gt;% rename(!!new_var := !!var) return(data) } my_rename(mtcars, mpg) %&gt;% head ## new_mpg cyl disp hp drat wt qsec vs am gear carb ## Mazda RX4 21.0 6 160 110 3.90 2.620 16.46 0 1 4 4 ## Mazda RX4 Wag 21.0 6 160 110 3.90 2.875 17.02 0 1 4 4 ## Datsun 710 22.8 4 108 93 3.85 2.320 18.61 1 1 4 1 ## Hornet 4 Drive 21.4 6 258 110 3.08 3.215 19.44 1 0 3 1 ## Hornet Sportabout 18.7 8 360 175 3.15 3.440 17.02 0 0 3 2 ## Valiant 18.1 6 225 105 2.76 3.460 20.22 1 0 3 1 Khi sử dụng với mutate my_mutate &lt;- function(data, var){ var &lt;- enquo(var) data &lt;- data %&gt;% mutate(!!&quot;new_var&quot; := !!var*2) return(data) } mtcars %&gt;% select(mpg) %&gt;% my_mutate(mpg) %&gt;% head ## mpg new_var ## 1 21.0 42.0 ## 2 21.0 42.0 ## 3 22.8 45.6 ## 4 21.4 42.8 ## 5 18.7 37.4 ## 6 18.1 36.2 5.3 Trường họp nhiều biến Khi có nhiều biến, ta dùng quos(...) &amp; !!! thay cho enquo và !! my_summarise &lt;- function(df, value_var,...){ group_var &lt;- quos(...) value_var &lt;- enquo(value_var) df %&gt;% group_by(!!!group_var) %&gt;% summarise(mean = mean(!!value_var)) } my_summarise(mtcars, mpg, cyl, vs) ## # A tibble: 5 x 3 ## # Groups: cyl [?] ## cyl vs mean ## &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 4 0 26 ## 2 4 1 26.7 ## 3 6 0 20.6 ## 4 6 1 19.1 ## 5 8 0 15.1 Lưu ý: Trong thực tế, khi không muốn viết hàm mà vẫn có thể sử dụng ứng dụng của toán tử !, ta có thể sử dụng toán tử !!! nhu sau. my_var &lt;- c(&quot;mpg&quot;, &quot;cyl&quot;) mtcars %&gt;% select(!!!my_var) %&gt;% head ## mpg cyl ## Mazda RX4 21.0 6 ## Mazda RX4 Wag 21.0 6 ## Datsun 710 22.8 4 ## Hornet 4 Drive 21.4 6 ## Hornet Sportabout 18.7 8 ## Valiant 18.1 6 5.4 Ứng dụng 5.4.1 Hàm tính toán tổng hợp nhiều biến analytics_summarise &lt;- function(data, value_var,...){ group_var &lt;- quos(...) value_var &lt;- enquo(value_var) data %&gt;% group_by(!!!group_var) %&gt;% summarise(n = n(), total = sum(!!value_var), min = min(!!value_var), q25 = quantile(!!value_var, 0.25, na.rm = T), q50 = quantile(!!value_var, 0.50, na.rm = T), q75 = quantile(!!value_var, 0.75, na.rm = T), q90 = quantile(!!value_var, 0.90, na.rm = T), q95 = quantile(!!value_var, 0.95, na.rm = T), max = max(!!value_var), mean = mean(!!value_var, na.rm = T), mean_trimed = mean(!!value_var, trim = 0.1, na.rm = T)) %&gt;% ungroup -&gt; result return(result) } # Một biến mtcars %&gt;% analytics_summarise(mpg, cyl) ## # A tibble: 3 x 12 ## cyl n total min q25 q50 q75 q90 q95 max mean ## &lt;dbl&gt; &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 4 11 293. 21.4 22.8 26 30.4 32.4 33.2 33.9 26.7 ## 2 6 7 138. 17.8 18.6 19.7 21 21.2 21.3 21.4 19.7 ## 3 8 14 211. 10.4 14.4 15.2 16.2 18.3 18.9 19.2 15.1 ## # ... with 1 more variable: mean_trimed &lt;dbl&gt; # Nhiều biến mtcars %&gt;% analytics_summarise(mpg, cyl, am) ## # A tibble: 6 x 13 ## cyl am n total min q25 q50 q75 q90 q95 max mean ## &lt;dbl&gt; &lt;dbl&gt; &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 4 0 3 68.7 21.5 22.2 22.8 23.6 24.1 24.2 24.4 22.9 ## 2 4 1 8 225. 21.4 25.2 28.8 30.9 32.8 33.4 33.9 28.1 ## 3 6 0 4 76.5 17.8 18.0 18.6 19.8 20.7 21.1 21.4 19.1 ## 4 6 1 3 61.7 19.7 20.4 21 21 21 21 21 20.6 ## 5 8 0 12 181. 10.4 14.0 15.2 16.6 18.6 18.9 19.2 15.0 ## 6 8 1 2 30.8 15 15.2 15.4 15.6 15.7 15.8 15.8 15.4 ## # ... with 1 more variable: mean_trimed &lt;dbl&gt; 5.4.2 Vẽ đồ thị với ggplot2 Tương tự với dplyr, lập trình NSE có thể sử dụng đơn giản với ggplot2. my_chart &lt;- function(data, value_var, group_var){ value_var &lt;- enquo(value_var) group_var &lt;- enquo(group_var) data %&gt;% ggplot(aes(!!group_var, !!value_var)) + geom_bar(stat = &quot;identity&quot;, aes(fill = !!group_var)) + theme_minimal() } mtcars %&gt;% mutate(cyl = as.factor(cyl)) %&gt;% group_by(cyl) %&gt;% summarise(mpg = sum(mpg)) %&gt;% my_chart(mpg, cyl) "],
["lp-trinh-chc-nang-ham-vi-purrr.html", "Chương 6 Lập trình chức năng hàm với purrr 6.1 Nhóm hàm map 6.2 Sửa đổi giá trị với modify 6.3 Tạo hàm nhanh với as_mapper 6.4 Xây dựng chuỗi các hàm liên tiếp với compose 6.5 Ứng dụng", " Chương 6 Lập trình chức năng hàm với purrr Khi phân tích dữ liệu phức tạp, ta thường xuyên phải thực hiện một nhóm các phân tích tương tự nhau cho các nhóm dữ liệu khác nhau. Việc sử dụng các hàm làm đơn vị thao tác cơ bản và phối hợp các hàm với nhau được gọi là lập trình chức năng hàm (functional programming). Để đơn giản, ta xét ví dụ sau. Sử dụng tập dữ liệu iris, với mỗi nhóm của Species, xây dựng mô hình hồi quy giữa Sepal.Length và Petal.Length, so sánh giá trị r.squared giữa các mô hình. Với cách làm thông thường, ta sẽ phải thức hiện theo thứ tự sau: Tạo các data.frame cho từng giá trị của Species Với mỗi data.frame vừa tạo, xây dựng mô hình lm Với mỗi mô hình vừa tạo, chiết xuất giá trị r.squared và lưu vào một data.frame Cách triển khai trên có thể sử dụng vòng lặp trong R với phương án như sau library(dplyr) library(purrr) category &lt;- iris$Species %&gt;% levels %&gt;% as.character() model_result &lt;- data.frame() for (i in category){ df &lt;- iris %&gt;% filter(Species == i) model &lt;- lm(Sepal.Length ~ Sepal.Width, data = df) model_summary &lt;- summary(model) df_temp &lt;- data.frame(species = i, r.square = model_summary$r.squared) model_result &lt;- bind_rows(model_result, df_temp) } Tuy nhiên, với lập trình chức năng hàm, ta có thể làm rất đơn giản như sau. library(purrr) iris %&gt;% split(.$Species) %&gt;% map(~lm(Sepal.Length ~ Sepal.Width, data = .)) %&gt;% map(summary) %&gt;% map_dbl(&quot;r.squared&quot;) ## setosa versicolor virginica ## 0.5513756 0.2765821 0.2090573 Trong chương này, chúng ta sẽ tìm hiểu các cách thức cơ bản lập trình chức năng hàm với R qua package purrr. Việc nắm vững kiến thức và kỹ năng lập trình hàm có rất nhiều ứng dụng trong công việc phân tích, giúp giảm thiểu rất lớn thời gian phân tích, làm cho quá trình phân tích mạch lạc hơn rất nhiều trong các bài toán khám phá dữ liệu 6.1 Nhóm hàm map Công thức tổng quát của nhóm hàm map map(.x, .f, ...) Giải thích: Với mỗi giá trị của .x, thực hiện .f. Trong đó, x là một list. Hàm map làm hàm tổng quát, ngoài ra, map còn có các biến thể chính sau Câu lệnh Kết quả map list map_dbl vector dạng double map_int vector dạng int map_chr vector dạng character map_df data.frame # Dạng list iris %&gt;% map(class) ## $Sepal.Length ## [1] &quot;numeric&quot; ## ## $Sepal.Width ## [1] &quot;numeric&quot; ## ## $Petal.Length ## [1] &quot;numeric&quot; ## ## $Petal.Width ## [1] &quot;numeric&quot; ## ## $Species ## [1] &quot;factor&quot; # Dạng char iris %&gt;% map_chr(class) ## Sepal.Length Sepal.Width Petal.Length Petal.Width Species ## &quot;numeric&quot; &quot;numeric&quot; &quot;numeric&quot; &quot;numeric&quot; &quot;factor&quot; # Dạng data.frame iris %&gt;% map_df(class) ## # A tibble: 1 x 5 ## Sepal.Length Sepal.Width Petal.Length Petal.Width Species ## &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; ## 1 numeric numeric numeric numeric factor Map theo điều kiện với map_if và map_at Tương tự với map, nhóm map_if và map_at cho phép tính toán theo điều kiện hoặc vị trí của list. Xem ví dụ sau. # map_if iris %&gt;% map_if(is.numeric, as.character) %&gt;% as.data.frame %&gt;% str ## &#39;data.frame&#39;: 150 obs. of 5 variables: ## $ Sepal.Length: Factor w/ 35 levels &quot;4.3&quot;,&quot;4.4&quot;,&quot;4.5&quot;,..: 9 7 5 4 8 12 4 8 2 7 ... ## $ Sepal.Width : Factor w/ 23 levels &quot;2&quot;,&quot;2.2&quot;,&quot;2.3&quot;,..: 15 10 12 11 16 19 14 14 9 11 ... ## $ Petal.Length: Factor w/ 43 levels &quot;1&quot;,&quot;1.1&quot;,&quot;1.2&quot;,..: 5 5 4 6 5 8 5 6 5 6 ... ## $ Petal.Width : Factor w/ 22 levels &quot;0.1&quot;,&quot;0.2&quot;,&quot;0.3&quot;,..: 2 2 2 2 2 4 3 2 2 1 ... ## $ Species : Factor w/ 3 levels &quot;setosa&quot;,&quot;versicolor&quot;,..: 1 1 1 1 1 1 1 1 1 1 ... # map_at iris %&gt;% map_at(c(1,2), as.character) %&gt;% str ## List of 5 ## $ Sepal.Length: chr [1:150] &quot;5.1&quot; &quot;4.9&quot; &quot;4.7&quot; &quot;4.6&quot; ... ## $ Sepal.Width : chr [1:150] &quot;3.5&quot; &quot;3&quot; &quot;3.2&quot; &quot;3.1&quot; ... ## $ Petal.Length: num [1:150] 1.4 1.4 1.3 1.5 1.4 1.7 1.4 1.5 1.4 1.5 ... ## $ Petal.Width : num [1:150] 0.2 0.2 0.2 0.2 0.2 0.4 0.3 0.2 0.2 0.1 ... ## $ Species : Factor w/ 3 levels &quot;setosa&quot;,&quot;versicolor&quot;,..: 1 1 1 1 1 1 1 1 1 1 ... Lưu ý: Với trường hợp có hai biến đầu vào, có thể sử dụng nhóm hàm map2. Ví dụ # Không chạy map_dbl(1:3, 4:6, sum) map2_dbl(1:3, 4:6, sum) ## [1] 5 7 9 Với các trường hợp phức tạp, ta cần vận dụng linh hoạt. Ví dụ: Với mỗi dòng trong iris , tách thành dataframe riêng và xoay chiều dữ liêu. Tên các cột trở thành biến attribute, giá trị các cột trở thành biến value. library(tidyverse) get_data &lt;- function(data, i){ df &lt;- data %&gt;% slice(i) %&gt;% t %&gt;% as.data.frame result &lt;- data.frame(attribute = rownames(df), value = df[,1]) rownames(result) &lt;- NULL return(result) } get_data(mtcars, 3) ## attribute value ## 1 mpg 22.80 ## 2 cyl 4.00 ## 3 disp 108.00 ## 4 hp 93.00 ## 5 drat 3.85 ## 6 wt 2.32 ## 7 qsec 18.61 ## 8 vs 1.00 ## 9 am 1.00 ## 10 gear 4.00 ## 11 carb 1.00 get_data(iris, 1) ## attribute value ## 1 Sepal.Length 5.1 ## 2 Sepal.Width 3.5 ## 3 Petal.Length 1.4 ## 4 Petal.Width 0.2 ## 5 Species setosa map2(replicate(3, iris, simplify = F), c(1:3), get_data) ## [[1]] ## attribute value ## 1 Sepal.Length 5.1 ## 2 Sepal.Width 3.5 ## 3 Petal.Length 1.4 ## 4 Petal.Width 0.2 ## 5 Species setosa ## ## [[2]] ## attribute value ## 1 Sepal.Length 4.9 ## 2 Sepal.Width 3 ## 3 Petal.Length 1.4 ## 4 Petal.Width 0.2 ## 5 Species setosa ## ## [[3]] ## attribute value ## 1 Sepal.Length 4.7 ## 2 Sepal.Width 3.2 ## 3 Petal.Length 1.3 ## 4 Petal.Width 0.2 ## 5 Species setosa 6.2 Sửa đổi giá trị với modify Tương tự như map, modify cho áp dụng hàm vào một nhóm các list. Tuy nhiên, khác với map, modify cho ra kết quả với cấu trúc dữ liệu ban đâu. # map đổi cấu trúc của dataframe iris %&gt;% map_if(is.factor, as.character) %&gt;% str ## List of 5 ## $ Sepal.Length: num [1:150] 5.1 4.9 4.7 4.6 5 5.4 4.6 5 4.4 4.9 ... ## $ Sepal.Width : num [1:150] 3.5 3 3.2 3.1 3.6 3.9 3.4 3.4 2.9 3.1 ... ## $ Petal.Length: num [1:150] 1.4 1.4 1.3 1.5 1.4 1.7 1.4 1.5 1.4 1.5 ... ## $ Petal.Width : num [1:150] 0.2 0.2 0.2 0.2 0.2 0.4 0.3 0.2 0.2 0.1 ... ## $ Species : chr [1:150] &quot;setosa&quot; &quot;setosa&quot; &quot;setosa&quot; &quot;setosa&quot; ... # modify giữ nguyên cấu trúc iris %&gt;% modify_if(is.factor, as.character) %&gt;% str ## &#39;data.frame&#39;: 150 obs. of 5 variables: ## $ Sepal.Length: num 5.1 4.9 4.7 4.6 5 5.4 4.6 5 4.4 4.9 ... ## $ Sepal.Width : num 3.5 3 3.2 3.1 3.6 3.9 3.4 3.4 2.9 3.1 ... ## $ Petal.Length: num 1.4 1.4 1.3 1.5 1.4 1.7 1.4 1.5 1.4 1.5 ... ## $ Petal.Width : num 0.2 0.2 0.2 0.2 0.2 0.4 0.3 0.2 0.2 0.1 ... ## $ Species : chr &quot;setosa&quot; &quot;setosa&quot; &quot;setosa&quot; &quot;setosa&quot; ... 6.3 Tạo hàm nhanh với as_mapper as_mapper cho phép tạo hàm nhanh, đặc biệt hữu ích khi ta chỉ muốn tạo và sử dụng một hàm trong một vài trường hợp đặc biệt. Công thức tổng quát # Với một tham số as_mapper(~f(.x)) # Với hai tham số as_mapper(f(.x, .y)) Xem các ví dụ sau: # Cộng 10 vào mỗi giá trị map_dbl(1:3, ~ .x+10) ## [1] 11 12 13 # Cộng hai vector với nhau map2_dbl(1:3, 5:7, ~.x + .y) ## [1] 6 8 10 # Cách viết khác map2_dbl(1:3, 5:7, as_mapper(~.x + .y)) ## [1] 6 8 10 6.4 Xây dựng chuỗi các hàm liên tiếp với compose Hàm compose cho phép kết hợp nhiều hàm với nhau với hàm ở bên phải là input đầu vào cho hàm bên trái. Cấu trúc như sau. compose(f_2, f_1) # Tương đương với argument %&gt;% f_2 %&gt;% f_1 Xem ví dụ sau: library(tidyverse) library(broom) lm(Sepal.Length ~ Sepal.Width, data = iris) %&gt;% tidy ## # A tibble: 2 x 5 ## term estimate std.error statistic p.value ## &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 (Intercept) 6.53 0.479 13.6 6.47e-28 ## 2 Sepal.Width -0.223 0.155 -1.44 1.52e- 1 Cách viết trên có thể thay thế như sau tidy_lm &lt;- compose(tidy, lm) tidy_lm(Sepal.Length ~ Sepal.Width, data = iris) ## # A tibble: 2 x 5 ## term estimate std.error statistic p.value ## &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 (Intercept) 6.53 0.479 13.6 6.47e-28 ## 2 Sepal.Width -0.223 0.155 -1.44 1.52e- 1 Ta có thể thêm các nhóm hàm khác đi cùng với compose như filter my_func &lt;- compose( as_mapper(~filter(.x, p.value &lt; 0.05)), tidy, lm) my_func(Sepal.Length ~ Sepal.Width, data = iris) ## # A tibble: 1 x 5 ## term estimate std.error statistic p.value ## &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 (Intercept) 6.53 0.479 13.6 6.47e-28 6.5 Ứng dụng 6.5.1 Biến đổi dữ liệu với modify và map_df Khi phân tích dữ liệu, đôi khi ta cần chuẩn hóa dữ liệu cho tất cả các biến numeric trong data.frame. Với nhóm hàm của purrr, ta có thể thực hiện như sau # Tạo hàm standardize_data &lt;- function(x){ x &lt;- (x - min(x))/(max(x) - min(x)) return(x) } # Sử dụng map_df df &lt;- iris df[, 1:4] &lt;- df[, 1:4] %&gt;% map_df(standardize_data) df %&gt;% head ## Sepal.Length Sepal.Width Petal.Length Petal.Width Species ## 1 0.22222222 0.6250000 0.06779661 0.04166667 setosa ## 2 0.16666667 0.4166667 0.06779661 0.04166667 setosa ## 3 0.11111111 0.5000000 0.05084746 0.04166667 setosa ## 4 0.08333333 0.4583333 0.08474576 0.04166667 setosa ## 5 0.19444444 0.6666667 0.06779661 0.04166667 setosa ## 6 0.30555556 0.7916667 0.11864407 0.12500000 setosa # Sử dụng modify # Sử dụng map_df df &lt;- iris df &lt;- df %&gt;% modify_if(is.numeric, standardize_data) df %&gt;% head ## Sepal.Length Sepal.Width Petal.Length Petal.Width Species ## 1 0.22222222 0.6250000 0.06779661 0.04166667 setosa ## 2 0.16666667 0.4166667 0.06779661 0.04166667 setosa ## 3 0.11111111 0.5000000 0.05084746 0.04166667 setosa ## 4 0.08333333 0.4583333 0.08474576 0.04166667 setosa ## 5 0.19444444 0.6666667 0.06779661 0.04166667 setosa ## 6 0.30555556 0.7916667 0.11864407 0.12500000 setosa 6.5.2 Phân tích nhiều nhóm khác nhau cùng lúc Khi phân tích dữ liệu, đôi khi ta muốn xây dựng chuẩn phân tích dữ liệu qua một số bước, bao gồm: Tính toán các chỉ số thống kê Vẽ đồ thị Xây dụng mô hình đơn giản Quy trình này sẽ không gặp vấn đề khi ta chỉ phải xử lý với một nhóm nhỏ dữ liệu. Khi số lượng nhóm tăng lên, việc phân tích dữ liệu trở nên khó khăn hơn rất nhiều và tốn thời gian. Tuy nhiên, với purrr, các vấn đề này trở nên rất đơn giản. Ví dụ: Với mỗi nhóm của Species trong tập dữ liệu iris: Tổng hơp dữ liệu Vẽ đồ thị điểm giữa Sepal.Length vs. Petal.Length Xây dựng mô hình hồi quy Sepal.Length ~ Petal.Length library(tidyverse) # Bước một: Xây dựng hàm my_stat &lt;- function(data){ print(&quot;Summary data&quot;) print(&quot;====================&quot;) summary(data) %&gt;% print p &lt;- data %&gt;% ggplot(aes(Sepal.Length, Petal.Length)) + geom_point() print(&quot;Summary model&quot;) print(&quot;====================&quot;) model &lt;- lm(Sepal.Length ~ Petal.Length, data = data) summary(model) %&gt;% print p %&gt;% print } # Test hàm my_stat(iris) ## [1] &quot;Summary data&quot; ## [1] &quot;====================&quot; ## Sepal.Length Sepal.Width Petal.Length Petal.Width ## Min. :4.300 Min. :2.000 Min. :1.000 Min. :0.100 ## 1st Qu.:5.100 1st Qu.:2.800 1st Qu.:1.600 1st Qu.:0.300 ## Median :5.800 Median :3.000 Median :4.350 Median :1.300 ## Mean :5.843 Mean :3.057 Mean :3.758 Mean :1.199 ## 3rd Qu.:6.400 3rd Qu.:3.300 3rd Qu.:5.100 3rd Qu.:1.800 ## Max. :7.900 Max. :4.400 Max. :6.900 Max. :2.500 ## Species ## setosa :50 ## versicolor:50 ## virginica :50 ## ## ## ## [1] &quot;Summary model&quot; ## [1] &quot;====================&quot; ## ## Call: ## lm(formula = Sepal.Length ~ Petal.Length, data = data) ## ## Residuals: ## Min 1Q Median 3Q Max ## -1.24675 -0.29657 -0.01515 0.27676 1.00269 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 4.30660 0.07839 54.94 &lt;2e-16 *** ## Petal.Length 0.40892 0.01889 21.65 &lt;2e-16 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 0.4071 on 148 degrees of freedom ## Multiple R-squared: 0.76, Adjusted R-squared: 0.7583 ## F-statistic: 468.6 on 1 and 148 DF, p-value: &lt; 2.2e-16 # Bước 2: Xây dựng map iris$Species %&gt;% unique %&gt;% map(function(value){ print(paste0(&quot;Analysis of &quot;, value)) iris %&gt;% filter(Species == value) %&gt;% my_stat }) ## [1] &quot;Analysis of setosa&quot; ## [1] &quot;Summary data&quot; ## [1] &quot;====================&quot; ## Sepal.Length Sepal.Width Petal.Length Petal.Width ## Min. :4.300 Min. :2.300 Min. :1.000 Min. :0.100 ## 1st Qu.:4.800 1st Qu.:3.200 1st Qu.:1.400 1st Qu.:0.200 ## Median :5.000 Median :3.400 Median :1.500 Median :0.200 ## Mean :5.006 Mean :3.428 Mean :1.462 Mean :0.246 ## 3rd Qu.:5.200 3rd Qu.:3.675 3rd Qu.:1.575 3rd Qu.:0.300 ## Max. :5.800 Max. :4.400 Max. :1.900 Max. :0.600 ## Species ## setosa :50 ## versicolor: 0 ## virginica : 0 ## ## ## ## [1] &quot;Summary model&quot; ## [1] &quot;====================&quot; ## ## Call: ## lm(formula = Sepal.Length ~ Petal.Length, data = data) ## ## Residuals: ## Min 1Q Median 3Q Max ## -0.57238 -0.20671 -0.03084 0.17339 0.93608 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 4.2132 0.4156 10.138 1.61e-13 *** ## Petal.Length 0.5423 0.2823 1.921 0.0607 . ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 0.3432 on 48 degrees of freedom ## Multiple R-squared: 0.07138, Adjusted R-squared: 0.05204 ## F-statistic: 3.69 on 1 and 48 DF, p-value: 0.0607 ## [1] &quot;Analysis of versicolor&quot; ## [1] &quot;Summary data&quot; ## [1] &quot;====================&quot; ## Sepal.Length Sepal.Width Petal.Length Petal.Width ## Min. :4.900 Min. :2.000 Min. :3.00 Min. :1.000 ## 1st Qu.:5.600 1st Qu.:2.525 1st Qu.:4.00 1st Qu.:1.200 ## Median :5.900 Median :2.800 Median :4.35 Median :1.300 ## Mean :5.936 Mean :2.770 Mean :4.26 Mean :1.326 ## 3rd Qu.:6.300 3rd Qu.:3.000 3rd Qu.:4.60 3rd Qu.:1.500 ## Max. :7.000 Max. :3.400 Max. :5.10 Max. :1.800 ## Species ## setosa : 0 ## versicolor:50 ## virginica : 0 ## ## ## ## [1] &quot;Summary model&quot; ## [1] &quot;====================&quot; ## ## Call: ## lm(formula = Sepal.Length ~ Petal.Length, data = data) ## ## Residuals: ## Min 1Q Median 3Q Max ## -0.73479 -0.20272 -0.02065 0.26092 0.69956 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 2.4075 0.4463 5.395 2.08e-06 *** ## Petal.Length 0.8283 0.1041 7.954 2.59e-10 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 0.3425 on 48 degrees of freedom ## Multiple R-squared: 0.5686, Adjusted R-squared: 0.5596 ## F-statistic: 63.26 on 1 and 48 DF, p-value: 2.586e-10 ## [1] &quot;Analysis of virginica&quot; ## [1] &quot;Summary data&quot; ## [1] &quot;====================&quot; ## Sepal.Length Sepal.Width Petal.Length Petal.Width ## Min. :4.900 Min. :2.200 Min. :4.500 Min. :1.400 ## 1st Qu.:6.225 1st Qu.:2.800 1st Qu.:5.100 1st Qu.:1.800 ## Median :6.500 Median :3.000 Median :5.550 Median :2.000 ## Mean :6.588 Mean :2.974 Mean :5.552 Mean :2.026 ## 3rd Qu.:6.900 3rd Qu.:3.175 3rd Qu.:5.875 3rd Qu.:2.300 ## Max. :7.900 Max. :3.800 Max. :6.900 Max. :2.500 ## Species ## setosa : 0 ## versicolor: 0 ## virginica :50 ## ## ## ## [1] &quot;Summary model&quot; ## [1] &quot;====================&quot; ## ## Call: ## lm(formula = Sepal.Length ~ Petal.Length, data = data) ## ## Residuals: ## Min 1Q Median 3Q Max ## -0.73409 -0.23643 -0.03132 0.23771 0.76207 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 1.05966 0.46677 2.27 0.0277 * ## Petal.Length 0.99574 0.08367 11.90 6.3e-16 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 0.3232 on 48 degrees of freedom ## Multiple R-squared: 0.7469, Adjusted R-squared: 0.7416 ## F-statistic: 141.6 on 1 and 48 DF, p-value: 6.298e-16 ## [[1]] ## ## [[2]] ## ## [[3]] 6.5.3 Phân tích nhiều biến trong dataframe cùng lúc Một biên thể khác của map là sử dụng trong phân tích cùng lúc nhiều biến số với một biến thuộc dạng nhóm (group). Khi lập trình với NSE, ta cần phải sử dụng hàm syms() trước khi map Ví dụ: Với mỗi biến số trong tập dữ liệu iris: So sánh giá trị trung bình của biến này với các nhóm khác nhau của Species Vẽ biểu đồ boxplot library(tidyverse) # Bước 1: Xây dựng hàm my_stat &lt;- function(x){ x &lt;- enquo(x) iris %&gt;% group_by(Species) %&gt;% summarise(mean = mean(!!x)) %&gt;% print iris %&gt;% ggplot(aes(Species, !!x)) + geom_boxplot(aes(fill = Species)) + theme_minimal() } my_stat(Sepal.Length) ## # A tibble: 3 x 2 ## Species mean ## &lt;fct&gt; &lt;dbl&gt; ## 1 setosa 5.01 ## 2 versicolor 5.94 ## 3 virginica 6.59 # Bước 2: Dùng map iris %&gt;% select_if(is.numeric) %&gt;% names %&gt;% syms %&gt;% map(my_stat) ## # A tibble: 3 x 2 ## Species mean ## &lt;fct&gt; &lt;dbl&gt; ## 1 setosa 5.01 ## 2 versicolor 5.94 ## 3 virginica 6.59 ## # A tibble: 3 x 2 ## Species mean ## &lt;fct&gt; &lt;dbl&gt; ## 1 setosa 3.43 ## 2 versicolor 2.77 ## 3 virginica 2.97 ## # A tibble: 3 x 2 ## Species mean ## &lt;fct&gt; &lt;dbl&gt; ## 1 setosa 1.46 ## 2 versicolor 4.26 ## 3 virginica 5.55 ## # A tibble: 3 x 2 ## Species mean ## &lt;fct&gt; &lt;dbl&gt; ## 1 setosa 0.246 ## 2 versicolor 1.33 ## 3 virginica 2.03 ## [[1]] ## ## [[2]] ## ## [[3]] ## ## [[4]] Lưu ý: Việc ứng dụng purrr với các hàm tính toán thống kê sẽ cho phép phân tích khám phá dữ liệu hàng loạt "],
["bin-i-d-liu-text.html", "Chương 7 Biến đổi dữ liệu text 7.1 Giới thiệu 7.2 Regular expression 7.3 Các ví dụ tổng hợp 7.4 Tài liệu tham khảo", " Chương 7 Biến đổi dữ liệu text library(dplyr) library(stringr) 7.1 Giới thiệu Bên cạnh dữ liệu định dạng số, dữ liệu chứa rất nhiều định dạng character. Do đó, việc nắm vững các nguyên lý và biến đổi dữ liệu định dạng character trong R sẽ giúp rất nhiều trong việc xử lý dữ liệu. Để tạo chuỗi string, trong R có 3 cách cơ bản: Dùng dấu ngoặc kép Dùng dấu ngoặc đơn Dùng hàm character x &lt;- &quot;Chú mèo nằm trên lan can&quot; y &lt;- &#39;Chú mèo nằm trên lan can&#39; Lưu ý: Ta có thể để dấu ngoặc đơn hoặc ngoặc kép trong character như sau: Ngoặc đơn trong ngoặc kép hoặc Ngoặc kép trong ngoặc đơn (x &lt;- &quot;This is &#39;MY&#39; apple&quot;) ## [1] &quot;This is &#39;MY&#39; apple&quot; (y &lt;- &#39;This is &quot;HIS&quot; orange&#39;) ## [1] &quot;This is \\&quot;HIS\\&quot; orange&quot; Lưu ý: Empty string là chuỗi không có ký tự, được viết \"\" a &lt;- &quot;&quot; a ## [1] &quot;&quot; a %&gt;% class ## [1] &quot;character&quot; paste và paste0: Đây là hai hàm cơ bản cho phép ghép các chuỗi lại với nhau #Hàm paste mặc định có khoảng cách giữa các object paste(&quot;I love&quot;, pi) ## [1] &quot;I love 3.14159265358979&quot; #Hàm paste0 không có khoảng cách paste0(&quot;I love&quot;, pi) ## [1] &quot;I love3.14159265358979&quot; #Paste có sep paste(&quot;I love&quot;, pi, sep = &quot;___&quot;) ## [1] &quot;I love___3.14159265358979&quot; #Paste khi có length khác nhau paste(1:3, &quot;a&quot;, sep = &quot;&quot;) ## [1] &quot;1a&quot; &quot;2a&quot; &quot;3a&quot; #Paste không có collapse paste(1:3, &quot;a&quot;, sep = &quot;&quot;, collapse = &quot;&quot;) ## [1] &quot;1a2a3a&quot; #Khi paste giá trị NA, R biến NA thành string paste(&quot;abc &quot;, NA) ## [1] &quot;abc NA&quot; toString cho phép biến đổi thành một vector thành chuối a &lt;- c(pi, 19) a ## [1] 3.141593 19.000000 a %&gt;% toString ## [1] &quot;3.14159265358979, 19&quot; Các hàm toupper, tolower, casefold cho phép biến đổi chuỗi thành các dạng in hoa và in thường #Chuyển sang lower tolower(c(&quot;aLL ChaRacterS in LoweR caSe&quot;, &quot;ABCDE&quot;)) ## [1] &quot;all characters in lower case&quot; &quot;abcde&quot; #Chuyển sang upper toupper(c(&quot;aLL ChaRacterS in LoweR caSe&quot;, &quot;ABCDE&quot;)) ## [1] &quot;ALL CHARACTERS IN LOWER CASE&quot; &quot;ABCDE&quot; #Sử dụng với case fold casefold(c(&quot;aLL ChaRacterS in LoweR caSe&quot;, &quot;ABCDE&quot;)) ## [1] &quot;all characters in lower case&quot; &quot;abcde&quot; casefold(c(&quot;aLL ChaRacterS in LoweR caSe&quot;, &quot;ABCDE&quot;), upper = T) ## [1] &quot;ALL CHARACTERS IN LOWER CASE&quot; &quot;ABCDE&quot; str_c: Tương tự như paste, sử dụng \"\" là ký tự phân tách chuỗi mặc định str_length: Kiểm tra độ dài của ký tự library(stringr) str_c(&quot;Ola&quot;, &quot;ede&quot;) ## [1] &quot;Olaede&quot; str_c(&quot;Ola&quot;, &quot;ede&quot;, sep = &quot;_&quot;) ## [1] &quot;Ola_ede&quot; c(&quot;olala&quot;) %&gt;% str_length() ## [1] 5 str_sub: Lấy chuỗi con trong 1 chuỗi x &lt;- c(&quot;English&quot;, &quot;Polish&quot;, &quot;Other language&quot;) #Lấy 2 ký tự đầu tiên str_sub(x, start = 1, end = 2) ## [1] &quot;En&quot; &quot;Po&quot; &quot;Ot&quot; #Lấy 2 ký tự cuối cùng str_sub(x, start = -2, end = -1) ## [1] &quot;sh&quot; &quot;sh&quot; &quot;ge&quot; #Sử dụng str_sub để thay thế str_sub(x, start = -2, end = -1) &lt;- str_sub(x, start = -2, end = -1) %&gt;% toupper() x ## [1] &quot;EngliSH&quot; &quot;PoliSH&quot; &quot;Other languaGE&quot; str_wrap: Wrap ký tự - hàm này rất hữu dụng khi sử dụng tên trong biểu đồ some_quote = c( &quot;I may not have gone&quot;, &quot;where I intended to go,&quot;, &quot;but I think I have ended up&quot;, &quot;where I needed to be&quot; ) some_quote &lt;- str_c(some_quote) some_quote ## [1] &quot;I may not have gone&quot; &quot;where I intended to go,&quot; ## [3] &quot;but I think I have ended up&quot; &quot;where I needed to be&quot; str_wrap(some_quote, width = 30, indent = 2) %&gt;% cat ## I may not have gone where I intended to go, but I think I have ended up where I needed to be Lưu ý: Khi làm việc với chuỗi, hàm cat cho phép thể hiện được các ký tự đặc biệt trên R console # Không dùng cat x &lt;- &quot;Dòng 1 \\nDòng 2&quot; x ## [1] &quot;Dòng 1 \\nDòng 2&quot; # Sử dụng cat x %&gt;% cat ## Dòng 1 ## Dòng 2 str_trim: Sử dụng để cắt các đoạn text có ký tự trắng bad_text = c(&quot;This&quot;, &quot; example &quot;, &quot;has several &quot;, &quot; whitespaces &quot;) bad_text %&gt;% str_trim(side = &quot;left&quot;) ## [1] &quot;This&quot; &quot;example &quot; &quot;has several &quot; &quot;whitespaces &quot; bad_text %&gt;% str_trim(side = &quot;both&quot;) ## [1] &quot;This&quot; &quot;example&quot; &quot;has several&quot; &quot;whitespaces&quot; str_extract: Chiết xuất giá trị khỏi chuỗi str_extract_all: Chiết xuất giá trị khỏi chuỗi, trả ra list strings &lt;- c( &quot;apple&quot;, &quot;219 733 8965&quot;, &quot;329-293-8753&quot;, &quot;Work: 579-499-7527; Home: 543.355.3679&quot; ) # Chiết xuất giá trị 5 ra khỏi chuỗi strings %&gt;% str_extract(&quot;5&quot;) ## [1] NA &quot;5&quot; &quot;5&quot; &quot;5&quot; strings %&gt;% str_extract_all(&quot;5&quot;) ## [[1]] ## character(0) ## ## [[2]] ## [1] &quot;5&quot; ## ## [[3]] ## [1] &quot;5&quot; ## ## [[4]] ## [1] &quot;5&quot; &quot;5&quot; &quot;5&quot; &quot;5&quot; &quot;5&quot; str_count: Kiểm tra số lần xuất hiện của chuỗi con trong chuỗi states &lt;- rownames(USArrests) states ## [1] &quot;Alabama&quot; &quot;Alaska&quot; &quot;Arizona&quot; &quot;Arkansas&quot; ## [5] &quot;California&quot; &quot;Colorado&quot; &quot;Connecticut&quot; &quot;Delaware&quot; ## [9] &quot;Florida&quot; &quot;Georgia&quot; &quot;Hawaii&quot; &quot;Idaho&quot; ## [13] &quot;Illinois&quot; &quot;Indiana&quot; &quot;Iowa&quot; &quot;Kansas&quot; ## [17] &quot;Kentucky&quot; &quot;Louisiana&quot; &quot;Maine&quot; &quot;Maryland&quot; ## [21] &quot;Massachusetts&quot; &quot;Michigan&quot; &quot;Minnesota&quot; &quot;Mississippi&quot; ## [25] &quot;Missouri&quot; &quot;Montana&quot; &quot;Nebraska&quot; &quot;Nevada&quot; ## [29] &quot;New Hampshire&quot; &quot;New Jersey&quot; &quot;New Mexico&quot; &quot;New York&quot; ## [33] &quot;North Carolina&quot; &quot;North Dakota&quot; &quot;Ohio&quot; &quot;Oklahoma&quot; ## [37] &quot;Oregon&quot; &quot;Pennsylvania&quot; &quot;Rhode Island&quot; &quot;South Carolina&quot; ## [41] &quot;South Dakota&quot; &quot;Tennessee&quot; &quot;Texas&quot; &quot;Utah&quot; ## [45] &quot;Vermont&quot; &quot;Virginia&quot; &quot;Washington&quot; &quot;West Virginia&quot; ## [49] &quot;Wisconsin&quot; &quot;Wyoming&quot; #Kiểm tra độ dài ký tự trong vector states %&gt;% str_length() ## [1] 7 6 7 8 10 8 11 8 7 7 6 5 8 7 4 6 8 9 5 8 13 8 9 ## [24] 11 8 7 8 6 13 10 10 8 14 12 4 8 6 12 12 14 12 9 5 4 7 8 ## [47] 10 13 9 7 #Đếm số lần xuất hiện ký tự trong vector str_count(states %&gt;% toupper(), &quot;A&quot;) ## [1] 4 3 2 3 2 1 0 2 1 1 2 1 0 2 1 2 0 2 1 2 2 1 1 0 0 2 2 2 1 0 0 0 2 2 0 ## [36] 2 0 2 1 2 2 0 1 1 0 1 1 1 0 0 str_detect: Kiểm tra tồn tại của ký tự trong chuỗi string &lt;- c(&quot;Hello&quot;, &quot;Vietnam&quot;, &quot;Apple&quot;) str_detect(string, &quot;l&quot;) ## [1] TRUE FALSE TRUE str_replace: Thay thế ký tự đầu tiên trong chuỗi, str_replace_all - thay thế tất cả ký tự trong chuỗi string &lt;- c(&quot;Hello&quot;, &quot;Vietnam&quot;, &quot;Apple&quot;, &quot;&quot;, NA) # Thay thế ký tự đầu tiên string %&gt;% str_replace(&quot;l&quot;, &quot;99&quot;) ## [1] &quot;He99lo&quot; &quot;Vietnam&quot; &quot;App99e&quot; &quot;&quot; NA # Thay thế tất cả ký tự string %&gt;% str_replace_all(&quot;l&quot;, &quot;99&quot;) ## [1] &quot;He9999o&quot; &quot;Vietnam&quot; &quot;App99e&quot; &quot;&quot; NA # Thay thế NA string %&gt;% str_replace_na(&quot;9999&quot;) ## [1] &quot;Hello&quot; &quot;Vietnam&quot; &quot;Apple&quot; &quot;&quot; &quot;9999&quot; Lưu ý: \"\" được coi là một ký tự vẫn có độ dài bằng 0 &quot;&quot; %&gt;% str_length() ## [1] 0 str_locate: Kiểm tra vị trí đầu tiên xuất hiện ký tự, str_locate_all - kiểm tra tất cả các vị trí xuất hiện ký tự &quot;abbbbcab&quot; %&gt;% str_locate(&quot;a&quot;) ## start end ## [1,] 1 1 &quot;abbbbcaab&quot; %&gt;% str_locate_all(&quot;a&quot;) ## [[1]] ## start end ## [1,] 1 1 ## [2,] 7 7 ## [3,] 8 8 7.2 Regular expression Regular expression (regex) là việc chỉ dẫn một hàm cách thức tìm, thay thế và xử lý dữ liệu dạng string. Cơ bản có 4 dạng: Concetenation (chuỗi ký tự): Tìm chuỗi ký tự: “abcd” Logical (phép logic): Tìm chuỗi chứa ab hoặc cd: “ab|cd” Repetition (lặp lại ký tự): Tìm chuỗi lặp lại một hoặc nhiều lần ký tự a: “a+” Grouping (Nhóm ký tự): Nhóm các ký tự muốn tìm, sử dụng dấu ngoặc đơn () Khi phân tích regex, có hai khía cạnh cần quan tâm: Hàm sử dụng trên R Cách thức tương tác Đối với cách thức tương tác trong R, có các nhóm vấn đề sau: Metadata Sequennces Character class 7.2.1 Metadata Metadata là các ký hiệu đặc biệt được thể hiện trong R nhằm biểu đạt toán tử nhất định. Để làm việc với các ký tự có định dạng giống meta data, cần thêm dâu \\\\. Xem ví dụ dưới đây x &lt;- &quot;Vietnam|HaNoi&quot; # Không đúng định dang x %&gt;% str_replace_all(&quot;|&quot;, &quot;_&quot;) ## [1] &quot;_V_i_e_t_n_a_m_|_H_a_N_o_i_&quot; # Sử dụng đúng metadata x %&gt;% str_replace_all(&quot;\\\\|&quot;, &quot;_&quot;) ## [1] &quot;Vietnam_HaNoi&quot; Trong trường hợp trên, gsub xử lý ký tự $ như regular expression cho các ký tự đặc biệt. gsub(pattern = &quot;\\\\$&quot;, replacement = &quot;.&quot;, &quot;Vietnam$Ha$Noi&quot;) ## [1] &quot;Vietnam.Ha.Noi&quot; Metadata trong regular expressions bao gồm các ký tự sau: $ * + . ? [ ] ^ { } | ( ) Ký tự Ý nghĩa . matches everything except for the empty sting ``. + the preceding item will be matched one or more times. * the preceding item will be matched zero or more times. ^ matches the empty string at the at the beginning of a line $ matches empty string at the end of a line. | infix operator: OR () brackets for grouping. [] character class brackets Lưu ý: Khi sử dụng dấu gạch ngoặc kép (\\\\) là ký tự bình thưởng, sử dụng 4 gạch chéo. Xem các ví dụ dưới đây meta_char &lt;- c(&quot;$&quot;,&quot;*&quot;,&quot;+&quot;,&quot;.&quot;,&quot;?&quot;,&quot;[&quot;,&quot;^&quot;,&quot;{&quot;,&quot;|&quot;,&quot;(&quot;,&quot;\\\\&quot;) meta_char ## [1] &quot;$&quot; &quot;*&quot; &quot;+&quot; &quot;.&quot; &quot;?&quot; &quot;[&quot; &quot;^&quot; &quot;{&quot; &quot;|&quot; &quot;(&quot; &quot;\\\\&quot; #Không sử dụng chính xác str_locate(meta_char, &quot;^&quot;) ## start end ## [1,] 1 0 ## [2,] 1 0 ## [3,] 1 0 ## [4,] 1 0 ## [5,] 1 0 ## [6,] 1 0 ## [7,] 1 0 ## [8,] 1 0 ## [9,] 1 0 ## [10,] 1 0 ## [11,] 1 0 #Sử dụng chính xác meta_char %&gt;% str_locate(&quot;\\\\^&quot;) ## start end ## [1,] NA NA ## [2,] NA NA ## [3,] NA NA ## [4,] NA NA ## [5,] NA NA ## [6,] NA NA ## [7,] 1 1 ## [8,] NA NA ## [9,] NA NA ## [10,] NA NA ## [11,] NA NA #Hai dâu gạch chéo meta_char %&gt;% str_locate(&quot;\\\\\\\\&quot;) ## start end ## [1,] NA NA ## [2,] NA NA ## [3,] NA NA ## [4,] NA NA ## [5,] NA NA ## [6,] NA NA ## [7,] NA NA ## [8,] NA NA ## [9,] NA NA ## [10,] NA NA ## [11,] 1 1 7.2.2 Sequences Sequence trong R có các nhóm sau: #sub thay thế giá trị phù hợp đầu tiên sub(&quot;\\\\d&quot;,&quot;*&quot;, &quot;Y2K was in 2000&quot;) ## [1] &quot;Y*K was in 2000&quot; #gsub thay thế tất cả các giá trị phù hợp gsub(&quot;\\\\d&quot;,&quot;*&quot;, &quot;Y2K was in 2000&quot;) ## [1] &quot;Y*K was in ****&quot; #Thay thế những ký tự không phải là số gsub(&quot;\\\\D&quot;,&quot;*&quot;, &quot;Y2K was in 2000&quot;) ## [1] &quot;*2*********2000&quot; #Thay thế space gsub(&quot;\\\\s&quot;,&quot;*&quot;, &quot;Y2K was in 2000&quot;) ## [1] &quot;Y2K*was*in*2000&quot; #Non space gsub(&quot;\\\\S&quot;,&quot;*&quot;, &quot;Y2K was in 2000&quot;) ## [1] &quot;*** *** ** ****&quot; 7.2.3 Character class Character class sẽ tìm MỘT ký tự phù hợp trong bracket [] #Tìm ký tự từ a-z gsub(&quot;[a-z]&quot;, &quot;*&quot;, &quot;Vietnam in 2016&quot;) ## [1] &quot;V****** ** 2016&quot; #Tìm số từ 0-9 gsub(&quot;[0-9]&quot;, &quot;*&quot;, &quot;Vietnam in 2016&quot;) ## [1] &quot;Vietnam in ****&quot; #Tìm các ký tự trừ số grep(&quot;[^0-9]&quot;, c(&quot;Vietnam in&quot;, &quot; $%^$&quot;, &quot;2016&quot;), value = T) ## [1] &quot;Vietnam in&quot; &quot; $%^$&quot; 7.2.4 POSIX class Posix class được thể hiện trong 2 dấu ngoặc kép định dạng [[:class:]] như sau. example &lt;- c(&quot;Alo ala #^ 12,6.7&quot;) #Loại blank gsub(&quot;[[:blank:]]&quot;, &quot;&quot;, example) ## [1] &quot;Aloala#^12,6.7&quot; #Loại dấu gsub(&quot;[[:punct:]]&quot;, &quot;&quot;, example) ## [1] &quot;Alo ala 1267&quot; 7.2.5 Quantifiers Quantifier là nhóm điều kiện đếm số lần lặp lại của 1 nhóm ký tự # people names people = c(&quot;rori&quot;, &quot;emilia&quot;, &quot;matteo&quot;, &quot;mehmet&quot;, &quot;filipe&quot;, &quot;anna&quot;, &quot;tyler&quot;, &quot;rasmus&quot;, &quot;jacob&quot;, &quot;youna&quot;, &quot;flora&quot;, &quot;adi&quot;) # Tìm tên có xuất hiện m nhiều nhất 1 lần grep(pattern = &quot;m?&quot;, people, value = TRUE) ## [1] &quot;rori&quot; &quot;emilia&quot; &quot;matteo&quot; &quot;mehmet&quot; &quot;filipe&quot; &quot;anna&quot; &quot;tyler&quot; ## [8] &quot;rasmus&quot; &quot;jacob&quot; &quot;youna&quot; &quot;flora&quot; &quot;adi&quot; # Tên xuất hiện m chính xác 1 lần grep(pattern = &quot;m{1}&quot;, people, value = TRUE, perl = FALSE) ## [1] &quot;emilia&quot; &quot;matteo&quot; &quot;mehmet&quot; &quot;rasmus&quot; # Tên có xuất hiện hoặc không có xuất hiện m rồi xuất hiện t grep(&quot;m*t&quot;, people, value = T) ## [1] &quot;matteo&quot; &quot;mehmet&quot; &quot;tyler&quot; #Tên có m một hoặc nhiều lần grep(&quot;m+t&quot;, people, value = T) ## character(0) #Tên có m một hoặc nhiều lần rồi đến t grep(&quot;m+.t&quot;, people, value = T) ## [1] &quot;matteo&quot; &quot;mehmet&quot; paris_tweets = c( &quot;#Paris is chock-full of cultural and culinary attractions&quot;, &quot;Some time in #Paris along Canal St.-Martin famous by #Amelie&quot;, &quot;While you&#39;re in #Paris, stop at cafe: http://goo.gl/yaCbW&quot;, &quot;Paris, the city of light&quot;) # match (all) hashtags in &#39;paris_tweets&#39; str_match_all(paris_tweets, &quot;#[a-zA-Z]{1,}&quot;) ## [[1]] ## [,1] ## [1,] &quot;#Paris&quot; ## ## [[2]] ## [,1] ## [1,] &quot;#Paris&quot; ## [2,] &quot;#Amelie&quot; ## ## [[3]] ## [,1] ## [1,] &quot;#Paris&quot; ## ## [[4]] ## [,1] 7.3 Các ví dụ tổng hợp 7.3.1 Ví dụ 1 Vấn đề: Thay thế các ký tự đặc biệt lặp đi lặp lại Giải pháp: Sử dụng dấu + d1 = data.frame( id...of....patient = c(1, 2), patient....age = c(1, + 2)) names(d1) &lt;- gsub(pattern = &quot;\\\\.+&quot;, replacement = &quot;_&quot;, x = names(d1)) names(d1) ## [1] &quot;id_of_patient&quot; &quot;patient_age&quot; 7.3.2 Ví dụ 2 Vấn đề: Lọc các ký tự trong 1 nhóm Giải pháp: Sử dụng ký tự [] Có thể sử dụng dấu - để tìm trong khoảng #Tìm vị trí các ký tự grep(&quot;[abcAB]&quot;, x = c(&quot;Con ca&quot;, &quot;Boong&quot;, &quot;Xoong&quot;)) ## [1] 1 2 grep(&quot;[0-3]&quot;, c(&quot;nose&quot;, &quot;letter38&quot;, &quot;window9&quot;, &quot;apple0&quot;), value = TRUE) ## [1] &quot;letter38&quot; &quot;apple0&quot; Chi tiết xem trong bảng dưới đây 7.3.3 Ví dụ 3 d &lt;- data.frame( id = c(11, 22, 33, 44, 55, 66, 77, 88), drug = c(&quot;vitamin E&quot;, &quot;vitamin ESTER-C&quot;, &quot; vitamin Eabc &quot;, &quot;vitamin E(ointment)&quot;, &quot;&quot;, &quot;vitamin E &quot;, &quot;provitamin E\\n&quot;, &quot;vit E&quot;), text = c(&quot;&quot;,&quot; &quot;, &quot; 3 times a day after meal&quot;, &quot;once a day&quot;, &quot; &quot;, &quot; one per day &quot;, &quot;\\t&quot;, &quot;\\n &quot;), stringsAsFactors = FALSE) (s &lt;- d$text) ## [1] &quot;&quot; &quot; &quot; ## [3] &quot; 3 times a day after meal&quot; &quot;once a day&quot; ## [5] &quot; &quot; &quot; one per day &quot; ## [7] &quot;\\t&quot; &quot;\\n &quot; #Các giá trị riêng của s unique(s) ## [1] &quot;&quot; &quot; &quot; ## [3] &quot; 3 times a day after meal&quot; &quot;once a day&quot; ## [5] &quot; one per day &quot; &quot;\\t&quot; ## [7] &quot;\\n &quot; #Xử lý ký tự #Các ký tự trong ngoặc vuông chuyển thành NA gsub(&quot;[\\t\\n\\r\\f\\v]+&quot;, NA, s) ## [1] &quot;&quot; &quot; &quot; ## [3] &quot; 3 times a day after meal&quot; &quot;once a day&quot; ## [5] &quot; &quot; &quot; one per day &quot; ## [7] NA NA #Chuyển tất cả các ký tự gsub(&quot;^$|^( +)$|[\\t\\n\\r\\f\\v]+&quot;, NA, s) ## [1] NA NA ## [3] &quot; 3 times a day after meal&quot; &quot;once a day&quot; ## [5] NA &quot; one per day &quot; ## [7] NA NA #Chuyển tất cả các khoảng trắng sang NULL gsub(&quot;^([ \\t\\n\\r\\f\\v]+)|([ \\t\\n\\r\\f\\v]+)$&quot;, &quot;&quot;, s) ## [1] &quot;&quot; &quot;&quot; ## [3] &quot;3 times a day after meal&quot; &quot;once a day&quot; ## [5] &quot;&quot; &quot;one per day&quot; ## [7] &quot;&quot; &quot;&quot; ###### #Tìm vitamin e t &lt;- d$drug grep(&quot;vitamin e&quot;, t, ignore.case = T, value = T) ## [1] &quot;vitamin E&quot; &quot;vitamin ESTER-C&quot; &quot; vitamin Eabc &quot; ## [4] &quot;vitamin E(ointment)&quot; &quot;vitamin E &quot; &quot;provitamin E\\n&quot; #Tìm vitamin e mà đi sau đi kèm ký tự alphabet grep(&quot;vitamin e([a-zA-Z])&quot;, t, ignore.case = T, value = T) ## [1] &quot;vitamin ESTER-C&quot; &quot; vitamin Eabc &quot; #Tìm vitamin e mà đi sau không đi kèm gì grep(&quot;vitamin e($)&quot;, t, ignore.case = T, value = T) ## [1] &quot;vitamin E&quot; #Tìm vitamin e mà đi sau không phải là ký tự alphabet grep(&quot;vitamin e($|[^a-zA-Z])&quot;, s, ignore.case = TRUE, value = TRUE) ## character(0) #Tìm thêm vit e grep(&quot;vitamin e($|[^a-zA-Z])|vit e($|[^a-zA-Z])&quot;, t, ignore.case = TRUE, value = TRUE) ## [1] &quot;vitamin E&quot; &quot;vitamin E(ointment)&quot; &quot;vitamin E &quot; ## [4] &quot;provitamin E\\n&quot; &quot;vit E&quot; #Tìm ký tự bắt đầu với _vit grep(&quot; vit&quot;, t, ignore.case = T, value = T) ## [1] &quot; vitamin Eabc &quot; #Loại prodvitamin grep(&quot;([^a-z]+|^)vitamin e($|[^a-zA-Z])|([^a-z]+|^)vit e($|[^a-zA-Z])&quot;, t, ignore.case = TRUE, value = TRUE) ## [1] &quot;vitamin E&quot; &quot;vitamin E(ointment)&quot; &quot;vitamin E &quot; ## [4] &quot;vit E&quot; 7.3.4 Ví dụ 4 Vấn đề: Thay thế ký tự rỗng Giải pháp: Sử dụng \"^$\" df &lt;- data.frame(x = c(&quot;1&quot;, &quot;viet&quot;, &quot;&quot;)) #Không chạy df$x %&gt;% as.character %&gt;% str_replace_na(&quot;Missing&quot;) ## [1] &quot;1&quot; &quot;viet&quot; &quot;&quot; #Chạy df$x %&gt;% as.character %&gt;% str_replace_all(&quot;^$&quot;, &quot;Missing&quot;) ## [1] &quot;1&quot; &quot;viet&quot; &quot;Missing&quot; 7.4 Tài liệu tham khảo gsub "],
["quan-ly-kt-qua-phan-tich-t-nhiu-mo-hinh.html", "Chương 8 Quản lý kết quả phân tích từ nhiều mô hình", " Chương 8 Quản lý kết quả phân tích từ nhiều mô hình Khi phân tích nhiều mô hình cùng lúc trong R, output từ các mô hình thường được lưu ở dạng list và rất khó kết hợp với nhau. Nhiều trường hợp, ta cần xây dựng cùng lúc nhiều mô hình và tổng hợp cùng lúc kết quả từ các mô hình này. Broom cho phép làm sạch các output của mô hình. Các hàm giúp làm sạch output: tidy: Data frame cho phép tổng hợp kết quả của các mô hình, bao gồm coefficient, p-value augment: Thêm cột vào tập dữ liệu được phân tích glance: Tổng quan các chỉ số của nhiều mô hình library(tidyverse) library(broom) lmfit &lt;- lm(mpg ~ wt, mtcars) lmfit %&gt;% summary ## ## Call: ## lm(formula = mpg ~ wt, data = mtcars) ## ## Residuals: ## Min 1Q Median 3Q Max ## -4.5432 -2.3647 -0.1252 1.4096 6.8727 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 37.2851 1.8776 19.858 &lt; 2e-16 *** ## wt -5.3445 0.5591 -9.559 1.29e-10 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 3.046 on 30 degrees of freedom ## Multiple R-squared: 0.7528, Adjusted R-squared: 0.7446 ## F-statistic: 91.38 on 1 and 30 DF, p-value: 1.294e-10 #Tidy toàn bộ mô hình lmfit %&gt;% tidy ## # A tibble: 2 x 5 ## term estimate std.error statistic p.value ## &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 (Intercept) 37.3 1.88 19.9 8.24e-19 ## 2 wt -5.34 0.559 -9.56 1.29e-10 #Nhìn tổng quan mô hình lmfit %&gt;% glance ## # A tibble: 1 x 11 ## r.squared adj.r.squared sigma statistic p.value df logLik AIC BIC ## &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 0.753 0.745 3.05 91.4 1.29e-10 2 -80.0 166. 170. ## # ... with 2 more variables: deviance &lt;dbl&gt;, df.residual &lt;int&gt; #Nối kết quả với mô hình lmfit %&gt;% augment %&gt;% head ## # A tibble: 6 x 10 ## .rownames mpg wt .fitted .se.fit .resid .hat .sigma .cooksd ## &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 Mazda RX4 21 2.62 23.3 0.634 -2.28 0.0433 3.07 1.33e-2 ## 2 Mazda RX~ 21 2.88 21.9 0.571 -0.920 0.0352 3.09 1.72e-3 ## 3 Datsun 7~ 22.8 2.32 24.9 0.736 -2.09 0.0584 3.07 1.54e-2 ## 4 Hornet 4~ 21.4 3.22 20.1 0.538 1.30 0.0313 3.09 3.02e-3 ## 5 Hornet S~ 18.7 3.44 18.9 0.553 -0.200 0.0329 3.10 7.60e-5 ## 6 Valiant 18.1 3.46 18.8 0.555 -0.693 0.0332 3.10 9.21e-4 ## # ... with 1 more variable: .std.resid &lt;dbl&gt; Khi xây dựng mô hình, ta có thể kết hợp broom và dplyr như sau. mtcars %&gt;% group_by(cyl) %&gt;% do(lm(mpg ~ disp, data=.) %&gt;% tidy) ## # A tibble: 6 x 6 ## # Groups: cyl [3] ## cyl term estimate std.error statistic p.value ## &lt;dbl&gt; &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 4 (Intercept) 40.9 3.59 11.4 0.00000120 ## 2 4 disp -0.135 0.0332 -4.07 0.00278 ## 3 6 (Intercept) 19.1 2.91 6.55 0.00124 ## 4 6 disp 0.00361 0.0156 0.232 0.826 ## 5 8 (Intercept) 22.0 3.35 6.59 0.0000259 ## 6 8 disp -0.0196 0.00932 -2.11 0.0568 Tuy nhiên, khi dữ liệu có quá nhiều, ta có thể sử dụng purrr thay cho broom như sau library(purrr) library(broom) mtcars %&gt;% split(.$cyl) %&gt;% map(~lm(mpg ~ wt, data = .)) %&gt;% map(summary) %&gt;% map_df(tidy) ## # A tibble: 6 x 5 ## term estimate std.error statistic p.value ## &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 (Intercept) 39.6 4.35 9.10 0.00000777 ## 2 wt -5.65 1.85 -3.05 0.0137 ## 3 (Intercept) 28.4 4.18 6.79 0.00105 ## 4 wt -2.78 1.33 -2.08 0.0918 ## 5 (Intercept) 23.9 3.01 7.94 0.00000405 ## 6 wt -2.19 0.739 -2.97 0.0118 #Version với broom mtcars %&gt;% group_by(cyl) %&gt;% do(lm(mpg ~ wt, data = .) %&gt;% tidy) ## # A tibble: 6 x 6 ## # Groups: cyl [3] ## cyl term estimate std.error statistic p.value ## &lt;dbl&gt; &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 4 (Intercept) 39.6 4.35 9.10 0.00000777 ## 2 4 wt -5.65 1.85 -3.05 0.0137 ## 3 6 (Intercept) 28.4 4.18 6.79 0.00105 ## 4 6 wt -2.78 1.33 -2.08 0.0918 ## 5 8 (Intercept) 23.9 3.01 7.94 0.00000405 ## 6 8 wt -2.19 0.739 -2.97 0.0118 Nhóm hàm của broom có thể sử dụng trong rất nhiều nhóm mô hình, bao gồm anova, t.test, quantile regression, hồi quy tuyến tính. Việc kết hợp và sử dụng nhóm hàm này sẽ giúp ích rất nhiều trong việc phân tích khám phá dữ liệu "],
["cac-nguyen-ly-d-bao.html", "Chương 9 Các nguyên lý dự báo 9.1 Giới thiệu 9.2 Các nguyên lý trong dự báo 9.3 Quy trình xây dựng mô hình dự báo 9.4 Ensemble methods 9.5 Lưu ý", " Chương 9 Các nguyên lý dự báo Trong chương này, chúng ta sẽ tìm hiểu những khái niệm và nguyên lý cơ bản nhất của học máy (machine learning hay statistical learning). Các nguyên lý này sẽ giúp ta nắm vững để có thể phát triển nhanh chóng trong lĩnh vực dự báo. Khi đã nắm vững các nguyên lý này, việc xây dựng mô hình với các thuật toán khác nhau sẽ không còn quan trọng nữa bởi tất cả sẽ đều phải đi qua các nguyên lý giống nhau. 9.1 Giới thiệu 9.1.1 Các nhánh trong học máy (machine learning) Statistical Learning (SL) hay Machine Learning là nghành học sử dụng nhiều phương pháp và công cụ toán học khác nhau để tìm hiểu về cấu trúc của dữ liệu. ML có thể được chia thành 2 dạng: Định hướng &amp; không định hướng: Phân tích có định hướng trước (Supervised learning): Xây dựng các mô hình giữa biến phụ thuộc với một hoặc nhiều biến độc lập. Trong đó, kết quả đầu ra y đã được xác định trước. Ví dụ: Dự báo khách hàng vỡ nợ dựa vào các đặc trưng về nhân khẩu học và hành vi giao dịch của khách hàng. Các thuật toán như cây quyết định, logistics, mô hình hồi quy tuyến tính đều thuộc loại này. Tuy nhiên, tùy thuộc vào biến cần dự báo, ta lại có hai nhóm nhỏ sau: Bài toán phân loại (classification): Khi biến phụ thuộc là các biến định dạng nhóm (category). Ví dụ, khách hàng tốt hay xấu, khách hàng mua hay không mua sản phẩm,… Bài toán dự báo (regression): Khi biến phụ thuộc là biến số cần dự báo giá trị. Ví dụ, giá trị của tổng các giao dịch một khách hàng có trong 1 tháng… Phân tích không định hướng trước(Unsupervised learning): Biến phụ thuộc chưa biết trước và mục tiêu của mô hình là tìm ra các mối qua hệ ẩn giữa các nhóm. . Ví dụ, phân nhóm khách hàng thành 5 nhóm các hành vi tương tự nhau. Các thuật toán như apriori, k-means, PCA, FA thuộc nhóm này. Các bài toán loại này có thể chia làm 2 nhóm lớn như sau: Bài toán phân nhóm (clustering): Bài toán loại này giúp ta chia tập dữ liệu sẵn có thành nhiều nhóm khác nhau để sao cho đặc trưng của mỗi nhóm là gần nhau nhất Bài toán tìm thành phần chính (PCA): Bài toán này giúp ta giảm số lượng các biến có sẵn trong dữ liệu gốc nhưng vẫn đảm bảo thể hiện được cấu trúc của toàn bộ dữ liệu 9.1.2 Cách xây dựng mô hình Khi bắt đầu tìm hiểu về học máy hoặc kinh tế lượng, ta sẽ thường xuyên gặp khái niệm mô hình. Mô hình là cách thức thể hiện mối quan hệ giữa các biến thông quan các công cụ toán học. Do mô hình là cách thức đơn giản hóa mối quan hệ giữa các biến trên thực tế, do đó, mô hình còn được nhiều nhà phân tích gọi là giả thuyết. Khi xây dựng một mô hình dự báo, bản chất là ta dựa vào tập dữ liệu cho trước, áp dụng một thuật toán để đưa ra một mô hình (giả thuyết) về mối quan hệ giữa biến đầu vào và đầu ra. Khi xây dựng mô hình, ta thường có ba tập dữ liệu Train: Tập dữ liệu được sử dụng khi xây dựng mô hình Validation: tập dữ liệu dùng để đánh giá chất lượng mô hình được xây trên tập train, cập nhật lại các hyper-parameters mô hình để đưa ra mô hình cuối cùng. Test: Tập dữ liệu độc lập dùng để đánh giá chất lượng mô hình cuối cùng Mô hình (hay giả thuyết h) giữa biến đầu vào và đầu ra được thể hiện như sau. \\[h_{\\theta} = \\theta_0 + \\theta_1 \\times X\\] \\(h_{\\theta}\\) được gọi là giả thuyết hay mô hình. \\(\\theta\\) là tham số của mô hình. Với các bài toán (supervised vs. nonsupervised) khác nhau, phương trình trên sẽ được thay đổi để phù hợp với bài toán thực tế. Khi xây dựng mô hình, tất cả các thuật toán của ML đều trải qua ba bước cơ bản theo sơ đồ sau. Bước 1: Xây dựng mô hình với tham số bất kỳ. Với mỗi mô hình, sẽ có các tham số khác nhau. Ví dụ, mô hình OLS, tham số là hệ số \\(\\beta\\) trong mô hình \\(y = \\beta*X\\). Bước 2: Đo lường sai số mô hình so với thực tế. Bước 3: Update lại tham số của mô hình để giảm thiểu sai số giữa mô hình và thực tế. Thuật toán sẽ tiếp tục diễn ra cho đến khi sai số của mô hình nhỏ hơn 1 mức sai số định trước. Với mô hình phân tích có định hướng (tồn tại biến Y cần dự báo), phương trình của mô hình dự báo có thể biểu diễn dưới dạng. \\[Y = f(X)\\] Trong đó: Y được gọi là biến phụ thuộc (dependent variables) X được gọi là biến độc lập (independent variables) hay biến dự báo (predictors) 9.1.3 Sử dụng mô hình Sau khi xây dựng, bước tiếp theo là sử dụng mô hình trong việc dự báo thực tế. Quy trình xây dựng phân tích dữ liệu thực tế 9.2 Các nguyên lý trong dự báo 9.2.1 Reducible vs. irreducible error Trong thực tế, mối quan hệ giữa X &amp; Y được biểu diễn qua hàm sau: \\[Y = f(X) + \\epsilon \\] Khi phân tích dữ liệu, ta tìm hàm \\(\\hat(Y)=\\hat{f}(X)\\) gần nhất với \\(f(X)\\). Sai số giữa thực tế và mô hình sẽ là: \\[E(Y-\\hat{Y})^2 = E[f(X) - \\hat{f}(X) - \\epsilon]^2 \\\\=E[(f(x) - \\hat{f}(X))^2 - 2*\\epsilon*(f(x) - \\hat{f}(X)) + \\epsilon^2)] \\\\=E[(f(x) - \\hat{f}(X))^2] - \\underbrace{2*E(\\epsilon)}_{= 0}*E(f(x) - \\hat{f}(X))) + E(\\epsilon^2) \\\\=E[(f(x) - \\hat{f}(X))^2] + [E(\\epsilon^2) - \\underbrace{E(\\epsilon)^2}_{=0}] \\\\= E\\underbrace{[f(X) - \\hat{f}(X)]^2}_{reducible} + \\underbrace{Var(\\epsilon)}_{irreducible}\\] Lưu ý: \\(Var(\\epsilon) = E(\\epsilon^2) - E(\\epsilon)^2\\) Khi xây dựng mô hình, ta chỉ có thể giảm bớt phần reducible error mà không thể giảm được phần variance của sai số. Do đó, mô hình sẽ không thể đạt được độ chính xác 100% mà luôn tồn tại một mức sai số nhất định. 9.2.2 Khả năng giải thích và khả năng dự báo Khi xây dựng mô hình, có hai khía cạnh cần phải xử lý: Khả năng giải thích hay khả năng rút ra kết luận từ mô hình (inference): Nhấn mạnh đến khả năng diễn đạt ý nghĩa các biến trong mô hình. Các câu hỏi thường dùng là: Biến độc lập (predictors) nào có quan hệ chặt chẽ với biến cần dự báo (dependent variables)? Mối quan hệ giữa biến độc lập và biến dự báo là gì? Mối quan hệ này có thể biểu diễn một cách đơn giản dạng mô hình tuyến tính hay phải mô tả dưới dạng phức tạp hơn? Ví dụ về khả năng giải thích của mô hình: Khách hàng trả nợ trễ hạn 3 lần sẽ làm tăng khả năng trốn nợ lên 20% Giá giảm 10% sẽ khiên doanh thu tăng thêm khoảng 6%. Các thuật toán như OLS, apriori, Logistics thuộc nhóm này. Khả năng dự báo của mô hình (Predictor): Ưu tiên hơn đến tính chính xác của mô hình dự báo, không quan tâm đến việc mô tả quan hệ giữa các biến. Ví dụ: Random Forest, Neuron Network, KNN Nguyên lý: Khi xây dựng mô hình, ta buộc phải dánh đổi giữa độ chính xác của mô hình vs. khả năng diễn giải mô hình và không tồn tại một mô hình tốt nhất cho mọi trường hợp. Do đó, ta cần phải lựa chọn mô hình theo từng đối tượng. Mô hình có độ chính xác cao thường khó mô tả mối quan hệ giữa các biến (VD: decision tree) và ngược lại (VD: OLS) 9.2.3 Mô hình có tham số cho trước hoặc không có tham số cho trước (Parametric vs. Nonparametric) Đây là hai loại hai trường phái được sử dụng khi xây dựng các mô hình thống kê. Parametric: Đưa ra mô hình biểu diễn mối quan hệ trước rồi sau đó ước lượng mô hình đưa ra. VD: OLS, Logistic Regression Nonparametric: Không đưa ra mô hình, chỉ đưa ra phương pháp và để thuật toán tự động tìm kết quả. VD: Association rule, decision tree… 9.2.4 Nguyên lý chữ U Khi xây dựng mô hình, để đánh giá chất lượng, ta sẽ đo lường sai số trên tập train và tập test. Sai số của mô hình được tính như sau (với trường hợp regression) \\[MSE = \\frac{1}{n} \\sum^n_{i=1}(y_i - \\hat{f}(x_i))^2\\] Khi đánh giá chất lượng mô hình, ta cần đánh giá trên cả tập train và tập test. Thông thường, mô hình sẽ được xây trên tập train và được đánh giá trên tập test. Các tham số trên tập train sẽ được thay đổi để sai số mô hình được tối ưu. Số lượng tham số cần tối ưu càng nhiều, mô hình càng phức tạp. Ví dụ: Ta cần dự báo \\(income\\), ta có hai mô hình sau. STT Phương trình 1 \\(income = \\beta_0 + \\beta_1*age\\) 2 \\(income = \\beta_0 + \\beta_1*age + \\beta_2*experience\\) Trong hai mô hình trên, mô hình 2 được gọi là phức tạp hơn mô hình 1 (compexity level). Khi xây dựng mô hình, nguyên lý chữ U cho ta biết về sự thay đổi sai số trên hai tập train và test theo độ phức tạp của mô hình như sau. Khi mô hình có độ phức tạp càng cao thì sai số của tập train sẽ ngày càng giảm trong khi sai số của tập test sẽ có dạng chữ U Lưu ý: Nguyên lý chữ U cho ta thấy, việc tăng thêm biến vào mô hình (# variables) không phải lúc nào cũng làm tăng chất lượng mô hình Khi xây dựng mô hình, cần phải tìm được điểm cân bằng giữa độ phức tạp mô hình và độ chính xác. Điểm này chính là điểm thấp nhất trên đường chữ U của sai số trên tập test 9.2.5 Bias Variance Trade-Off Người ta chứng minh được rằng: \\[E(y_0 - \\hat{f}(x_0))^2 = Var(\\hat{f}(x_0)) + [Bias(\\hat{f}(x_0))]^2 + Var(\\epsilon)\\] Trong đó: \\(E(y_0 - \\hat{f}(x_0))^2\\): Kỳ vọng của MSE \\(Var(\\hat{f}(x_0))\\): Phương sai của MSE trong tập test khi ta thay đổi tập training \\([Bias(\\hat{f}(x_0))]^2\\): Sai số của mô hình ước lượng \\(\\hat{f}(x)\\) so với mô hình thực tế \\(f(x)\\). VD: Mô hình thực tế là \\(y=x^2\\), mô hình ước lượng là \\(y=0.9*x^2\\) \\(Var(\\epsilon)\\): Phương sai của nhiễu Nguyên lý: Variance của mô hình tăng thì Bias sẽ giảm và ngược lại. Độ biến động (variance) vs. độ chuẩn xác (bias): variance là độ biến động của mô hình so với dữ liệu, đo độ nhạy cảm của các tham số trong mô hình khi thay đổi dữ liệu. Nói cách khác, một mô hình được gọi là biến động lớn khi dữ liệu mô hình thay đổi sẽ dẫn đến một sự thay đổi lớn trong mô hình. Ví dụ, mô hình sử dụng trung vị làm biến dự báo sẽ ít biến động hơn khi sử dụng giá trị trung bình. bias đo độ chuẩn xác của mô hình. Một mô hình được gọi là bias thấp khi kết quả dự báo gần với kết quả thực tế và ngược lại. Các mô hình có bias thấp có khả năng thích ứng với dữ liệu tốt, các mô hình như cây quyết đinh, neural network thuộc dạng này. Phương pháp tính toán phân loại - The Classification Setting \\[training\\_error\\_rate = \\frac{1}{n}\\sum_{i=1}^n I(y_i \\neq \\hat {y_i})\\] Trong đó \\(I(y_i \\neq \\hat{y_i}\\) là “indicator variable”, có giá trị bằng 1 nếu \\(y_i \\neq \\hat{y_i}\\), có giá trị bằng 0 nếu \\(y_i= \\hat{y_i}\\) 9.2.6 Overfitting &amp; regularization Khi xây dựng mô hình dự báo, có thể xảy ra 3 trường hợp: Underfiting: Hiện tượng mô hình quá đơn giản, tính khái quát hóa cao nhưng dự báo không tốt, dẫn đến sai số cả ở tập train &amp; test đều cao. Mô hình underfitting thường đi kèm với bias cao Overfitting là hiện tượng xảy ra khi mô hình hoạt động tốt trên tập train nhưng dự báo rất kém trên tập test. Mô hình loại này sẽ khiến sai số trên các tập dữ liệu khác nhau thường rất khác nhau. Do đó, overfitting còn đi kèm với variance lớn Mô hình được xây dựng tốt: Là loại mô hình hoạt động tốt trên cả tập train &amp; test. 3 trường hợp xây dựng mô hình được thể hiện ở hình dưới đây. library(tidyverse) df &lt;- data.frame( x = seq(0, 13, by = 0.5) ) %&gt;% mutate(good_model = 10 + 2*x - 1/10*x^2) %&gt;% mutate(y = good_model + rnorm(nrow(.), 0, 1)) %&gt;% mutate(underfit = 10 + 1.2*x) %&gt;% mutate(overfit = y + rnorm(nrow(.), 0, 0.01)) library(reshape2) df2 &lt;- df %&gt;% melt(id.vars = c(&quot;x&quot;, &quot;y&quot;)) df2 %&gt;% ggplot() + geom_line(aes(x, value, col = variable)) + geom_point(aes(x, y), col = &quot;black&quot;, alpha = 0.8) + theme_minimal() + theme(legend.position = &quot;top&quot;) + scale_color_discrete(name = &quot;Model&quot;) + labs(title = &quot;Different type of Model&quot;, y = &quot;y&quot;) Để giải quyết vấn đề overfiting, ta có thể làm như sau: Giảm biến bằng cách chọn biến thủ công hoặc áp dụng các kỹ thuật lựa chọn mô hình Regularization: Kỹ thuật cho phép giữ nguyên số lượng biến đầu vào nhưng giảm giá trị của các tham số \\(\\theta\\) trong mô hình Regularization: Regularization cho phép tính toán giá trị của các tham số \\(\\theta\\) trong mô hình, sao cho các giá trị của \\(\\theta\\) càng nhỏ càng tốt. Hàm tối ưu hóa được thay đổi lại như sau. \\[J(\\theta_0. \\theta_1) = \\underset{\\theta_0, \\theta_1}{\\text{minimize}}\\frac{1}{2m}\\sum_{i=1}^m(h_\\theta(x^{(i)}) - y^{(i)})^2 + \\lambda\\sum_{i=1}^n\\theta_j^2\\] Trong đó, \\(\\lambda\\sum_{i=1}^n\\theta_j^2\\) được gọi là tham số regularization. Hệ số, \\(\\lambda\\) sẽ quyết định vị độ mạnh của regularization. Nếu \\(\\lambda\\) càng lớn, yếu tố làm giảm thiểu overfitting càng mạnh. Tuy nhiên, nếu \\(\\lambda\\) quá lớn (ví dụ, \\(10^{10}\\)) sẽ khiến cho toàn bộ các tham số \\(\\theta\\) trở về 0, và mô hình sẽ thay đổi từ overfitting sang underfitting. 9.3 Quy trình xây dựng mô hình dự báo Quá trình dự báo: Trong thực tế, quá trình dự báo diễn ra như sau 9.4 Ensemble methods Ensemble methods (tạm dịch: phương pháp kết hợp) là phương pháp tổng hợp nhiều kết quả khác nhau từ những mô hình riêng biệt để có thể xây dựng một mô hình có độ chính xác cao hơn. Có 3 loại như sau: Bagging: Xây dựng cùng lúc nhiều mô hình cùng loại trên nhiều tập con của train, mô hình cuối là tổng của các mô hình con (VD: Random Forest) Boosting: Nhiều mô hình cùng loại trên nhiều tập con, mô hình sau học và đánh trọng số trên những trường hợp phân loại sai của mô hình trước đó (VD: GBM, XGBoost) Stacking: Là phương pháp sử dụng nhiều mô hình khác loại, kết hợp để tạo thành 1 mô hình tốt hơn từng mô hình riêng rẽ. Xem ví dụ sau: #Dữ liệu thực tế 1111111111 #Kết quả dự báo qua 3 classifier 1111111100 (80%) 0111011101 (70%) 1000101111 (60%) #3 classifier theo số đông 1111111101 (90%) Lưu ý: Phương pháp này thực hiện tốt với những mô hình phân loại ít tương quan với nhau. Với những mô hình tương quan với nhau nhiều, mô hình này sẽ không áp dụng tốt. Xem ví dụ sau: #Dữ liệu thực tế 1111111111 #Kết quả dự báo qua 3 classifier 1111111100 (80%) 1111011101 (80%) 1111011101 (80%) #3 classifier theo số đông 1111011101 (80%) Các phương pháp stacking thông dụng: Linear ensemble: trung bình có trọng số xác suất các dự báo của từng mô hình phân loại. VD: C1(A-80%, B-20%), C2(A-60%,B-40%) =&gt; Mô hình cuối C(70%, B-30%) Max ensemble: Mỗi quan sát lấy xác suất lớn nhất trong tất cả các classifier. VD: C1(A-80%, B-20%), C2(A-60%,B-40%) =&gt; Mô hình cuối C(80%, B-40%) Vote ensemble: Giá trị cuối được lấy vote theo từng classifier Two step ensemble: Lấy điểm xác suất của mỗi mô hình làm biến đầu vào của data frame, sau đó áp dụng mô hình dự báo trên data frame mới. 9.5 Lưu ý Khi xây dựng mô hình, hiệu ứng của các biến dự báo (predictor) có thể lớn hơn thuật toán rất nhiều Với cùng một nhóm các biến dự báo đúng thực tế, các thuật toán khác nhau có thể đưa ra các kết quả tương tự nhau. "],
["mo-hinh-ols.html", "Chương 10 Mô hình OLS 10.1 Giới thiệu 10.2 Xây dựng mô hình cơ bản 10.3 Đánh giá chất lượng mô hình 10.4 Dự báo 10.5 Thuật toán tối ưu với gradient decent", " Chương 10 Mô hình OLS 10.1 Giới thiệu Với mô hình hồi quy tuyến tính, ví dụ, tìm mối quan hệ giữa giá nhà và diện tích, ý tưởng để xây dựng mô hình là tìm một tổ hợp giữa \\(\\theta_0\\) và \\(\\theta_1\\) sao cho khoảng cách giữa mô hình tìm được và các quan sát là nhỏ nhất. Để tìm ra được tổ hợp tham số \\(\\theta\\), mục tiêu của thuật toán là tối ưu hóa tổng khoảng cách giữa các điểm dự báo và các điểm thực tế. Nói cách khác, ta cần tối ưu hàm sau. \\[\\underset{\\theta_0, \\theta_1}{\\text{minimize}}\\sum_{i=1}^m(h_\\theta(x^{(i)}) - y^{(i)})^2\\] Trong đó: \\((i)\\) là thứ tự các quan sát trong tập train \\(m\\) là số lượng quan sát \\(h_\\theta\\) là mô hình ước lượng mối quan hệ giữa x và y \\(y^{(i)}\\) là quan sát thực tế của biến cần dự báo \\(x^{(i)}\\) là các quan sát thực tế của biến inputs Trong thực tế, để dễ dàng hơn trong việc tính toán, ta sẽ tìm giá trị nhỏ nhất của trung bình của bình phương độ lêch. Ta sẽ thay đổi thành hàm sau. \\[J(\\theta_0. \\theta_1) = \\underset{\\theta_0, \\theta_1}{\\text{minimize}}\\frac{1}{2m}\\sum_{i=1}^m(h_\\theta(x^{(i)}) - y^{(i)})^2\\] \\(J(\\theta_0. \\theta_1)\\) được gọi là cost function. Về mặt bản chất, cost function cho phép chúng ta biết được chất lượng của mô hình so với thực tế. Thông thường, giá trị của hàm này cang thấp, chất lượng mô hình càng tốt. 10.2 Xây dựng mô hình cơ bản Để xây dựng mô hình, ta sử dụng tập dữ liệu phân tích về doanh số bán hàng với quảng cáo. # Packages library(tidyverse) library(modelr) library(broom) library(ISLR) # Load data (remove row numbers included as X1 variable) #advertising &lt;- read_csv(&quot;http://www-bcf.usc.edu/~gareth/ISL/Advertising.csv&quot;) %&gt;% # select(-X1) %&gt;% # rename_all(tolower) advertising &lt;- read_csv(&quot;data/Advertising.csv&quot;) %&gt;% select(-X1) %&gt;% rename_all(tolower) advertising %&gt;% head ## # A tibble: 6 x 4 ## tv radio newspaper sales ## &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 230. 37.8 69.2 22.1 ## 2 44.5 39.3 45.1 10.4 ## 3 17.2 45.9 69.3 9.3 ## 4 152. 41.3 58.5 18.5 ## 5 181. 10.8 58.4 12.9 ## 6 8.7 48.9 75 7.2 Trong tập dữ liệu trên, sales là doanh số bán hàng (ngàn sản phẩm), các biến khác là chi tiêu marketing theo kênh (ngàn USD). Để xây dựng mô hình, ta cũng chia dữ liệu thành hai phần: train và test. set.seed(123) train &lt;- sample_frac(advertising,size = 0.6) test &lt;- advertising %&gt;% anti_join(train) Ta cũng có thể tạo train và test theo cách khác như sau # Cách khác train &lt;- sample &lt;- sample(c(TRUE, FALSE), nrow(advertising), replace = T, prob = c(0.6,0.4)) train &lt;- advertising[sample, ] test &lt;- advertising[!sample, ] 10.2.1 Mô hình hồi quy đơn biến Mô hình hồi quy đơn biến là mô hình chỉ có 1 biến X. Trong ví dụ này, ta sẽ xây dựng mô hình mối quan hệ giữa doanh số bán hàng và ngân sách được dành cho quảng cáo. \\[ Y = \\beta_0 + \\beta_1X + \\epsilon \\tag{1}\\] Trong đó: \\(Y\\) là biến sales \\(X\\) là biến TV advertising budget \\(\\beta_0\\) là hệ số tự do (intercept) \\(\\beta_1\\) là hệ số tương quan hay hệ số góc (độ dốc của đường hồi quy tuyến tính) \\(\\epsilon\\) là sai số của mô hình - sai số này thường được giả định là có giá trị trung bình bằng 0 và tuân theo phân phối chuẩn. Để xây dựng mô hình, ta thực hiện như sau. model1 &lt;- lm(sales ~ tv, data = train) model1 %&gt;% summary ## ## Call: ## lm(formula = sales ~ tv, data = train) ## ## Residuals: ## Min 1Q Median 3Q Max ## -8.0398 -2.0282 -0.1357 2.2867 7.4639 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 7.066290 0.626365 11.28 &lt;2e-16 *** ## tv 0.046164 0.003588 12.87 &lt;2e-16 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 3.331 on 118 degrees of freedom ## Multiple R-squared: 0.5838, Adjusted R-squared: 0.5803 ## F-statistic: 165.5 on 1 and 118 DF, p-value: &lt; 2.2e-16 train %&gt;% mutate(sale_fit = predict(model1, train)) %&gt;% sample_frac(0.4) %&gt;% ggplot(aes(tv, sales)) + geom_point(alpha = 0.5, size = 2) + geom_line(aes(x = tv, y = sale_fit), col = &quot;darkred&quot;, size = 1) + geom_segment(aes(x = tv, xend = tv, y = sales, yend = sale_fit)) + theme_bw() + labs(title = &quot;Sales vs. TV marketing budget&quot;) + theme(panel.border = element_blank(), axis.line = element_line(colour = &quot;black&quot;)) 10.2.2 Diễn giải mô hình result &lt;- model1 %&gt;% tidy result ## # A tibble: 2 x 5 ## term estimate std.error statistic p.value ## &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 (Intercept) 7.07 0.626 11.3 1.80e-20 ## 2 tv 0.0462 0.00359 12.9 3.28e-24 Hàm tidy trong broom cho phép làm sạch kết quả mô hình hồi quy và chuyển dữ liệu thành dạng bảng dataframe. Mối quan hệ giữa sale và marketing được thể hiện như sau. sales = 7.0662898 + 0.0461636 * tivi marketing budget Như vậy, nếu marketing tăng 1 đơn vị (1000 USD), doanh số sẽ tăng 46.1636238 sản phẩm Đánh giá mức độ ý nghĩa của hệ số góc \\[SE(\\beta_0)^2 = \\sigma^2\\bigg[\\frac{1}{n}+\\frac{\\bar{x}^2}{\\sum^n_{i=1}(x_i - \\bar{x})^2} \\bigg], \\quad SE(\\beta_1)^2 = \\frac{\\sigma^2}{\\sum^n_{i=1}(x_i - \\bar{x})^2} \\tag{3} \\] trong đó \\[\\sigma^2 = Var(\\epsilon)\\]. Việc tính toán \\(SE\\) cho phép ta tính được mức độ tinh cậy 95% của hệ số \\(\\beta\\) như sau. \\[ \\beta \\pm 2 \\cdot SE(\\beta) \\tag{4}\\] Trong R, để tìm ra mức độ tin cậy 95% của hệ số \\(\\beta\\), ta có thể làm như sau. confint(model1) ## 2.5 % 97.5 % ## (Intercept) 5.82591632 8.30666330 ## tv 0.03905843 0.05326882 Như vậy, ta thấy độ tin cậy 95% của hệ số \\(\\beta_1\\) khác 0. Ta có thể kiểm tra dựa trên p-value của mô hình đều nhỏ hơn 0.05 model1 %&gt;% tidy ## # A tibble: 2 x 5 ## term estimate std.error statistic p.value ## &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 (Intercept) 7.07 0.626 11.3 1.80e-20 ## 2 tv 0.0462 0.00359 12.9 3.28e-24 10.3 Đánh giá chất lượng mô hình Để đánh giá chất lượng mô hình, ta sẽ quan tâm đến 3 chỉ số sau: Sai số của mô hình R bình phương (\\(R^2\\)) F-statistic 10.3.1 Sai số của mô hình \\[ RSE = \\sqrt{\\frac{1}{n-2}\\sum^n_{i=1}(y_i - \\hat{y}_i)^2} \\tag{6}\\] sigma(model1) ## [1] 3.331281 Với sai số như trên, ta có thể nói rằng, doanh số thực tế trên thị trường sẽ có sai số so với dự báo 1 khoảng 3.3. Sai số tương đối của mô hình so với thực tế sẽ là sigma(model1)/mean(train$sales) ## [1] 0.2360657 Như vậy, sai số của mô hình so với thực tế sẽ là 23.6%. 10.3.2 R bình phương \\(R^2\\) Chỉ số RSE cho ta biết được sai số của mô hình khi dự báo. Bên cạnh đó, 1 chỉ số thường xuyên được sử dụng là chỉ số \\(R^2\\). Chỉ số này đo lường tỷ lệ biến động của biến Y được dự báo qua việc sử dụng mô hình (proportion of variance explained). Nếu không sử dụng mô hình, ta có thể dự báo giá trị của Y bằng giá trị trung bình. Tổng bình phương các sai số giữa giá trị thực tế và giá trị trung bình được gọi là độ biến động của dữ liệu (TSS - Total Sum of Square) Tổng bình phương các sai số giữa giá trị dự báo và giá trị thực tế được gọi là sai số của mô hình (RSS - Residual Sum of Square) Tổng bình phương các sai số giữa giá trị dự báo và giá trị trung bình thực tế được gọi là tổng biến động được giải thích qua mô hình (ESS - Explained Sum of Square) \\[ R^2 = 1 - \\frac{RSS}{TSS}= 1 - \\frac{\\sum^n_{i=1}(y_i-\\hat{y}_i)^2}{\\sum^n_{i=1}(y_i-\\bar{y}_i)^2} \\tag{7}\\] rsquare(model1, data = train) ## [1] 0.58383 Với kết quả trên, mô hình giải thích được 58.3829997% độ biến động của dữ liệu thực tế. 10.3.3 F statistic Chỉ số thống kê F được tính như sau. \\[F = \\frac{(TSS-RSS)/p}{RSS/(n-p-1)} \\tag{8} \\] Chỉ số này được sử dụng để kiểm định các hệ số trong mô hình có thực sự khác 0. \\(H_0\\): Tất cả các hệ số \\(\\beta\\) trong mô hình đều bằng không \\(H_1\\): Có ít nhất một hệ số \\(\\beta\\) trong mô hình khác không Trong mô hình trên, giá trị p-value của F-statistic rất thấp, như vậy, có ít nhất một hệ số trong mô hình khác không. 10.4 Dự báo Để dự báo kết quả mô hình, ta có thể dùng hàm predict trong R như sau. # Model result result &lt;- test %&gt;% select(tv, sales) %&gt;% mutate(predict = predict(model1, test)) %&gt;% mutate(error = predict - sales) sqrt(sum(result$error^2)/nrow(result)) ## [1] 3.165155 # Cách hai modelr::rmse(model1, test) ## [1] 3.165155 Vẽ kết quả mô hình với dự báo result %&gt;% select(-error) %&gt;% rename(predicted_sales = predict, real_sales = sales) %&gt;% gather(key = &quot;type&quot;, value = &quot;value&quot;, -tv) %&gt;% ggplot(aes(tv, value)) + geom_point(aes(col = type), alpha = 0.5) + geom_line(aes(col = type), alpha = 0.6) + theme_bw() + theme(legend.position = &quot;top&quot;) + labs(title = &quot;Sales vs. TV marketing budget&quot;, y = &quot;Sales&quot;, x = &quot;TV budget&quot;) + scale_color_manual(values = c(&quot;darkred&quot;, &quot;darkblue&quot;)) + theme(panel.border = element_blank(), axis.line = element_line(colour = &quot;black&quot;)) 10.5 Thuật toán tối ưu với gradient decent Ở phần trước, ta đã học cách áp dụng mô hình hồi quy đơn giản trong việc dự báo các biến liên tục. Ở phần này, ta sẽ tìm hiểu thêm về cách sử dụng học máy trong việc xây dựng mô hình hồi quy tuyến tính. Việc ưu hóa hàm chi phí với các tham số khác nhau là vấn đề cốt lõi của bất kỳ thuật toán machine learning nào. Tuy nhiên làm thế nào để tối ưu được, ta cần phải áp dụng các thuật toán tối ưu. Trong đó, quan trọng và phổ biến nhất là thuật toan gradient descent. Để tối ưu hóa cost function, ta có thể thay đổi các tổ hợp giá trị của \\(\\theta\\) cho đến khi tìm được tổ hợp mà tại đó, cost function đạt giá trị nhỏ nhất. Tuy nhiên, với trường hợp mô hình nhiều biến, ta sử dụng thuật toán gradient decient để tìm tập \\(\\theta\\) mà tại đó, cost function đạt giá trị nhỏ nhất. Quá trình sẽ diễn ra cho đến khi kết quả của cost function hội tụ tại 1 điểm với thuật toán sau. \\[\\theta_j := \\theta_j - \\alpha\\frac{d}{d\\theta_j}J(\\theta_0, \\theta_1)\\] Trong đó: \\(\\alpha\\) được gọi là learning rate, cho phép kiểm soát tốc độ update tham số trong mô hình. := là dấu gán \\(\\frac{d}{d\\theta_j}J(\\theta_0, \\theta_1)\\) là đạo hàm riêng phần theo \\(\\theta\\) của hàm \\(cost function\\) Áp dụng công thức tính đạo hàm hàm hợp, ta được \\[\\cases{\\theta_0 := \\theta_0 - \\alpha\\frac{1}{m}\\sum_{i = 1}^m(h_{\\theta}(x^{(i) - y^{(i)}}))\\\\ \\theta_1 := \\theta_1 - \\alpha\\frac{1}{m}\\sum_{i = 1}^m(h_{\\theta}(x^{(i) - y^{(i)}}))\\times x^{(i)}}\\] Thuật toán trên còn được gọi là batch gradient bởi lẽ thuật toán trên tính toán đến toàn bộ quan sát trên tập dữ liệu huấn luyện (train data). Bên cạnh batch gradient, còn có stochastic gradient descent và mini batch gradient descent. Công thức tổng quát để update tham số trong mô hình machine learning như sau. \\[\\Theta^1 = \\Theta^0 - \\alpha \\nabla J(\\Theta)\\] Trong đó: \\(\\Theta^1\\): Giá trị tham số mới \\(\\Theta^0\\): Giá trị tham số hiện tại \\(\\alpha\\): Learning rate, tốc độc update tham số \\(\\nabla J(\\Theta)\\): Đạo hàm riêng phần của từng tham số theo giá trị của biến đầu vào. \\(-\\): Hướng update tham số. Nếu giá trị đạo hàm riêng phần là dương, ta cần giảm giá trị của tham số, nếu giá trị đạo hàm riêng phần là âm, ta cần tăng giá trị của tham số. Giải thích gradient descent bằng ngôn ngữ đơn giản: Thuật toán gradient descent trông thì phức tạp, nhưng thực tế, cách triển khai và tối ưu thuật toán khá giống với phương pháp agile trong phát triển phần mềm. Thuật toán có thể được mô tả đơn giản lại như sau. Xây dựng mô hình một cách nhanh chóng (build). Tại bước này, ta chọn ngẫu nhiên giá trị của tham số để có thể xây dựng mô hình một cách đơn giản nhất. Đo lường độ lệch của mô hình so với thực tế (measure). Tại bước này, ta tính toán sai số giữa mô hình vừa xây dựng và dữ liệu thực tế để đo lường sai số của mô hình. Thay đổi/update mô hình dựa trên feedback (learn/action). Bước này sẽ phức tạp hơn một chút do ta phải trả lời hai câu hỏi: Tham số cần được thay đổi theo hướng nào? Tăng giá trị lên hay giảm giá trị xuống? Tốc độ thay đổi là bao nhiêu. Với câu hỏi thứ nhất, ta cần tính được giá trị đạo hàm riêng phần của từng tham số tại giá trị của X. Nếu giá trị này dương, ta cần giảm giá trị của tham số và ngược lại Với câu hỏi thứ hai, ta phải đặt trước một giá trị quy định tốc độ thay đổi của tham số. Giá trị này được gọi là learning rate. Mô hình hồi quy. library(dplyr) library(ggplot2) n &lt;- 200 # số lượng quan sát bias &lt;- 4 slope &lt;- 3.5 dot &lt;- `%*%` # Hàm tính phép nhân ma trận x &lt;- rnorm(n) * 2 x_b &lt;- cbind(x, rep(1, n)) # Thêm hệ số beta_0 cho ma trận x y &lt;- bias + slope * x + rnorm(n) df &lt;- data_frame(x = x, y = y) df %&gt;% ggplot(aes(x,y)) + geom_point(alpha = 0.5) + theme_minimal() learning_rate &lt;- 0.05 n_iterations &lt;- 100 theta &lt;- matrix(c(20, 20)) # Giá trị tham số đầu tiên b0 &lt;- vector(&quot;numeric&quot;, length = n_iterations) b1 &lt;- vector(&quot;numeric&quot;, length = n_iterations) sse_i &lt;- vector(&quot;numeric&quot;, length = n_iterations) for (iteration in seq_len(n_iterations)) { # Mô hình dự báo với theta yhat &lt;- dot(x_b, theta) # Tính sai só của mô hình residuals_b &lt;- yhat - y # Tính gradient (đạo hàm riêng phần cho giá trị theta) gradients &lt;- 2/n * dot(t(x_b), residuals_b) # Lưu ý theta &lt;- theta - learning_rate * gradients # update theta sse_i[[iteration]] &lt;- sum((y - dot(x_b, theta))**2) b0[[iteration]] &lt;- theta[2] b1[[iteration]] &lt;- theta[1] } model_i &lt;- data.frame(model_iter = 1:n_iterations, sse = sse_i, b0 = b0, b1 = b1) Lưu ý: Với mô hình hồi quy, hàm chi phí (cost function) là MSE. \\[J(\\beta_0, \\beta_1) = \\frac{1}{n}(\\beta_0 + \\beta_1*X - Y)^2\\] \\[\\frac{dJ}{d\\beta_0}=\\frac{2}{n}(\\beta_0 + \\beta_1*X - Y) = 2 * error\\] \\[\\frac{dJ}{d\\beta_1}=\\frac{2}{n}(\\beta_0 + \\beta_1*X - Y)*X= 2 * error * X\\] Do đó, ta có thể khái quát hóa gradient cho hai tham số như sau. \\[\\nabla J = 2*error*X&#39;\\] Trong đó, X’ là [1, X] p1 &lt;- df %&gt;% ggplot(aes(x=x, y=y)) + geom_abline(aes(intercept = b0, slope = b1, colour = -sse), data = model_i, alpha = .50 ) + geom_point(alpha = 0.4) + geom_abline(aes(intercept = b0, slope = b1), data = model_i[100, ], alpha = 0.5, size = 2, colour = &quot;dodger blue&quot;) + geom_abline(aes(intercept = b0, slope = b1), data = model_i[1, ], colour = &quot;red&quot;, alpha = 0.5, size = 2) + scale_color_continuous(low = &quot;red&quot;, high = &quot;grey&quot;) + guides(colour = FALSE) + theme_minimal() p1 p2 &lt;- model_i[1:30,] %&gt;% ggplot(aes(model_iter, sse, colour = -sse)) + geom_point(alpha = 0.4) + theme_minimal() + labs(x = &quot;Model iteration&quot;, y = &quot;Sum of Sqaured errors&quot;) + scale_color_continuous(low = &quot;red&quot;, high = &quot;dodger blue&quot;) + guides(colour = FALSE) p2 "],
["mo-hinh-hi-quy-logistic.html", "Chương 11 Mô hình hồi quy logistic 11.1 Cách xây dựng mô hình 11.2 Ví dụ xây dựng mô hình đơn giản 11.3 Các chỉ số cần quan tâm khác 11.4 Đánh giá chất lượng mô hình 11.5 Thực hành 11.6 Xây dựng mô hình Logistics theo Machine Learning", " Chương 11 Mô hình hồi quy logistic Khi phân tích dữ liệu, bên cạnh biến liên tục, ta thường xuyên phải làm việc với các biến rời rạc, các biến có đặc tinh nhóm (category) như “mua hàng vs. không mua hàng”, “giàu vs. nghèo”, “nợ xấu vs. không nợ xấu”. Quá trình phân tích và dự báo các quan sát thuộc về nhóm nào được gọi là quá trình phân loại (classification). Đối với bài toán phân loại, ta có thể chia thành hai nhóm lớn là phân loại hai biến và phân loại nhiều biến. Trong đó, bài toán phân loại hai biến là bài toán phổ biến nhất, được ứng dụng trong nhiều lĩnh vực trong đời sống như đánh giá điểm tín dụng của khách hàng, dự báo khách hàng phản ứng với chiến dịch marketing… Trong chương này, chúng ta sẽ làm quen vơi kỹ thuật nổi tiếng và được ứng dụng sớm nhất trong việc phân loại hai biến (binary classification) là logistic regression. Nếu như mô hình hồi quy tuyến tính thường (linear regression) cho phép chúng ta tìm được mối quan hệ giữa biến phụ thuộc liên tục (continous dependent variable) và một/nhiều biến độc lập (independent variable) - liên tục hoặc không liên tục thì mô hình hồi quy Logistic tìm mối quan hệ giữa biến phụ thuộc là biến nhị phân - binary variable. Biến cần dự báo dạng này chỉ nhận 2 giá trị: có/không, sống/chết, yêu/không yêu… và các biến độc lập có thể là liên tục hoặc không liên tục. Ví dụ, ta tìm hiểu mối quan hệ giữa việc KH có nợ xấu hay không dựa vào dư nợ của KH - thông qua việc tìm hiểu khả năng khách hàng có nợ xấu dựa vào thông tin dư nợ của KH. Trong đó: Biến phụ thuộc: \\[Y = \\begin{cases}1, &amp; KH\\:có\\:nợ\\:xấu\\\\ 0, &amp; KH\\:không\\:có\\:nợ\\:xấu\\end{cases}\\] Biến độc lập: X - dư nợ của KH (hay số tiền khách hàng nợ ngân hàng) Trong mô hình này, ta không thể sử dụng mô hình hồi quy thông thường bởi lẽ biến phụ thuộc chỉ có hai giá trị 0 và 1. Hơn nữa, cái ta cần dự báo là “xác suất” xảy ra sự kiện (xác suất vỡ nợ) cho mỗi quan sát. Để giải quyết vấn đề nay, ta có thể sử dụng mô hình logistic có dạng như sau: \\[p= Pr(Y=1)=\\frac{e^{B_{0} + B_{1}X}}{1+e^{B_{0} + B_{1}X}}\\] Trong đó, \\(p\\) là xác suất khách hàng có nợ xấu, \\((1-p)\\) là xác suất khách hàng không có nợ xấu. \\[Pr(Y=0)=1-p=\\frac{1}{1+e^{B_{0} + B_{1}X}}\\] Lưu ý: Trong các bài toán phân loại, nhóm ta cần dự báo còn được gọi là class positive hay event of interest. Khi ứng dụng với các bài toán cụ thể khác nhau, class sẽ được xây dựng khác nhau. Ví dụ: trong dự báo khách hàng phản hồi, thì sự kiện khách hàng phản hồi email là 1, không phản hồi email là 0. Khi dự báo khách hàng có nợ xấu, khách hàng có nợ xấu được đánh dấu là 1, không có nợ xấu được đánh dấu là 0. Trong mô hình logistic, ta còn có một chỉ số khác được gọi là \\(Odds\\: ratio\\), tạm dịch là khả năng xảy ra sự kiện so với khả năng không xảy ra sự kiện. Với bài toán dự báo nợ xấu, chỉ số này cho chúng ta biết xác suất khách hàng có nợ xấu cao hơn bao nhiêu lần bao nhiêu lần so với xác suất khách hàng không có nợ xấu. \\[Odds=\\frac{p}{1-p}=e^{B_{0} + B_{1}X}\\] Ví dụ: Xác suất khách hàng có nợ xấu: p = 0.8 Xác suất khách hàng không có nợ xấu: 1-p = 1-0.8=0.2 Odd ration: \\(p/(1-p) = 4\\) - khả năng khách hàng có nợ xấu cao gấp 4 lần khả năng khách hàng không có nợ xấu. Logit: Là hàm log của odd ratio, được tính như sau. \\[logit=ln(Odds)=ln(\\frac{p}{1-p}) = B_{0} + B_{1}X\\] Phương trình trên gọi là phương trình hồi quy Logistic. Hàm này có thể chứa các giá trị bất kỳ trong khoảng từ \\((-\\infty, +\\infty)\\). Lúc này, hàm trên đã trở về hàm hồi quy thông thường. Như vậy, mô hình chúng ta cần xây dựng có dạng: \\[logit(p) = \\alpha + \\beta X\\] hay: \\[odds(p) = \\frac{p}{1-p} = e^{\\alpha + \\beta X}\\] Giải thích ý nghĩa: Khi x tăng thêm 1 đơn vị, thì Odds ratio (tỷ số khả dĩ) sẽ tăng thêm \\(e^{\\beta}\\). Nghĩa là, khi x tăng thêm 1 đơn vị, khả năng (chance) xảy ra biến cố X sẽ tăng lên \\(e^{\\beta}\\) lần. Khi \\(x = x_0\\), khả năng xảy ra biến cố là: \\[odds(p|x=x_0) = e^{\\alpha + \\beta x_0}\\] Khi \\(x = x_0 + 1\\), khả năng xảy ra biến cố là: \\[odds(p|x=x_0+1) = e^{\\alpha + \\beta (x_0+1)}\\] Tỷ số giữa 2 khả năng là: \\[\\frac{e^{\\alpha + \\beta (x_0+1)}}{e^{\\alpha + \\beta (x_0)}} = e^{\\beta}\\] Lưu ý: Cách giải thích ý nghĩa của hệ số \\(\\beta\\) trong Logistic rất dễ nhầm lẫn. Cơ bản có 2 cách như sau: Cách 1: Giải thích dựa trên “xác suất” xảy ra biến cố A. Cách tính này dựa trên việc đưa ra tính toán xác suất xảy ra A với X cho trước. Cách 2: Dựa trên “khả năng” xảy ra biến cố A (Odd ratio) ## Loading tidyverse: ggplot2 ## Loading tidyverse: tibble ## Loading tidyverse: tidyr ## Loading tidyverse: readr ## Loading tidyverse: purrr ## Loading tidyverse: dplyr ## Warning: package &#39;ggplot2&#39; was built under R version 3.4.4 ## Warning: package &#39;tibble&#39; was built under R version 3.4.4 ## Warning: package &#39;tidyr&#39; was built under R version 3.4.4 ## Warning: package &#39;readr&#39; was built under R version 3.4.4 ## Warning: package &#39;purrr&#39; was built under R version 3.4.4 ## Warning: package &#39;dplyr&#39; was built under R version 3.4.4 ## Conflicts with tidy packages ---------------------------------------------- ## filter(): dplyr, stats ## lag(): dplyr, stats ## Warning: The `printer` argument is deprecated as of rlang 0.3.0. ## This warning is displayed once per session. ## Warning: package &#39;bindrcpp&#39; was built under R version 3.4.4 11.1 Cách xây dựng mô hình Để ước lượng các tham số \\(\\beta_j\\), ta dùng phương pháp hợp lý tối đa (maximum likelihood), nghĩa là tìm \\(\\beta_j\\) để tối đa hóa hàm likelihood. \\[L = p_1*p_2*...*p_n= \\prod_{i=1}^np_i\\] Trong đó, \\(p_i\\) là xác suất xảy sự kiện của quan sát thứ \\(i\\). Trong bài toán hồi quy logistic, ta quan tâm đến việc dự báo chính xác sự kiện xảy ra (\\(P_{y = 1}\\)). Do đó, hàm likelihood trong bài toán nhị phân được tính như sau. \\[L = \\prod[-y_i*log(h_\\theta(x_i)) - (1-y_i)log(1-h_\\theta(x_i))]\\] Trong đó: \\(y_i\\): Là giá trị thực tế của biến cần dự báo (1 hoặc 0) tại quan sát i \\(h_{\\theta}(x_i)\\): Giá trị dự báo của quan sát i. Giá trị này nằm trong khoảng [0, 1] Xem ví dụ dưới đây: Thực tế Dự báo L 1 0.7 0.7 0 0.6 0.4 1 0.8 0.8 Với mô hình trên, ta có: \\[L = 0.7 * 0.4 * 0.8 = 0.224\\] \\[log(L) = log(0.224) = -1.496\\] Như vậy, về mặt hình ảnh, hàm Likelihood sẽ tìm cách fit dữ liệu sao cho sát với dữ liệu thực tế nhiều nhất. Hàm này sẽ đạt giá trị tối đa bằng 1 (L = 1), ứng với \\(log(L_{max}) = 0\\) Để hiểu hơn về cách tối ưu hóa Logistic trong trường hợp đơn giản, xem ví dụ trong file excel đính kèm 11.2 Ví dụ xây dựng mô hình đơn giản library(ISLR) library(tidyverse) data &lt;- Default %&gt;% select(-student) %&gt;% mutate(default = case_when( default == &quot;Yes&quot; ~ 1, TRUE ~ 0 )) library(broom) ## Warning: package &#39;broom&#39; was built under R version 3.4.4 model &lt;- glm(default ~ income + balance, data = data, family = &quot;binomial&quot;) model %&gt;% summary ## ## Call: ## glm(formula = default ~ income + balance, family = &quot;binomial&quot;, ## data = data) ## ## Deviance Residuals: ## Min 1Q Median 3Q Max ## -2.4725 -0.1444 -0.0574 -0.0211 3.7245 ## ## Coefficients: ## Estimate Std. Error z value Pr(&gt;|z|) ## (Intercept) -1.154e+01 4.348e-01 -26.545 &lt; 2e-16 *** ## income 2.081e-05 4.985e-06 4.174 2.99e-05 *** ## balance 5.647e-03 2.274e-04 24.836 &lt; 2e-16 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## (Dispersion parameter for binomial family taken to be 1) ## ## Null deviance: 2920.6 on 9999 degrees of freedom ## Residual deviance: 1579.0 on 9997 degrees of freedom ## AIC: 1585 ## ## Number of Fisher Scoring iterations: 8 library(broom) model %&gt;% tidy %&gt;% filter(term != &quot;(Intercept)&quot;) %&gt;% select(term, estimate) %&gt;% mutate(`odd_increase (thousands USD)` = exp(1000*estimate)) ## # A tibble: 2 x 3 ## term estimate `odd_increase (thousands USD)` ## &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 income 0.0000208 1.02 ## 2 balance 0.00565 283. Ý nghĩa: Nếu balance tăng 1000 USD, khả năng khách hàng có nợ xấu tăng 283 lần Nếu income tăng 1000 USD, khả năng khách hàng có nợ xấu tăng 1.02 lần Khi ước lượng được các tham số \\(\\beta_j\\), với mỗi KH cụ thể ta có thể ước lượng được XS KH có nợ xấu bằng bao nhiêu dựa vào dư nợ của KH đó. Từ đó, ta có thể phân loại được KH Good/Bad (ko có nợ xấu/có nợ xấu) bằng việc so sánh XS có nợ xấu của KH với giá trị ngưỡng (“cutoff point”). Ví dụ, XS có nợ xấu của KH A là 0.2, điểm cutoff ta lựa chọn để phân loại là 0.5 -&gt; KH A - “Good”. Dự báo trên tập mới. Để dự báo trên tập dữ liệu mới, ta có thể dùng hàm predict với option type = \"response\" để trả ra kết quả dưới dạng xác suất xảy ra event (trong ví dụ này là xác suất xảy ra nợ xâu) predict(model, data %&gt;% head, type = &quot;response&quot;) ## 1 2 3 4 5 ## 0.0015047280 0.0012619299 0.0080262106 0.0004059957 0.0018267237 ## 6 ## 0.0020424398 11.3 Các chỉ số cần quan tâm khác Null deviance vs. Residual Deviance LL: Log Likelihood \\[null\\;deviance = 2(LL(saturated\\;model) - LL(null\\;model))\\] \\[residual\\;deviance = 2(LL(saturated\\;model) - LL(proposed\\;model))\\] Trong đó: Saturated Model: Mỗi quan sát có 1 tham số riêng cần phải ước lượng (n tham số) Null Model: Tất cả quan sát chỉ có 1 tham số Proposed Model: Mô hình có p tham số ứng với p biến và 1 tham số ứng với intercept (p+1 tham số) Nếu Null deviance thấp, nghĩa là mô hình có thể giải thích bằng mô hình đơn giản 11.4 Đánh giá chất lượng mô hình Khi xây dựng mô hình dự báo, kết quả dự báo sẽ được trả ra dưới dạng 1 dãy các điểm xác suất xảy ra sự kiện. Khi đó, ta sẽ phải lựa chọn điểm mà tại đó, nếu xác suất dự báo cao hơn điểm này, các quan sát sẽ được đánh dấu là nợ xấu. Xem ví dụ sau. Customer ID Xác suất nợ xấu 1 0.7 2 0.5 3 0.4 4 0.3 Nếu cutoff là 0.6: Khách hàng 1 được dự báo sẽ có nợ xấu Nếu cutoff là 0.45: Khách hàng 1, 2 được dự báo sẽ có nợ xấu Như vậy, với mỗi điểm cutoff khác nhau, ta sẽ có số lượng khách hàng bị nợ xấu khác nhau Pseudo \\(R^2\\) Không giống như mô hình OLS, trong mô hình OLS không có \\(R^2\\) để giải thích độ biến động của dữ liệu khi sử dụng mô hình. Tuy nhiên, mô hình logistic có chỉ số pseudo \\(R^2\\) được McFadden’s đưa ra. Chỉ số này có thể được dùng tương tự như \\(R^2\\) trong OLS. \\[R^2 = 1−\\frac{ln(LM_1)}{ln(LM_0)}\\] Trong đó: \\(ln(LM_1)\\) là log likelihood của mô hình \\(ln(LM_0)\\) là log likelihood của mull model, nghĩa là mô hình chỉ có hệ số tự do intercept. Trong mô hình logistic, nếu \\(R^2\\) có giá trị trên 0.4 đã được coi là mô hình tương đối tốt. 11.4.1 Confusion Matrix Confusion Matrix: Bảng mô tả kết quả dự báo vs. thực tế ứng với 1 điểm cut-off xác định trước. Tính toán các chỉ số: Accuracy (acc): Tỷ lệ dự báo chính xác trên cả hai class. Acc \\[Accuracy = \\frac{TP+TN}{TP+TN+FN+FP}\\]. True Negative Rate (tnr): Tỷ lệ dự đoán Negative chính xác trong toàn bộ quan sát thực tế là Negative \\[tnr = Specificity = \\frac{TN}{TN + FP}\\] False Positive Rate (fpr - fall out): Tỷ lệ dự đoán Positive sai, hay tỷ lệ các quan sát thực tế là Negative nhưng lại được dự báo thành Positive \\[fpr = 1- Specificity = \\frac{FP}{TN + FP}\\] True Positive Rate (tpr - Sensitivity - hay còn gọi là recall): Tỷ lệ dự báo Positive chính xác trên toàn bộ số lượng quan sát thực tế là Positive \\[tpr = Sensitivity = \\frac{TP}{TP + FN}\\] False Negative Rate (fnr - Sensitivity): Tỷ lệ dự báo negative sai - tỷ lệ positive nhưng dự báo nhầm thành Negative \\[fnr = 1- Sensitivity = \\frac{FN}{FN + TP}\\] Support: Tỷ lệ dự báo Positive trong toàn mẫu \\[sup = \\frac{TP + FP}{N} = \\frac{predicted pos}{total}\\] Precision: Trong tổng số quan sát dự báo là Positive, bao nhiêu quan sát thực sự là Positive \\[Precision = \\frac{TP}{TP+FP}\\] Rate of positive predictions (RPP): tỷ lệ tổng số quan sát được dự báo là Y=1 (có nợ xấu) trên tổng số quan sát. Trong các chỉ số trên, các chỉ số True Positive Rate, Precision và Accuracy là quan trọng nhất. Để hiểu hơn về các chỉ số trên, giả sử ta có bảng confusion matrix như sau: Predicted/Actual 1 0 1 0.4 0.1 0 0.2 0.3 Các chỉ số trong bảng trên được tính như sau: \\(accurracy = 0.4 + 0.3 = 0.7\\): Tỷ lệ dự báo chính xác trên cả hai class là 70%. Nói cách khác, trong 100 quan sát, mô hình dự báo chính xác 70% \\(True \\; Positive \\; Rate = TPR = \\frac{0.4}{0.4 + 0.2} = 0.67\\) Trong 100 khách hàng thực sự bị nợ xấu, mô hình dự báo chính xác được 67 khách hàng. \\(Precision = \\frac{0.4}{0.4 + 0.1} = 0.8\\) Trong 100 khách hàng được dự báo là nợ xấu, có 80 khách hàng thực sự bị nợ xấu \\(True \\;Negative \\;Rate = Specificity = \\frac{0.3}{0.3+0.1}=0.75\\) Trong 100 khách hàng tốt, mô hình dự đoán chính xác 75 khách hàng Với phương pháp sử dụng Confusion Matrix, kết quả dự báo của mô hình trở nên đơn giản, dễ hiểu, dễ giải thích. Tuy nhiên, để có được confusion matrix, ta phải xác định trước giá trị của điểm cut-off. Điều này không hề dễ dàng bởi lẽ mỗi điểm cut-off sẽ cho ta 1 giá trị của confusion matrix hoàn toàn khác nhau. Để khắc phục điểm yếu này, một chỉ số khác thường được sử dụng là đường ROC với chỉ số AUC. 11.4.2 ROC ROC Curve: Receiver Operating Characteristic. ROC cho ta đánh giá hiệu quả của mô hình thông qua sự đánh đổi giữa TPR và FPR. Ý nghĩa: Mô tả tỷ lệ đánh đổi giữa 2 tỷ lệ: i, Tỷ lệ dự báo chính xác sự kiện A và ii, Tỷ lệ dự báo rằng sự kiện đó không diễn ra Dự báo mang tính ngẫu nhiên (50 - 50), ROC là đường 45 độ Dự báo hoàn toàn chính xác thì ROC là đường có dạng tam giác Dự báo kém hơn kỳ vọng thì ROC ở dưới đường 45 độ AUC (Area Under Curve): Cho phép so sánh độ chính xác của mô hình. AUC càng cao (ROC ở vị trí càng cao) thì mô hình càng chính xác 11.4.2.1 Lý thuyết Để xây dựng đường ROC trong, ta có các ký hiệu sau. N: số lượng quan sát NEG: số lượng NEGATIVE, neg: tỷ lệ negative (\\(neg = \\frac{NEG}{N}\\)) POS: số lượng POSITIVE, pos: tỷ lệ positive (\\(pos = \\frac{POS}{N}\\)) Quan hệ giữa acc và fpr, tpr: \\[acc = \\frac{TP}{N} \\frac{TN}{N}= \\frac{TP}{POS}\\frac{POS}{N} + \\frac{NEG-FP}{N} = \\frac{TP}{POS}\\frac{POS}{N} + \\frac{NEG}{N} - \\frac{FP}{NEG}\\frac{NEG}{N}\\] \\[acc = tpr * pos + neg - neg * fpr\\] hay: \\[tpr = \\frac{acc-neg}{pos} + \\frac{neg}{pos}fpr\\] công thức trên giúp ta vẽ được đồ thị đường acc theo tpr và fpr Đồ thị acc với 1 số trường hợp: Lưu ý: Mỗi điểm trên ROC thể hiện kết quả của mô hình ứng vơi MỘT điểm cut-off. Do đó khi vẽ đường ROC, máy tính sẽ thay đổi các điểm cutoff từ 0-1, sau đó tính tpr &amp; fpr với từng điểm cutoff. Tập hợp tất cả các điểm này tạo thành ROC Để tìm điểm cutoff tốt nhất, nghia là với mô hình đã đưa ra, chọn cutoff \\(\\alpha\\) sao cho \\(acc\\) đạt max. Thuật toán: Vẽ đường acc theo tpr và fpr Tịnh tiến đường acc cho đến khi acc tiếp tuyến với ROC Tiếp điểm chính là điểm \\(\\alpha\\) cần tìm 11.4.2.2 Cách vẽ ROC thực tế Trên thực tế, ROC được vẽ với thuật toán như sau: Gọi F là hàm tính xác suất của Y trong GLM \\[ F(X) ~ \\frac{P(X)}{1-P(X)} = e^{\\beta X}\\] Tính F(X) với tập dữ liệu phân tích, sắp xếp F(X) theo giá trị từ cao xuống thấp. Tập hợp này tạo thành tập hợp A của biến score = x Bắt đầu từ điểm (0,0) Với mỗi giá trị của \\(x \\in A\\), thực hiện như sau: Nếu x positive, di chuyển lên trên 1/pos Nếu x negative, di chuyển sang phải 1/neg Đường ROC nằm càng hướng về phía đỉnh góc bên trái thì độ chính xác của dự báo càng cao, điều đó thể hiện true positive rate cao và false positive rate thấp; còn nếu càng tiến tới đường chéo 45 độ so với trục hoành thì độ chính xác của dự báo càng kém. Đường chéo 45 độ so với trục hoành thể hiện sự phân loại là không có ý nghĩa. AUC (Area under ROC curve) - diện tích phía dưới đường cong ROC, là thước đo độ chính xác của sự phân loại. AUC càng lớn thì độ chính xác càng cao. AUC từ 0.5-0.6: Mô hình chỉ tương đương với việc chọn ngẫu nhiên 0.6-0.7: Chất lượng mô hình kém 0.7-0.8: Tương đối tốt 0.8-0.9: Tốt trên 0.9: Rất tốt Nói cách khác, chỉ số AUC cho phép ta biết được chất lượng phân loại của mô hình. AUC càng cao, cho ta thấy các quan sát xảy ra positive class (nợ xấu), càng có điểm score cao (tương đối) và ngược lại. 11.4.3 Gain và Lift Gain và Lift là hai biểu đồ vô cùng quan trọng với các nhà phân tích dữ liệu thực tiễn nhưng lại ít được các nhà lý thuyết sử dụng. Hai đường này đặc biệt quan trọng khi xây dựng các chiến lược marketing. Đường gain là tổ hợp giữa TPR (True Positive Rate) và RPP (Rate of Positive Prediction). Vị trí các điểm trên đường gain cho ta biết nên chọn điểm cut-off như thế nào. library(scales) ## Warning: package &#39;scales&#39; was built under R version 3.4.4 ## ## Attaching package: &#39;scales&#39; ## The following object is masked from &#39;package:purrr&#39;: ## ## discard ## The following object is masked from &#39;package:readr&#39;: ## ## col_factor df &lt;- data.frame(tpr = c(0, 0.4, 0.7, 0.8, 0.85, 0.88, 0.93, 0.94, 0.97, 0.99, 1), rpp = c(0,seq(0.1,1, by = 0.1))) %&gt;% mutate(lift = tpr/rpp) %&gt;% mutate(lift = replace_na(lift, 0)) df %&gt;% ggplot(aes(rpp, tpr)) + geom_line(col = &quot;darkred&quot;, alpha = 0.7) + geom_point(col = &quot;darkred&quot;, size = 2) + geom_text(aes(label = tpr), vjust = -0.5) + theme_classic() + scale_x_continuous(breaks = seq(0.1, 1, by = 0.1)) + geom_line(aes(rpp, rpp), col = &quot;black&quot;, alpha = 1, linetype = &quot;dashed&quot;) + annotate(geom = &quot;text&quot;, x = 0.5, y = 0.45, label = &quot;Base line (No model)&quot;, hjust = 0) + geom_vline(xintercept = seq(0.1, 1, by = 0.1), linetype = &quot;dashed&quot;, col = &quot;darkgrey&quot;, alpha = 0.3) + labs(x = &quot;Rate of Positive Prediction&quot;, y = &quot;True Positive Rate&quot;, title = &quot;Gain chart&quot;) Giải thích: Với top 20% khách hàng có điểm score cao nhất sẽ phủ được 70% khách hàng có nợ xấu trong thực tế. Với top 30% khách hàng có điểm score cao nhất sẽ phủ được 80% khách hàng có nợ xấu trong thực tế. Thông thường, đường gain được chia thành các decile (10%) để marketing/risk manager quyết định điểm cut-off trong mô hình. Nếu số lượng khách hàng ít, 1000 khách hàng, đôi khi ta có thể quyết định gửi email marketing cho cả 1000 khách hàng này. Trong khi đó, đường lift thể hiện khía cạnh khác trong việc xây dựng mô hình. Đường lift cho ta biết, ứng với từng decile, chất lượng mô hình cao hơn bao nhiêu lần so với việc không sử dụng mô hình (ngẫu nhiên). Đường lift luôn giảm về 1 khi ta dự báo 100% tất cả quan sát thuộc về nhóm positive. df %&gt;% filter(lift &gt; 0) %&gt;% ggplot(aes(rpp, lift)) + geom_line() + geom_point() + theme_classic() + geom_line(col = &quot;darkred&quot;, alpha = 0.7) + geom_point(col = &quot;darkred&quot;, size = 2) + geom_text(aes(label = round(lift, 1)), vjust = -0.5, hjust = 0) + theme_classic() + scale_x_continuous(breaks = seq(0.1, 1, by = 0.1)) + annotate(geom = &quot;text&quot;, x = 0.1, y = 1.1, label = &quot;Base line (No model)&quot;, hjust = 0) + geom_vline(xintercept = seq(0.1, 1, by = 0.1), linetype = &quot;dashed&quot;, col = &quot;darkgrey&quot;, alpha = 0.3) + geom_hline(yintercept = 1, linetype = &quot;dashed&quot;, col = &quot;darkblue&quot;, alpha = 1, size = 0.7) + labs(x = &quot;Rate of Positive Prediction&quot;, y = &quot;Lift ratio&quot;, title = &quot;Lift chart&quot;) Ý nghĩa: Trong đồ thị trên, với top 10% khách hàng có điểm score mua hàng cao nhất, tỷ lệ khách hàng mua hàng cao hơn 4 lần so với việc không sử dụng mô hình (hay ngẫu nhiên) 11.4.4 Kolmogorov–Smirnov statistics Chỉ số thống kê Kolmogorov-Smirnov là chỉ số khác để đo lường chất lượng mô hình. Chỉ số này cho ta biết được liệu phân phối của của nhóm positive có thực sự khác nhóm negative hay không. Chỉ số KS được đo lường bằng khoảng cách lớn nhất giữa hai đường phân phối lũy kế xác suất trên hai class. Khi xây dựng mô hình, KS có thể được tính theo công thức sau. \\[KS = max(abs(tpr - fpr))\\] pos_class &lt;- c(0.8, 0.75, 0.7, 0.55, 0.4) pos_class ## [1] 0.80 0.75 0.70 0.55 0.40 neg_class &lt;- c(0.7, 0.4, 0.3, 0.2, 0.15) neg_class ## [1] 0.70 0.40 0.30 0.20 0.15 ks.test(pos_class, neg_class, alternative = &quot;greater&quot;) ## Warning in ks.test(pos_class, neg_class, alternative = &quot;greater&quot;): cannot ## compute exact p-value with ties ## ## Two-sample Kolmogorov-Smirnov test ## ## data: pos_class and neg_class ## D^+ = 0, p-value = 1 ## alternative hypothesis: the CDF of x lies above that of y data.frame(pos_class, neg_class) %&gt;% ggplot() + stat_ecdf(aes(pos_class), col = &quot;darkred&quot;, geom = &quot;line&quot;) + stat_ecdf(aes(neg_class), col = &quot;darkblue&quot;, geom = &quot;line&quot;) + annotate(geom = &quot;text&quot;, x = 0.5, y = 0.5, label = &quot;KS = 0.6&quot;) + theme_minimal() ## Warning: package &#39;StatMeasures&#39; was built under R version 3.4.4 result$ksTable ## dec total responders nonresponders cumResponders cumNonresponders ## 1 1 1 1 0 1 0 ## 2 2 1 1 0 2 0 ## 3 4 2 1 1 3 1 ## 4 5 1 1 0 4 1 ## 5 7 2 1 1 5 2 ## 6 8 1 0 1 5 3 ## 7 9 1 0 1 5 4 ## 8 10 1 0 1 5 5 ## perCumResponders perCumNonresponders split ## 1 20 0 20 ## 2 40 0 40 ## 3 60 20 40 ## 4 80 20 60 ## 5 100 40 60 ## 6 100 60 40 ## 7 100 80 20 ## 8 100 100 0 result$ks %&gt;% print ## [1] 60 Trong bốn nhóm chỉ số đánh giá chất lượng mô hình đã trình bày ở trên, nhóm chỉ số AUC, Gain và Lift là quan trọng nhất khi dự báo trong thực tế, đặc biệt là khi score khách hàng trong chiến dịch marketing, bán chéo, giảm thiểu khách hàng churn. Khi xây dựng các mô hình dự báo nợ xấu, phê duyệt tín dụng, ta cần sử dụng cả 4 nhóm chỉ số trên để đánh giá hiệu quả mô hình. Đặc biệt, nếu xây dựng mô hình score-card, ta còn phải sử dụng 1 số kỹ thuật khác để có thể xây dựng hệ thống score-card hiệu quả. Chi tiết phần score card sẽ được trình bày trong chương tiếp theo. 11.5 Thực hành 11.5.1 Dữ liệu Dùng gói dữ liệu Default có sẵn trong R, bao gồm thông tin của 10,000 khách hàng sử dụng thẻ tín dụng. Giải thích các biến: Default: Yes/No (khách hàng có nợ xấu/không có nợ xấu) Student: Yes/No (khách hàng là sinh viên/không phải là sinh viên) Balance: Dư nợ TB còn lại của KH sau khi trả tiền hàng tháng. Income: Thu nhập của khách hàng data &lt;- Default data %&gt;% head ## default student balance income ## 1 No No 729.5265 44361.625 ## 2 No Yes 817.1804 12106.135 ## 3 No No 1073.5492 31767.139 ## 4 No No 529.2506 35704.494 ## 5 No No 785.6559 38463.496 ## 6 No Yes 919.5885 7491.559 11.5.2 Xây dựng mô hình trên tập dữ liệu gốc glm.fit1 &lt;- glm(default ~ ., data = data, family = binomial) glm.fit1 %&gt;% summary ## ## Call: ## glm(formula = default ~ ., family = binomial, data = data) ## ## Deviance Residuals: ## Min 1Q Median 3Q Max ## -2.4691 -0.1418 -0.0557 -0.0203 3.7383 ## ## Coefficients: ## Estimate Std. Error z value Pr(&gt;|z|) ## (Intercept) -1.087e+01 4.923e-01 -22.080 &lt; 2e-16 *** ## studentYes -6.468e-01 2.363e-01 -2.738 0.00619 ** ## balance 5.737e-03 2.319e-04 24.738 &lt; 2e-16 *** ## income 3.033e-06 8.203e-06 0.370 0.71152 ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## (Dispersion parameter for binomial family taken to be 1) ## ## Null deviance: 2920.6 on 9999 degrees of freedom ## Residual deviance: 1571.5 on 9996 degrees of freedom ## AIC: 1579.5 ## ## Number of Fisher Scoring iterations: 8 glm.fit2 &lt;- step(glm.fit1) ## Start: AIC=1579.54 ## default ~ student + balance + income ## ## Df Deviance AIC ## - income 1 1571.7 1577.7 ## &lt;none&gt; 1571.5 1579.5 ## - student 1 1579.0 1585.0 ## - balance 1 2907.5 2913.5 ## ## Step: AIC=1577.68 ## default ~ student + balance ## ## Df Deviance AIC ## &lt;none&gt; 1571.7 1577.7 ## - student 1 1596.5 1600.5 ## - balance 1 2908.7 2912.7 glm.fit2 %&gt;% summary ## ## Call: ## glm(formula = default ~ student + balance, family = binomial, ## data = data) ## ## Deviance Residuals: ## Min 1Q Median 3Q Max ## -2.4578 -0.1422 -0.0559 -0.0203 3.7435 ## ## Coefficients: ## Estimate Std. Error z value Pr(&gt;|z|) ## (Intercept) -1.075e+01 3.692e-01 -29.116 &lt; 2e-16 *** ## studentYes -7.149e-01 1.475e-01 -4.846 1.26e-06 *** ## balance 5.738e-03 2.318e-04 24.750 &lt; 2e-16 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## (Dispersion parameter for binomial family taken to be 1) ## ## Null deviance: 2920.6 on 9999 degrees of freedom ## Residual deviance: 1571.7 on 9997 degrees of freedom ## AIC: 1577.7 ## ## Number of Fisher Scoring iterations: 8 Giải thích: Beta(balance) &gt; 0: Balance và XS có nợ xấu của KH có quan hệ cùng chiều (cái này tăng thì cái kia tăng). Beta(balance) = 0.0057: Balance tăng 1 đơn vị -&gt; Ln(Odds) tăng 0.0057 đơn vị. Beta(studentYes) &lt; 0: KH là sinh viên có XS có nợ xấu thấp hơn so với KH ko là sinh viên. Giả sử KH B có balance = 1000$: \\[Pr(default=Yes|student=Yes)=\\frac{e^{-10.75-0.7149*1+0.0057*1000}}{1+e^{-10.75-0.7149*1+0.0057*1000}}= 0.003\\] \\[Pr(default=Yes|student=No)=\\frac{e^{-10.75-0.7149*0+0.0057*1000}}{1+e^{-10.75-0.7149*0+0.0057*1000}}= 0.006\\] 11.5.3 Xây dựng mô hình trên hai tập train và test 11.5.3.1 Chia tập train/test #chia tập train/test set.seed(1) train &lt;- sample(nrow(data), 0.7 * nrow(data)) df.train &lt;- data[train,] # tập train df.test &lt;- data[-train,] # tập test df.train %&gt;% summary ## default student balance income ## No :6766 No :4952 Min. : 0.0 Min. : 2541 ## Yes: 234 Yes:2048 1st Qu.: 474.6 1st Qu.:21436 ## Median : 819.1 Median :34646 ## Mean : 831.7 Mean :33609 ## 3rd Qu.:1157.8 3rd Qu.:43826 ## Max. :2654.3 Max. :73554 df.test %&gt;% summary ## default student balance income ## No :2901 No :2104 Min. : 0.0 Min. : 772 ## Yes: 99 Yes: 896 1st Qu.: 495.1 1st Qu.:20962 ## Median : 831.3 Median :34396 ## Mean : 844.0 Mean :33303 ## 3rd Qu.:1177.5 3rd Qu.:43751 ## Max. :2502.7 Max. :70022 11.5.3.2 Xây dựng mô hình fit.logit &lt;- glm(default~., data = df.train, family = binomial) #giảm biến ko có ý nghĩa thống kê logit.fit.reduced &lt;- step(fit.logit) ## Start: AIC=1078.68 ## default ~ student + balance + income ## ## Df Deviance AIC ## - income 1 1070.7 1076.7 ## &lt;none&gt; 1070.7 1078.7 ## - student 1 1076.2 1082.2 ## - balance 1 2041.2 2047.2 ## ## Step: AIC=1076.74 ## default ~ student + balance ## ## Df Deviance AIC ## &lt;none&gt; 1070.7 1076.7 ## - student 1 1088.7 1092.7 ## - balance 1 2042.7 2046.7 logit.fit.reduced %&gt;% summary ## ## Call: ## glm(formula = default ~ student + balance, family = binomial, ## data = df.train) ## ## Deviance Residuals: ## Min 1Q Median 3Q Max ## -2.5097 -0.1357 -0.0522 -0.0184 3.7777 ## ## Coefficients: ## Estimate Std. Error z value Pr(&gt;|z|) ## (Intercept) -1.098e+01 4.539e-01 -24.185 &lt; 2e-16 *** ## studentYes -7.372e-01 1.787e-01 -4.125 3.71e-05 *** ## balance 5.890e-03 2.843e-04 20.716 &lt; 2e-16 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## (Dispersion parameter for binomial family taken to be 1) ## ## Null deviance: 2050.5 on 6999 degrees of freedom ## Residual deviance: 1070.7 on 6997 degrees of freedom ## AIC: 1076.7 ## ## Number of Fisher Scoring iterations: 8 11.5.3.3 Dự báo xác suất có nợ xấu trên tập test prob.test &lt;- predict(logit.fit.reduced, df.test, type = &quot;response&quot;) #phân loại (chọn cutoff = 0.5) class.pred.test &lt;- factor(prob.test &gt; 0.5, levels = c(FALSE, TRUE), labels = c(&quot;No&quot;, &quot;Yes&quot;)) #kết quả trên tập test prob.test.df &lt;- prob.test %&gt;% as.data.frame() names(prob.test.df)[1] &lt;- &quot;prob.default&quot; #Lụa chọn điểm cutoff là 0.5 prob.test.df &lt;- prob.test.df %&gt;% mutate(class = as.factor(ifelse(prob.default &gt; 0.5, &quot;Yes&quot;, &quot;No&quot;))) prob.test.df$student &lt;- df.test$student prob.test.df$balance &lt;- df.test$balance prob.test.df$income &lt;- df.test$income prob.test.df %&gt;% select(3:5,1,2) %&gt;% sample_n(10) ## student balance income prob.default class ## 2097 Yes 703.06307 17692.66 5.136338e-04 No ## 1364 Yes 1026.15356 25393.77 3.434202e-03 No ## 1941 No 7.06953 48711.64 1.781314e-05 No ## 1840 No 0.00000 49250.46 1.708667e-05 No ## 799 No 421.89690 37323.76 2.050024e-04 No ## 2778 Yes 1068.45464 13638.70 4.401567e-03 No ## 849 No 708.81398 38295.23 1.109890e-03 No ## 2511 Yes 521.84148 10316.34 1.767045e-04 No ## 1864 No 1803.17026 36192.63 4.117378e-01 No ## 2133 No 800.24334 19051.32 1.900229e-03 No 11.5.3.4 Đánh giá chất lượng dự báo trên tập test 11.5.3.4.1 Confusion matrix #conf.matrix logit.perf &lt;- table(df.test$default, class.pred.test) logit.perf ## class.pred.test ## No Yes ## No 2889 12 ## Yes 71 28 ###performance #function to perform model model.performance &lt;- function(confusion_matrix) { a &lt;- confusion_matrix[1,1] b &lt;- confusion_matrix[1,2] c &lt;- confusion_matrix[2,2] d &lt;- confusion_matrix[2,1] recall &lt;- c/(c+d) precision &lt;- c/(b+c) accuracy &lt;- (a+c)/(a+b+c+d) print(paste(&#39;recall :&#39;,round(recall,2))) print(paste(&#39;precision :&#39;,round(precision,2))) print(paste(&#39;accuracy :&#39;,round(accuracy,2))) } model.performance(logit.perf) ## [1] &quot;recall : 0.28&quot; ## [1] &quot;precision : 0.7&quot; ## [1] &quot;accuracy : 0.97&quot; 11.5.3.4.2 ROC &amp; AUC Tất cả các chỉ số về chất lượng mô hình đều có thể sử dụng với package ROCR thông qua hai bước Bước 1: Tạo object dự báo pred &lt;- prediction(prob_value, actual_value). Object này sẽ chứa 1 loạt các chỉ số của confusion matrix với từng điểm cut-off Bước 2: Tính toán các chỉ số với hàm performance với giá trị trục x và y mong muốn Lưu ý: Object pred được lưu dưới dạng S4 class. Để chiết xuất dữ liệu cần dùng toán tử @ thay cho $ như S3 # Bước 1: Tạo object pred pred &lt;- prediction(prob.test, df.test$default) pred %&gt;% str ## Formal class &#39;prediction&#39; [package &quot;ROCR&quot;] with 11 slots ## ..@ predictions:List of 1 ## .. ..$ : Named num [1:3000] 1.25e-03 9.43e-03 3.86e-04 1.71e-05 1.71e-05 ... ## .. .. ..- attr(*, &quot;names&quot;)= chr [1:3000] &quot;1&quot; &quot;3&quot; &quot;4&quot; &quot;10&quot; ... ## ..@ labels :List of 1 ## .. ..$ : Ord.factor w/ 2 levels &quot;No&quot;&lt;&quot;Yes&quot;: 1 1 1 1 1 1 1 1 1 1 ... ## ..@ cutoffs :List of 1 ## .. ..$ : Named num [1:2863] Inf 0.954 0.942 0.913 0.895 ... ## .. .. ..- attr(*, &quot;names&quot;)= chr [1:2863] &quot;&quot; &quot;1161&quot; &quot;7438&quot; &quot;3977&quot; ... ## ..@ fp :List of 1 ## .. ..$ : num [1:2863] 0 0 0 1 1 1 1 1 1 2 ... ## ..@ tp :List of 1 ## .. ..$ : num [1:2863] 0 1 2 2 3 4 5 6 7 7 ... ## ..@ tn :List of 1 ## .. ..$ : num [1:2863] 2901 2901 2901 2900 2900 ... ## ..@ fn :List of 1 ## .. ..$ : num [1:2863] 99 98 97 97 96 95 94 93 92 92 ... ## ..@ n.pos :List of 1 ## .. ..$ : int 99 ## ..@ n.neg :List of 1 ## .. ..$ : int 2901 ## ..@ n.pos.pred :List of 1 ## .. ..$ : num [1:2863] 0 1 2 3 4 5 6 7 8 9 ... ## ..@ n.neg.pred :List of 1 ## .. ..$ : num [1:2863] 3000 2999 2998 2997 2996 ... # Bước 2: Tạo object tính toán prediction cho AUC roc &lt;- performance(pred, x.measure = &quot;fpr&quot;, measure = &quot;tpr&quot;) # Xây dựng hàm để convert giá trị sang data.frame get_df &lt;- function(pred){ pred_new &lt;- data.frame(x = pred@x.values %&gt;% unlist, y = pred@y.values %&gt;% unlist) return(pred_new) } get_df(roc) %&gt;% ggplot(aes(x, y)) + geom_line() + theme_minimal() + labs(x = &quot;FPR&quot;, y = &quot;TPR&quot;, title = &quot;ROC curve&quot;) # Tính chỉ số AUC auc_test &lt;- performance(pred, &quot;auc&quot;, &quot;cutoff&quot;) auc_test@y.values %&gt;% unlist ## [1] 0.9402992 11.5.3.4.3 Gain và Lift # GAIN charts on testing set gain_df &lt;- performance(pred, &quot;tpr&quot;, &quot;rpp&quot;) get_df(gain_df) %&gt;% ggplot(aes(x, y)) + geom_line() + theme_minimal() + labs(title = &quot;Gain chart&quot;, x = &quot;RPP&quot;, y = &quot;TPR&quot;) # LIFT charts on testing set lift_df &lt;- performance(pred, &quot;lift&quot;, &quot;rpp&quot;) get_df(lift_df) %&gt;% ggplot(aes(x, y)) + geom_line() + theme_minimal() + labs(title = &quot;Lift chart&quot;, x = &quot;RPP&quot;, y = &quot;Lift&quot;) 11.5.3.4.4 KS statistics lift_df &lt;- performance(pred, &quot;tpr&quot;, &quot;fpr&quot;) lift_df %&gt;% get_df %&gt;% mutate(ks = abs(x - y)) %&gt;% pull(ks) %&gt;% max ## [1] 0.7445917 data.frame(y = (df.test$default %&gt;% as.numeric) - 1, prob = prob.test) %&gt;% ggplot(aes(prob, col = as.factor(y))) + stat_ecdf() + theme_minimal() + theme(legend.position = &quot;none&quot;) + labs(tittle = &quot;CDF &amp; KS statistics&quot;) 11.6 Xây dựng mô hình Logistics theo Machine Learning Cost function: Đối với mô hình logistic, nếu sử dụng cost function tương tự như như hồi quy tuyến tính, chúng ta sẽ gặp vấn đề về bài toán tối ưu. Hàm tối ưu sẽ không thể tìm được điểm nhỏ nhất do tồn tại vô số các điểm “local optimization”. Do đó, ta phải xây dựng một hàm tối ưu mới cho bài toán phân loại của logistic như sau. \\[Cost(h_\\theta(x), y)\\cases{ -log(h_\\theta(x) \\text{ với } y = 1 \\\\ -log(1-h_\\theta(x)) \\text{ với } y = 0) }\\] Với hàm tối ưu trên, có 1 số điểm lưu ý như sau. Nếu y = 1 và \\(h_\\theta = 1\\) nghĩa là mô hình dự báo chính xác với trường hợp xảy ra sự kiện, cost = 0 Tuy nhiên, nếu y = 1 và \\(h_\\theta \\rightarrow 0\\), nghĩa là mô hình dự báo không xảy ra sự kiện (y = 0) trong khi thực tế xảy ra, \\(cost \\rightarrow \\infty\\). Trong thực tế, hàm tối ưu được đơn giản hóa như sau. \\[Cost(h_\\theta, y) = -y log(h_\\theta(x)) - (1-y)log(1-h_\\theta(y))\\] Với từng giá trị y = 0 hoặc y = 1, hàm tối ưu quay trở lại như bình thường. Đối với hàm tối ưu như trên, thuật toán gradient descent quay trở lại tương tự như hàm hồi quy tuyến tính. Tối ưu hóa: Bên cạnh thuật toán gradient descent, còn có các thuật toán khác giúp tối ưu hóa hàm tối ưu (cost function) với tốc độ nhanh hơn và không cần phải lựa chọn tham số \\(\\alpha\\) (learning rate) thủ công. Một số thuật toán có thể kể đến như: Conjungate gradient BFGS L-BFGS "],
["cay-quyt-inh.html", "Chương 12 Cây quyết định 12.1 Giới thiệu 12.2 Regression Trees 12.3 Classification Trees", " Chương 12 Cây quyết định Trong chương này, chúng ta sẽ sẽ tìm hiểu về những phương pháp dựa vào cây quyết định (tree-based methods) để giải quyết những bài toán hồi quy (regression) cũng như những bài toán phân loại (classification). Các phương pháp này sẽ nhóm các quan sát vào một số vùng (regions) nhất định, và để dự báo một quan sát mới, chúng ta thường sử dụng giá trị trung bình (đối với bài toán hồi quy) hoặc giá trị mode, hay majority vote (đối với bài toán phân loại) của những quan sát tập dữ liệu training trong vùng mà quan sát đó thuộc về. Trong chương này, chúng ta sẽ được giới thiệu lần lượt các phương pháp tree-based điển hình như: decision trees, bagging, random forests và boosting. 12.1 Giới thiệu Phương pháp decision trees (cây quyết định) gồm tập hợp các nguyên tắc phân nhóm (spliting rules) được sử dụng để nhóm các quan sát vào một số vùng (regions) nhất định mà được thống kê trong một cây. Phương pháp decision trees có thể áp dụng đối với cả hai bài toán hồi quy (regression) và phân loại (classification). 12.2 Regression Trees Phương pháp cây quyết định hồi quy (regression trees) dùng cho trường hợp khi biến đầu ra chúng ta muốn dự báo là biến liên tục hay biến định lượng Chúng ta sẽ sử dụng dữ liệu có sẵn trong R - Hitters trong packge ISLR để dự báo thu nhập của cầu thủ bóng chày (salary) dựa vào số năm chơi bóng tại các giải đấu lớn (years) và số điểm ghi được trong mùa giải trước (hits). library(ISLR) library(tidyverse) data(&quot;Hitters&quot;) names(Hitters) &lt;- names(Hitters) %&gt;% tolower # Lấy 3 biến: `years`, `hits`, `salary` data &lt;- Hitters %&gt;% select(years,hits,salary) Đầu tiên chúng ta sẽ loại bỏ những quan sát bị missing ở biến salary, và lấy log của biến salary để biến này tiệm cận với phân phối chuẩn hơn (phân phối hình chuông). Biến salary đơn vị là nghìn USD. data_new &lt;- data %&gt;% # Loại bỏ giá trị missing ở biến `salary` filter(!is.na(salary)) %&gt;% # log-transform mutate(salary = log(salary)) # Biểu đồ phân phối biến `salary` của `data` data %&gt;% ggplot(aes(salary))+ geom_density()+ theme_minimal() # Biểu đồ phân phối biến `salary` của `data_new` data_new %&gt;% ggplot(aes(salary))+ geom_density()+ theme_minimal() Giả sử chúng ta xây dựng một cây hồi quy (regression tree) với dữ liệu nói trên và thu được kết quả như sau: Với tập dữ liệu này, trung bình log-transform salary là 5.9, tức thu nhâp trung bình của các cầu thủ bóng chày là exp(5.9) = 375 (nghìn USD). Cây quyết định trên bao gồm tập hợp các nguyên tắc phân nhóm (spliting rules), bắt đầu từ trên xuống dưới. Với nguyên tắc phân nhóm đầu tiên Years &lt; 4.5 thì ở nhánh bên trái là những quan sát thỏa mãn điều kiện trên, và trung bình log-transform salary của nhóm này là 5.1. Nghĩa là, nhóm cầu thủ có số năm chơi bóng tại các giải đấu lớn ít hơn 4.5 năm có mức thu nhập trung bình là exp(5.1) = 164 (nghìn USD). Còn đối với những cầu thủ mà chơi bóng từ 4.5 năm trở lên tại các giải đấu lớn thì có thu nhập trung bình là exp(6.4) = 601 (nghìn USD). Tuy nhiên nhóm này còn chia thành 2 nhóm nhỏ hơn với nguyên tắc phân nhóm Hits &lt; 118. Nhóm thỏa mãn điều kiện Hits &lt; 118 (số điểm ghi được trong mùa giải trước ít hơn 118) có mức thu nhập trung bình là exp(6) = 403 (nghìn USD) Nhóm ghi được từ 118 điểm trở lên tại mùa giải trước có mức thu nhập trung bình là exp(6.7) = 812 (nghìn USD). Như vậy, cây quyết định nói trên đã phân loại các cầu thủ và 3 vùng/nhóm (regions of predictor space): R1: years &lt; 4.5 R2: years &gt;= 4.5, hits &lt; 118 R3: years &gt;= 4.5, hits &gt;= 118 data_new %&gt;% ggplot(aes(years,hits)) + geom_point(col = &quot;gold&quot;)+ theme_bw()+ geom_vline(aes(xintercept = 4.5), col = &quot;blue&quot;, size = 1)+ geom_segment(aes(x = 4.5, y = 118, xend = Inf, yend = 118 ), col = &quot;blue&quot;, size = 1)+ theme(panel.grid = element_blank())+ scale_x_continuous(breaks = 4.5)+ scale_y_continuous(breaks = 118)+ theme(axis.text = element_text(face = &quot;bold&quot;, size = 10))+ annotate(&quot;text&quot;, x = 4.5/2, y = 118, label = &quot;R1&quot;, col = &quot;red&quot;, size = 6)+ annotate(&quot;text&quot;, x = (32-4.5)/2, y = 118/2, label = &quot;R2&quot;, col = &quot;red&quot;, size = 6)+ annotate(&quot;text&quot;, x = (32-4.5)/2, y = 118*1.5, label = &quot;R3&quot;, col = &quot;red&quot;, size = 6) Mức thu nhập dự báo của 3 nhóm này lần lượt là 164,000 USD, 403,000 USD, 812,000 USD. Trong ví dụ này R1, R2, R3 ở đây được gọi là terminal nodes hoặc leaves (lá cây). years &lt; 4.5 và hits &lt; 118 được gọi là internal nodes hoặc decision nodes. Chúng ta có thể diễn giải cây quyết định hồi quy nói trên như sau: Biến years là quan trọng nhất ảnh hưởng tới việc dự báo salary. Nhóm cầu thủ có số năm chơi bóng tại các giải đấu lớn nhiều hơn sẽ có mức thu nhập cao hơn. Nhóm cầu thủ có số năm chơi bóng dưới 4.5 năm thì số điểm họ ghi được trong mùa giải trước đấy không ảnh hưởng đến thu nhập của họ, trong khi đó nhóm cầu thủ có số năm chơi bóng từ 4.5 năm trở lên tại các giải đấu lớn thì ngược lại. Số điểm ghi được trong mùa giải trước đó của nhóm cầu thủ này có ảnh hưởng đến thu nhập của họ, nhóm cầu thủ ghi được từ 118 điểm trở lên có mức thu nhập cao hơn nhóm còn lại. Bây giờ, chúng ta sẽ cùng tìm hiểu quá trình xây dựng cây quyết định hồi quy (regression tree). Bao gồm 2 bước sau: Bước 1: Chia các quan sát (tức tìm tập hợp những giá trị phù hợp cho các biến X1, X2,…, Xp) vào j vùng/nhóm khác nhau (non-overlapping) R1, R2,…, Rj. Bước 2: Với mỗi quan sát mà thuộc về vùng/nhóm Rj, chúng ta sẽ dự báo chúng cùng một giá trị, bằng với trung bình các giá trị quan sát ở tập dữ liệu training trong vùng/nhóm Rj. Như ví dụ về dự báo thu nhập của cầu thủ bóng chày nói trên, ở bước 1 chúng ta chia các cầu thủ thành 3 nhóm: R1, R2, R3. Thu nhập trung bình của 3 nhóm trên lần lượt là 164,000 USD, 403,000 USD, 812,000 USD. Vậy nếu một cầu thủ bất kỳ mà thuộc về R1, thì chúng ta sẽ dự báo thu nhập của cầu thủ này là 164,000 USD, nếu thuộc về R2, thì dự báo là 403,000 USD, và nếu thuộc R3 - 812,000 USD. Chúc ta sẽ cùng tìm hiểu sâu hơn về bước 1 nói trên. Làm thế nào để xây dựng được các vùng/nhóm R1,.., Rj? Về lý thuyết, các vùng này có thể có hình dạng bất kỳ. Tuy nhiên, chúng ta sẽ chia các quan sát (predictor space) thành boxes (các hộp) để cho đơn giản và dễ giải thích kết quả dự báo của mô hình. Mục đích là đi tìm boxes R1,…, Rj sao cho tối thiểu hóa RSS (Residual Sum of Squares): \\[\\sum_{j=1}^{J}\\sum_{i \\epsilon R_j}^{J}(y_i - \\widehat{y}_{R_j})^2\\] Trong đó yRj là trung bình kết quả của những quan sát trên tập dữ liệu training trong box thứ j. Tuy nhiên việc tính toán biểu thức trên sẽ rất phức tạp, do vậy chúng ta sẽ dùng cách tiếp cận top-down, greedy hay còn được gọi là recursive binary splitting. Sở dĩ cách tiếp cận trên là top-down vì nó bắt đầu từ phần đỉnh của cây (nơi mà tất cả các quan sát thuộc một nhóm ban đầu), sau đó sẽ phân nhóm các quan sát, mỗi sự phân nhóm sẽ chia làm 2 nhánh (branches) mới xuống phía dưới. Còn việc nói cách tiếp cận trên là greedy vì tại mỗi bước trong quá trình xây dựng cây, sự phân nhóm tốt nhất (best split) sẽ được sử dụng. Để có thể thực hiện cách tiếp cận recursive binary splitting nêu trên, chúng ta đầu tiên sẽ lấy những biến Xj và cutpoint s để chia các quan sát vào các vùng/nhóm {X|Xj &lt; s} và {X|Xj &gt;= s} sao cho tối thiểu hóa RSS. {X|Xj &lt; s} ở đây được hiểu là vùng bao gồm các quan sát mà thỏa mãn điều kiện Xj &lt; s. Nghĩa là, chúng ta sẽ xem xét tất cả các biến X1,…, Xp và tất cả các cutpoint s cho mỗi biến, rồi sau đó sẽ lựa chọn những biến và cutpoint để sao cho cây quyết định cuối cùng có RSS nhỏ nhất. Nói một cách tổng quát hơn, với mọi giá trị j và s, chúng ta xác định cặp nửa mặt phẳng sau: \\[R_1(j,s) = \\lbrace X|X_j &lt; s \\rbrace , R_2(j,s) = \\lbrace X|X_j &gt;= s \\rbrace\\] Và chúng ta sẽ đi tìm giá trị j và s để tối thiểu hóa biểu thức sau: \\[\\sum_{i: x_i \\epsilon R_1(j,s)}^{J}(y_i - \\widehat{y}_{R_1})^2 + \\sum_{i: x_i \\epsilon R_2(j,s)}^{J}(y_i - \\widehat{y}_{R_2})^2\\] Trong đó: yR1 - trung bình kết quả các quan sát trên tập dữ liệu training trong R1(j,s) yR2 - trung bình kết quả các quan sát trên tập dữ liệu training trong R2(j,s) Việc tìm j và s sẽ khá nhanh, đặc biệt đối với trường hợp khi số lượng biến p ít. Tiếp theo đó, chúng ta sẽ lặp lại quá trình nói trên, lựa chọn biến tốt nhất và cutpoint tốt nhất để tiếp tục split dữ liệu sao để tối thiểu hóa RSS trong mỗi vùng/nhóm kết quả. Tuy nhiên, lần này thay vì việc chúng ta split toàn bộ các quan sát, chúng ta chỉ split 1 trong 2 vùng/nhóm đã được xác định trước đó. Như vậy, bây giờ chúng ta có 3 vùng/nhóm. Cứ tiếp tục như vậy, chúng ta lại split 1 trong 3 vùng/nhóm này để tối thiểu hóa RSS. Quá trình cứ tiếp tục diễn ra cho đến khi nó dừng lại theo tiêu chí đặt ra của chúng ta, ví dụ chúng ta đặt điều kiện là sẽ tiếp tục quá trình cho đến khi không có vùng/nhóm nào bao gồm nhiều hơn 10 quan sát. Như vậy, các vùng R1, R2,…, Rj được tạo ra, chúng ta dự báo kết quả của các quan sát mới bằng việc sử dụng giá trị trung bình kết quả các quan sát trên tập dữ liệu training ở vùng/nhóm mà quan sát mới đó thuộc về. Tree Pruning Quá trình mô tả bên trên có thể dự báo tương đối chính xác trên tập dữ liệu training, nhưng có thể thiếu chính xác trên tập dữ liệu testing, vấn đề này được gọi là overfitting. Đó là bởi vì cây quyết định được xây dựng quá phức tạp (nhiều splits). Cây quyết định nhỏ hơn với ít splits hơn (ít vùng/nhóm hơn) có thể dẫn đến việc variance thấp hơn và tính giải thích cao hơn. Để khắc phục vấn đề overfitting, chúng ta có thể dùng kỹ thuật “tỉa” cây (pruning). Cách tiếp cận tốt nhất là chúng ta sẽ xây dựng một cây lớn To, sau đó sẽ “tỉa” (prune) cây để thành subtree (cây con). Nhưng làm thế nào để xác định được cách “tỉa” cây tốt nhất? Theo trực giác, chúng ta sẽ chọn subtree mà có tỷ lệ sai số thấp nhất trên tập dữ liệu mới (test error rate). Khi subtree được xác định, chúng ta có thể ước lượng sai số trên tập dữ liệu mới của subtree đó bằng việc sử dụng cross-validation hoặc dữ liệu mới (validation set). Tuy nhiên việc ước lượng cross-validation error cho từng subtree có thể có sẽ rất phức tạp và tốn nhiều công sức, vì số lượng subtree có thể sẽ rất nhiều. Thay vì việc đó, chúng ta cân nhắc việc lựa chọn một tập hợp nhỏ các subtree để xem xét. Cost complexity pruning, còn gọi là weakest link pruning là một cách để chúng ta thực hiện việc nói trên. Thay vì việc xem xét tất cả các subtree có thể, chúng ta có thể xem xét một chuỗi các cây được indexed bởi một tham số không âm α. Thuật toán xây dựng cây quyết định hồi quy (regression tree): Bước 1: Sử dụng recursive binary splitting để xây dựng một cây lớn từ tập dữ liệu training, và chỉ dừng lại khi mỗi terminal node có ít hơn một số lượng quan sát tối thiểu nhất định. Bước 2: Áp dụng cost complexity pruning để có được một chuỗi các subtree tốt nhất, as a function of α. Bước 3: Sử dụng K-fold cross-validation để lựa chọn α. Tức là, chia các quan sát của tập dữ liệu training vào K-fold. Với k = 1,…, K: Lập lại bước 1 và 2 nhưng với k-th fold của tập dữ liệu training. Tính toán mean squared prediction error on the data in the left-out kth fold, as a function of α. Tính trung bình các kết quả cho từng giá trị α, và chọn α để tối thiểu hóa sai số trung bình (average error). Bước 4: Lựa chọn subtree từ bước 2 mà tương ứng với việc lựa chọn α. Với mỗi giá trị α sẽ tương ứng với subtree T thuộc To mà: \\[\\sum_{m=1}^{|T|}\\sum_{i: x_i \\epsilon R_m}^{J}(y_i - \\widehat{y}_{R_m})^2 + α|T|\\] nhỏ nhất có thể. Ở đây, T - số lượng terminal nodes của cây, Rm - rectangle (the subset of predictor space) tương ứng với terminal node thứ m, yRm - giá trị dự báo (tức là trung bình các giá trị quan sát trong Rm). Tham số α sẽ kiểm soát sự đánh đổi giữa độ phức tạp của subtree và chất lượng dự báo trên tập dữ liệu train. Nếu α = 0, thì subtree T = To, vì biểu thức nói trên trở thành training error. Biểu thức trên làm chúng ta liên tưởng đến lasso - một biểu thức tương tự dùng để kiểm soát độ phức tạp của mô hình hồi quy tuyến tính. 12.3 Classification Trees Cây quyết định phân loại (classification tree) tương tự như cây quyết định hồi quy (regression tree), chỉ khác điểm đối với classification tree thì biến đầu ra muốn dự báo là biến rời rạc hay biến định tính (categorical variable) thay vì là biến liên tục như đối với regression tree. Đối với regression tree thì kết quả dự báo cho một quan sát mới chính là trung bình kết quả của các giá trị quan sát trong tập dữ liệu training tại vùng (region) mà quan sát đó thuộc về. Nhưng đối với classification tree, kết quả dự báo cho một quan sát mới sẽ là giá trị mà có tần suất xuất hiện nhiều nhất trong số các quan sát của tập dữ liệu training tại vùng mà quan sát đó thuộc về. Giả sử, chúng ta đã xây được một cây quyết định dự báo xem khách hàng có trả được hết nợ hoặc không (tức biến đầu ra có 2 class là trả được nợ và không trả được nợ). Và kết quả là cây quyết định đó chia thành khách hàng vào 3 vùng (regions) khác nhau: A,B,C. Bây giờ chúng ta đang muốn dự báo xem một khách hàng mới Nguyễn Văn T có trả được nợ hay không dựa vào mô hình đã được xây dựng. Giả sử, với những nguyên tắc phân nhóm (spliting rules) của cây quyết định, chúng ta đã phân nhóm được khách hàng nói trên vào vùng A nhất định. Như vậy, nếu trong các khách hàng thuộc vùng A trước đó, đa số là khách hàng trả được nợ (tức số lượng khách hàng trả được nợ nhiều hơn số lượng khách hàng không trả được nợ), thì chúng ta có thể dự báo rằng khách hàng mới nói trên có trả được nợ. Khi diễn giải kết quả của classification tree, chúng ta không chỉ quan tâm đến class được dự báo tại mỗi vùng (region) nhất định mà còn quan tâm đến class propotions giữa các quan sát trong tập dữ liệu training mà thuộc về vùng đó. Như trong ví dụ trên, ngoài việc dự báo được khách hàng thuộc vùng A sẽ có khả năng trả được nợ, chúng ta còn muốn biết cụ thể tỷ lệ khách hàng trả được nợ và không trả được nợ tại vùng A là bao nhiêu, và tỷ lệ khách hàng thuộc vùng A trên tổng số khách hàng là bao nhiêu. Cũng giống như regression tree, việc xây dựng classification tree cũng sử dụng recursive binary splitting. Tuy nhiên đối với classification tree, RSS không được sử dụng như một tiêu chí để làm phân nhóm nhị phân (binary splits), thay vào đó là classification error rate. Khi chúng ta đã xác định được quan sát mới vào class mà tần suất xuất hiện nhiều nhất trong vùng mà quan sát đó thuộc về, classification error rate sẽ chính là tỷ lệ số quan sát trên tập dữ liệu training trong vùng đó mà không thuộc vào class đa số: \\[E = 1 - \\max_k(\\widehat{p}_{mk})\\] Ở đây pmk - tỷ trọng quan sát trên tập dữ liệu training trong vùng thứ m từ class thứ k. Tuy nhiên, classification error đôi khi sẽ không hiệu quả đối với một số trường hợp trong thực tế, vì vậy, có 2 thước đo khác mà chúng ta nên sử dụng: Chỉ số Gini: \\[G = \\sum_{k=1}^{K}\\widehat{p}_{mk}(1-\\widehat{p}_{mk})\\] Đây là một chỉ số về tổng variance qua K classes. Dễ nhận thấy rằng chỉ số Gini sẽ càng nhỏ nếu pmk tiếp cận 0 hoặc 1. Vì lý do đó chỉ số Gini được coi là thước đo độ đồng nhất của node (node purity). Một chỉ số khác thay thế cho chỉ số Gini là cross-entropy: \\[D = -\\sum_{k=1}^{K}\\widehat{p}_{mk}log(\\widehat{p}_{mk})\\] pmk nhận giá trị từ 0 đến 1, nên -pmk*log(pmk) sẽ không âm. Cross-entropy sẽ tiếp cận đến 0 nếu tất cả các giá trị pmk tiệm cận đến 0 hoặc 1. Vì vậy, cũng giống như chỉ số Gini, cross-entropy sẽ mang giá trị nhỏ nếu như node thứ m đồng nhất. Khi xây dựng classification tree, cả chỉ số Gini và cross-entropy đều thường dùng để đánh giá chất lượng của split cụ thể nào đó. Hai chỉ số trên more sensitive với node purity hơn là classification error rate. Cả 3 chỉ số trên đều có thể sử dụng để “tỉa” (prune) cây, nhưng classification error rate sẽ được ưu tiên sử dụng khi chúng ta muốn tính độ chính xác của dự báo (prediction accuracy) của cây quyết định cuối cùng sau khi đã được “tỉa”. Như vậy, classification tree thiết lập một tập hợp các nguyên tắc phân nhóm nhị phân (binary splits) với các biến đầu vào (predictor variables) để xây dựng một cây quyết định sao cho có thể phân loại (classify) những quan sát mới vào một trong hai nhóm xác định. Để thực hành xây dựng classification tree trên R, chúng ta sẽ sử dụng dữ liệu Breast Cancer Wisconsin, bao gồm: Biến đầu ra là biến rời rạc nhị phân (benign/malignant - ung thư lành tính/ác tính) và tập hợp các biến đầu vào (the nine cytology measurements - 9 đơn vị đo tế bào). Thuật toán gồm các bước sau: Bước 1: Chọn biến đầu vào phân nhánh tốt nhất (best splits) để chia dữ liệu thành 2 nhóm sao cho tính đồng nhất (the purity/homogeneity) của outcome trong 2 nhóm là maximized (tức là càng nhiều trường hợp lành tính vào một nhóm, và các nhiều trường hợp ác tính vào nhóm còn lại). Nếu biến đầu vào là biến liên tục (continuous), chọn một điểm cut-off để maximize purity của 2 nhóm được tạo ra. Nếu biến đầu vào là biến rời rạc (categorical), combine the categories to obtain two groups with maximum purity. Bước 2: Chia dữ liệu thành 2 nhóm nói trên và tiếp tục quá trình cho từng nhóm con (subgroup) Bước 3: Lặp lại bước 1 và bước 2 cho đến khi a subgroup bao gồm số quan sát ít hơn một số lượng nhất định mà chúng ta đặt ra hoặc không thể phân nhóm tiếp để làm tăng tính đồng nhất The subgroups in the final set được gọi là terminal nodes. Mỗi terminal node được phân loại vào một category of the outcome or the other dựa vào the most frequent value of the outcome for the sample in that node. Bước 4: To classify a case, run it down the tree to a terminal node, and assign it the modal outcome value assigned in step 3. Quá trình trên có thể dẫn đến việc cây được xây dựng rất là lớn và sẽ dẫn đến overfitting (chỉ “chính xác” trên tập dữ liệu huấn luyện - training data, nhưng thiếu “chính xác” trên tập dữ liệu kiểm tra - testing data). Kết quả là, những quan sát mới sẽ không được phân loại chính xác. Để khắc phục vấn đề trên chúng ta có thể dùng kỹ thuật “tỉa” cây (pruning) bằng việc lựa chọn cây với 10-fold cross-validated prediction error nhỏ nhất. Cây đã được tỉa sẽ được sử dụng cho việc dự báo các quan sát trong tương lai. Trong R, việc xây dựng cây quyết định (decision trees) cũng như “tỉa cành” chúng ta có thể dùng hàm rpart() và prune() trong package rpart. Dưới đây sẽ là ví dụ về việc xây dựng cây quyết định để phân loại các quan sát vào 2 nhóm bệnh nhân ung thư: lành tính hay ác tính. # Data loc &lt;- &quot;http://archive.ics.uci.edu/ml/machine-learning-databases/&quot; ds &lt;- &quot;breast-cancer-wisconsin/breast-cancer-wisconsin.data&quot; url &lt;- paste(loc, ds, sep=&quot;&quot;) breast &lt;- read.table(url, sep=&quot;,&quot;, header=FALSE, na.strings=&quot;?&quot;) names(breast) &lt;- c(&quot;ID&quot;, &quot;clumpThickness&quot;, &quot;sizeUniformity&quot;, &quot;shapeUniformity&quot;, &quot;maginalAdhesion&quot;, &quot;singleEpithelialCellSize&quot;, &quot;bareNuclei&quot;, &quot;blandChromatin&quot;, &quot;normalNucleoli&quot;, &quot;mitosis&quot;, &quot;class&quot;) df &lt;- breast[-1] df$class &lt;- factor(df$class, levels=c(2,4), labels=c(&quot;benign&quot;, &quot;malignant&quot;)) set.seed(1234) train &lt;- sample(nrow(df), 0.7*nrow(df)) df.train &lt;- df[train,] df.validate &lt;- df[-train,] # Biến đầu ra của dữ liệu huấn luyện table(df.train$class) ## ## benign malignant ## 329 160 # Biến đầu ra của dữ liệu kiểm tra table(df.validate$class) ## ## benign malignant ## 129 81 Trước tiên, chúng ta sẽ xây dựng một cây quyết định với tập dữ liệu huấn luyện, sử dụng hàm rpart(). # Classical decision tree library(rpart) set.seed(1234) # 1. Grows the tree dtree &lt;- rpart(class ~ ., data = df.train, method = &quot;class&quot;, # biến đầu ra - rời rạc parms=list(split=&quot;information&quot;) ) Để xem và thống kê kết quả của mô hình vừa xây dựng, chúng ta có thể sử dụng hàm print() và summary(). Cây quyết định có thể sẽ rất lớn và cần phải sử dụng kỹ thuật “tỉa cành”. print(dtree) ## n= 489 ## ## node), split, n, loss, yval, (yprob) ## * denotes terminal node ## ## 1) root 489 160 benign (0.67280164 0.32719836) ## 2) sizeUniformity&lt; 3.5 347 25 benign (0.92795389 0.07204611) ## 4) bareNuclei&lt; 2.5 303 2 benign (0.99339934 0.00660066) * ## 5) bareNuclei&gt;=2.5 44 21 malignant (0.47727273 0.52272727) ## 10) shapeUniformity&lt; 2.5 23 5 benign (0.78260870 0.21739130) ## 20) clumpThickness&lt; 3.5 15 0 benign (1.00000000 0.00000000) * ## 21) clumpThickness&gt;=3.5 8 3 malignant (0.37500000 0.62500000) * ## 11) shapeUniformity&gt;=2.5 21 3 malignant (0.14285714 0.85714286) * ## 3) sizeUniformity&gt;=3.5 142 7 malignant (0.04929577 0.95070423) * Để lựa chọn kích thước của cây, chúng ta có thể kiểm tra phần cptable trong kết quả model dtree như sau: # Data about the prediction error for various tree sizes dtree$cptable ## CP nsplit rel error xerror xstd ## 1 0.800000 0 1.00000 1.00000 0.06484605 ## 2 0.046875 1 0.20000 0.30625 0.04150018 ## 3 0.012500 3 0.10625 0.20625 0.03467089 ## 4 0.010000 4 0.09375 0.18125 0.03264401 Kết quả cho chúng ta thấy thông tin về sai số dự báo đối với mỗi kích thước khác nhau của cây. CP - tham số đánh giá độ phức tạp của mô hình (complexity parameter) dùng để penalize larger trees nsplit - số lần phân nhóm (number of branch splits) mô tả kích thước của cây. Một cây với n splits sẽ có n+1 terminal nodes rel error - tỷ lệ sai số (error rate) của cây đối với từng kích thước được xác định trên tập dữ liệu training. xerror - cross-validated error dựa trên 10-fold cross validation (cũng dựa trên dữ liệu training) xstd - sai số chuẩn (standard error) của cross validation error Chúng ta có thể sử dụng hàm plotcp() để xem mối quan hệ giữa cross-validated error (xerror) và tham số complexity (CP). Một sự lựa chọn tốt cho kích thước của cây là cây nhỏ nhất mà cross-validated error nằm trong khoảng giá trị 1 standard error của cross-validated error nhỏ nhất # Plot the cross-validated error against the complexity parameter plotcp(dtree) Trong trường hợp này, min xerror là 0.18 và tương ứng với xstd = 0.0326, như vậy cây nhỏ nhất với xerror nằm trong khoảng giá trị (0.18-0.0326, 0.18+0.0326), hay (0.15, 0.21). Xem bảng kết quả cptable có thể thấy cây với 3 splits (xerror = 0.20625) thỏa mãn điều kiện nói trên. Còn nếu dựa vào plot mối quan hệ giữa xerror và CP, chúng ta sẽ lựa chọn cây với giá trị CP ngoài cùng bên trái (CP cao nhất) nằm dưới đường line. Như vậy, kết quả là chúng ta nên lựa chọn cây với 3 splits (4 terminal nodes). Hàm prune() sử dụng tham số complexity để tỉa cây theo kích thước mong muốn. Cụ thể hơn, cách này sẽ cắt bỏ đi các splits mà ít quan trọng nhất của cây theo giá trị của tham số complexity mà chúng ta mong muốn. Trong trường hợp này, chúng ta lựa chọn cây với 3 splits, tương ứng với complexity (CP = 0.0125), do đó để lựa chọn cây với kích thước mong muốn, chúng ta sử dụng câu lệnh sau: # 2. Prunes the tree dtree.pruned &lt;- prune(dtree, cp = 0.0125) print(dtree.pruned) ## n= 489 ## ## node), split, n, loss, yval, (yprob) ## * denotes terminal node ## ## 1) root 489 160 benign (0.67280164 0.32719836) ## 2) sizeUniformity&lt; 3.5 347 25 benign (0.92795389 0.07204611) ## 4) bareNuclei&lt; 2.5 303 2 benign (0.99339934 0.00660066) * ## 5) bareNuclei&gt;=2.5 44 21 malignant (0.47727273 0.52272727) ## 10) shapeUniformity&lt; 2.5 23 5 benign (0.78260870 0.21739130) * ## 11) shapeUniformity&gt;=2.5 21 3 malignant (0.14285714 0.85714286) * ## 3) sizeUniformity&gt;=3.5 142 7 malignant (0.04929577 0.95070423) * Hàm prp() trong package rpart.plot sử dụng để trực quan hóa cây quyết định cuối cùng. Để phân loại một quan sát, bắt đầu từ phần đỉnh của cây, sau đó di chuyển sang nhánh bên trái nếu một điều kiện là đúng hoặc sang phải trường hợp ngược lại. Tiếp tục di chuyển xuống phía dưới cho đến khi gặp terminal node (lá), tức đã được phân loại. library(rpart.plot) prp(dtree.pruned, type = 2, # Draws the split labels below each node extra = 104, # Includes the probabilities for each class, along with the percentage of observations in each node fallen.leaves = T, # Displays the terminal nodes at the bottom of the graph main = &quot;Decision Tree&quot; ) Cách đọc kết quả từ cây quyết định từ trên xuống dưới như sau: Tập dữ liệu huấn luyện ban đầu của chúng ta (tức bao gồm tất cả quan sát - 100%) có 67% là benign, 33% là malignant Với điều kiện phân nhánh đầu tiên sizeUnif &lt; 3.5: 71% thỏa mãn điều kiện (sẽ thuộc nhóm benign), trong đó : 93% là benign 7% là malignant 29% ko thỏa mãn điều kiện (sẽ thuộc nhóm malignan), trong đó: 5% là benign 95% là malignant … Kết quả cuối cùng, các quan sát trong tập training được chia thành 4 vùng: R1: sizeUnif &gt;= 3.5 -&gt; thuộc về nhóm malignan R2: sizeUnif &lt; 3.5 &amp; bareNucl &lt; 2.5 -&gt; thuộc về nhóm benign R3: sizeUnif &lt; 3.5 &amp; bareNucl &gt;= 2.5 &amp; shapeUni &lt; 2.5 -&gt; thuộc về nhóm benign R4: sizeUnif &lt; 3.5 &amp; bareNucl &gt;= 2.5 &amp; shapeUni &gt;= 2.5 -&gt; thuộc về nhóm malignan Và sau khi đã build được mô hình cây quyết định hoàn chỉnh, chúng ta sẽ phân loại những quan sát mới - tập dữ liệu validation bằng việc sử dụng hàm predict(), sau đó só sánh kết quả dự báo và thực tế như sau: # Phân loại những quan sát mới dtree.pred &lt;- predict(dtree.pruned, df.validate, type = &quot;class&quot; ) # So sánh kết quả dự báo và thực tế dtree.perf &lt;- table(df.validate$class, dtree.pred, dnn = c(&quot;Actual&quot;, &quot;Predicted&quot;) ) dtree.perf ## Predicted ## Actual benign malignant ## benign 122 7 ## malignant 2 79 Kết quả cho chúng ta thấy trong số 210 quan sát của tập dữ liệu kiểm tra (quan sát mới) có 122 quan sát là benign và 79 quan sát là malignant được dự báo đúng. Như vậy, tổng số quan sát được dự báo đúng là 201 (122+79), tức tỷ lệ quan sát dự báo đúng (accuracy) là 96% (201/210). Lưu ý rằng phương pháp cây quyết định có thể bị biased đối với những biến đầu vào có nhiều giá trị hoặc nhiều giá trị bị missing. Mô hình cây và mô hình tuyến tính Mô hình cây quyết định là một cách tiếp cận khác so với mô hình tuyết tính, cụ thể là hồi quy tuyến tính có dạng: \\[f(X) = \\beta_0 + \\sum_{j=1}^{P}X_jB_j\\] Như vậy, câu hỏi đặt ra là mô hình nào tốt hơn? Câu trả lời là: Tùy thuộc vào bài toán mà chúng ta muốn giải quyết là gì. Nếu bài toán chúng ta muốn giải quyết bao gồm biến đầu ra và các biến đầu vào có mối quan hệ gần giống với tuyến tính thì chúng ta nên sử dụng hồi quy tuyến tính. Còn nếu giữa các biến đầu vào và biến đầu ra có mối quan hệ phi tuyến tính hoặc phức tạp thì chúng ta nên sử dụng mô hình cây quyết định. Để đánh giá chất lượng dự báo của mô hình cây quyết định và mô hình tuyến tính, chúng ta có thể tính toán sai số trên tập dữ liệu validation hoặc tính toán sai số sử dụng cross-validation. Ưu điểm và nhược điểm của mô hình cây quyết định Mô hình cây quyết định có những ưu điểm vượt trội so với mô hình tuyến tính như sau: Thứ nhất, mô hình cây quyết định có thể mô tả bằng graphic, vì vậy rất dễ dàng diễn giải kết quả mô hình, thậm chí là dễ dàng hơn so với mô hình tuyến tính. Thứ hai, mô hình cây quyết định có thể sử dụng với những biến đầu vào là biến rời rạc mà không cần phải biến đổi về dạng dummy (0-1). Tuy nhiên, nhược điểm của mô hình cây quyết định là thiếu ổn định (non-robust). Hay nói cách khác, với một sự thay đổi nhỏ trong tập dữ liệu, có thể dẫn đến sự thay đổi lớn trong kết quả dự báo. Tuy nhiên, những phương pháp mà tổng hợp nhiều cây quyết định như: bagging, random forests, boosting có thể khắc phục được nhược điểm nêu trên và cải thiện chất lượng dự báo. Chúng ta sẽ cùng tìm hiểu trong các phần tiếp theo. "],
["bagging-boosting-va-random-forest.html", "Chương 13 Bagging - Boosting và Random Forest 13.1 Bagging 13.2 Random Forests 13.3 Boosting", " Chương 13 Bagging - Boosting và Random Forest 13.1 Bagging Mô hình cây quyết định như đã đề cập, sẽ gặp phải vấn đề high variance. Nghĩa là nếu chúng ta chia ngẫu nhiên tập dữ liệu training làm 2 tập dữ liệu con, và sau đó xây dựng mô hình trên 2 tập dữ liệu con đó, kết quả nhận được sẽ có thể khá khác nhau. Boostrap aggregation hay bagging sẽ có thể làm giảm variance, đây là một phương pháp rất hiệu quả và phổ biến khi chúng ta sử dụng các phương pháp liên quan đến cây quyết định. Giả sử, có tập hợp n quan sát độc lập Z1,…, Zn, mỗi quan sát với variance σ^2, variance của giá trị trung bình các quan sát Z là σ^2/n. Nói cách khác, trung bình tập hợp các quan sát sẽ làm giảm variance. Vì thế cách tự nhiên nhất để làm giảm variance và tăng độ chính xác của dự báo là lấy thật nhiều các dữ liệu training khác nhau, rồi xây dựng các mô hình dự báo sử dụng các tập dữ liệu training đó, sau đó lấy trung bình các kết quả dự báo. Nói cách khác, chúng ta sẽ tính toán fˆ1(x), fˆ2(x), . . . , fˆB(x) sử dụng B tập dữ liệu training khác nhau, sau đó lấy trung bình để nhận được một mô hình đơn với variance thấp như sau: \\[\\widehat{f}_{avg}(x) = \\frac{1}{B}\\sum_{b=1}^{B}\\widehat{f}^{b}(x)\\] Tuy nhiên, điều đó là không thực tế vì chúng ta không thể có nhiều dữ liệu training khác nhau, dữ liệu là hữu hạn. Vì vậy, chúng ta sẽ sử dụng boostrap, tức phương pháp lấy mẫu ngẫu nhiên có hoàn lại từ tập dữ liệu training duy nhất của chúng ta. Theo đó, chúng ta sẽ tạo ra B boostrapped dữ liệu training khác nhau để có được các giá trị dự báo khác nhau, và sau đó sẽ lấy trung bình tất cả các kết quả dự báo, thu được kết quả cuối cùng: \\[\\widehat{f}_{bag}(x) = \\frac{1}{B}\\sum_{b=1}^{B}\\widehat{f^{*}}^{b}(x)\\] Đây gọi là bagging. Bagging có thể cải thiện chất lượng dự báo cho rất nhiều các mô hình hồi quy, đặc biệt hiệu quả đối với mô hình cây quyết định. Để áp dụng bagging với regression trees, chúng ta đơn giản chỉ cần xây dựng B regression trees sử dụng B boostrapped dữ liệu training, sau đó lấy trung bình các kết quả dự báo. Những cây quyết định này được xây dựng rất sâu (nhiều tầng) và không được “tỉa”. Vì thế mỗi cây quyết định trên sẽ có variance cao, nhưng bias thấp. Lấy trung bình kết quả B cây quyết định này sẽ làm giảm variance. Bagging có thể cải thiện chất lượng dự báo một cách đáng kể khi kết hợp hàng trăm hoặc thậm chí hàng nghìn cây quyết định lại với nhau. Cho đến thời điểm hiện tại, chúng ta đã mô tả phương pháp bagging đối với regression trees, tức dự báo biến đầu ra là biến liên tục. Vậy phương pháp bagging có thể sử dụng với bài toán mà biến đầu ra là biến rời rạc không? Trong trường hợp này, giả sử khi chúng ta muốn phân loại một quan sát mới, chúng ta có thể dự báo được quan sát mới trên thuộc class nào trong B cây quyết định khác nhau, rồi sau đó, lấy majority vote - tức là, quan sát mới trên sẽ rơi vào class mà tần suất xuất hiện của nó nhiều nhất trong B kết quả dự báo khác nhau. Trong phương pháp bagging, tham số về số lượng cây quyết định B nói trên mà càng lớn thì cũng không thể dẫn đến overfitting. Trong thực tế, chúng ta sẽ chọn số lượng cây quyết định đủ lớn để sao cho sai số đủ nhỏ. Out-of-Bag Error Estimation Có một cách khá đơn giản để ước lượng sai số dự báo (test error) của mô hình bagging mà không cần dùng cross-validation hoặc tập dữ liệu validation. Nhắc lại key của phương pháp bagging là việc các cây quyết định sẽ được xây dựng nhiều lần sử dụng những tập dữ liệu bootstrapped khác nhau. Mỗi một cây quyết định được xây đều sử dụng khoảng 2/3 số quan sát, còn 1/3 quan sát còn lại không được sử dụng trong quá trình xây dựng mô hình sẽ được gọi là những quan sát out-of-bag (OOB). Chúng ta có thể dự báo kết quả cho quan sát thứ i sử dụng từng cây quyết định mà các quan sát là OOB. Điều này sẽ mang lại khoảng B/3 giá trị dự báo cho quan sát thứ i này. Để dự báo giá trị cuối cùng của quan sát đó, chúng ta sẽ lấy trung bình các kết quả dự báo (đối với bài toán hồi quy - regression) hoặc lấy theo số đông - majority vote (đối với bài toán phân loại - classification). Đó chính là kết quả dự báo OOB (OOB prediction) cho quan sát i. OOB prediction có thể sử dụng cho từng n quan sát, và có thể tính toán được OOB MSE (đối với bài toán hồi quy) hoặc classification error (đối với bài toán phân loại). Cách sử dụng OOB để ước lượng sai số dự báo (test error) sẽ thuận tiện hơn so với cách cross-validation khi sử dụng bagging đối với tập dữ liệu lớn. Variable Importance Measures Như vừa tìm hiểu, bagging sẽ cải thiện được độ chính xác của dự báo so với mô hình cây quyết định đơn lẻ. Tuy nhiên, bagging lại rất khó để giải thích kết quả mô hình do việc tổng hợp rất nhiều cây quyết định khác nhau. Mặc dù đối với bagging rất khó để giải thích kết quả mô hình, nhưng chúng ta vẫn có thể xem được thống kê tổng quát về mức độ quan trọng của các biến đầu vào trong mô hình bằng việc sử dụng RSS (đối với bagging regression trees) hoặc chỉ số Gini (đối với bagging classification trees). 13.2 Random Forests Cũng giống như bagging, random forests cũng xây dựng một tập hợp các cây quyết định sử dụng các tập dữ liệu con được chia theo phương pháp boostrap (lấy mẫu ngẫu nhiên có hoàn lại) từ tập dữ liệu training ban đầu. Tuy nhiên, với phương pháp random forests thì những tập dữ liệu con đó sẽ không bao gồm tất cả các biến đầu vào (p - tổng số lượng biến đầu vào) trong tập dữ liệu training ban đầu như bagging mà chỉ bao gồm m biến nhất định (thông thường m ~ sqrt(p)). Đối với bagging các cây quyết định có thể tương quan chặt chẽ với nhau (highly correlated) do các cây đều lấy cùng một số lượng là tất cả các biến đầu vào trong tập dữ liệu training ban đầu. Điều đó sẽ dẫn đến việc variance sẽ cao. Trong khi đó, random forests có thể khắc phục được vấn đề trên khi mỗi cây quyết định được xây dựng chỉ lấy ngẫu nhiên m biến đầu vào ngẫu nhiên. Quá trình đó được gọi là decorrelating. Quá trình này sẽ giúp kết quả dự báo đáng tin cậy hơn. Như vậy, điểm khác biệt quan trọng nhất giữa bagging và random forests là việc chọn số lượng biến đầu vào: bagging lấy tất cả các biến đầu vào (p) random forests lấy m biến nhất định (m ~ sqrt(p)) Vì thế, nếu xây dựng mô hình random forests với số lượng biến đầu vào m = p (tức lấy tất cả các biến đầu vào) thì mô hình trở thành bagging. Do đó, có thể nói bagging là một trường hợp đặc biệt của random forests. Thuật toán của Random Forest sẽ bao gồm việc lấy mẫu và chọn biến để xây dựng một số lượng lớn các cây quyết định khác nhau. Kết quả dự báo cuối cùng của một quan sát sẽ lấy trung bình các kết quả dự báo của các cây quyết định (đối với bài toán hồi quy - regression) hoặc lấy majority vote từ các kết quả dự báo từ các cây quyết định để xác định quan sát đó thuộc class nào (đối với bài toán phân loại - classification). Giả định rằng N là số lượng quan sát của tập dữ liệu training và p là số lượng biến đầu vào. Thuật toán sẽ diễn ra theo các bước như sau: Bước 1: Xây dựng số lượng lớn các cây quyết định bằng việc lấy mẫu ngẫu nhiên có hoàn lại N quan sát từ tập dữ liệu training Bước 2: Với mỗi cây quyết định lựa chọn m &lt; p biến nhất định. Những biến này được cân nhắc lựa chọn để phân nhánh, với mỗi cây quyết định thì đều có số lượng biến là m. Bước 3: Xây dựng các cây (không thực hiện tỉa cây) Bước 4: Dự báo các quan sát mới bằng việc lấy trung bình các kết quả dự báo của các cây quyết định khác nhau đã được xây dựng (đối với bài toán hồi quy), hoặc lấy theo kết quả số đông của các cây quyết định khác nhau đã được xây dựng (đối với bài toán phân loại). Out-of-bag (OOB) error estimate được tính toán bằng việc phân loại quan sát mới mà không có trong tập dữ liệu training khi xây dựng cây quyết định. Việc này sẽ hữu ích khi mà chúng ta không có dữ liệu validation. Trong R để xây dựng mô hình random forests chúng ta có thể sử dụng hàm randomForest() trong package randomForest. Số lượng cây mặc định là 500, số lượng biến tại mỗi cây mặc định là sqrt(tổng số biến), và kích thước nhỏ nhất của cây mặc định là 1. # Data loc &lt;- &quot;http://archive.ics.uci.edu/ml/machine-learning-databases/&quot; ds &lt;- &quot;breast-cancer-wisconsin/breast-cancer-wisconsin.data&quot; url &lt;- paste(loc, ds, sep=&quot;&quot;) breast &lt;- read.table(url, sep=&quot;,&quot;, header=FALSE, na.strings=&quot;?&quot;) names(breast) &lt;- c(&quot;ID&quot;, &quot;clumpThickness&quot;, &quot;sizeUniformity&quot;, &quot;shapeUniformity&quot;, &quot;maginalAdhesion&quot;, &quot;singleEpithelialCellSize&quot;, &quot;bareNuclei&quot;, &quot;blandChromatin&quot;, &quot;normalNucleoli&quot;, &quot;mitosis&quot;, &quot;class&quot;) df &lt;- breast[-1] df$class &lt;- factor(df$class, levels=c(2,4), labels=c(&quot;benign&quot;, &quot;malignant&quot;)) set.seed(1234) train &lt;- sample(nrow(df), 0.7*nrow(df)) df.train &lt;- df[train,] df.validate &lt;- df[-train,] library(ISLR) library(tidyverse) ## Loading tidyverse: ggplot2 ## Loading tidyverse: tibble ## Loading tidyverse: tidyr ## Loading tidyverse: readr ## Loading tidyverse: purrr ## Loading tidyverse: dplyr ## Warning: package &#39;ggplot2&#39; was built under R version 3.4.4 ## Warning: package &#39;tibble&#39; was built under R version 3.4.4 ## Warning: package &#39;tidyr&#39; was built under R version 3.4.4 ## Warning: package &#39;readr&#39; was built under R version 3.4.4 ## Warning: package &#39;purrr&#39; was built under R version 3.4.4 ## Warning: package &#39;dplyr&#39; was built under R version 3.4.4 ## Conflicts with tidy packages ---------------------------------------------- ## filter(): dplyr, stats ## lag(): dplyr, stats library(randomForest) # Package sử dụng cho Random Forests ## randomForest 4.6-12 ## Type rfNews() to see new features/changes/bug fixes. ## ## Attaching package: &#39;randomForest&#39; ## The following object is masked from &#39;package:dplyr&#39;: ## ## combine ## The following object is masked from &#39;package:ggplot2&#39;: ## ## margin set.seed(1234) # Xây dựng mô hình trên tập training fit_rf &lt;- randomForest(class ~ ., data = df.train, na.action = na.roughfix, # Xử lý giá trị missing importance = T ) fit_rf ## ## Call: ## randomForest(formula = class ~ ., data = df.train, importance = T, na.action = na.roughfix) ## Type of random forest: classification ## Number of trees: 500 ## No. of variables tried at each split: 3 ## ## OOB estimate of error rate: 3.68% ## Confusion matrix: ## benign malignant class.error ## benign 319 10 0.03039514 ## malignant 8 152 0.05000000 Để xem mức độ quan trọng của các biến trong mô hình chúng ta có thể sử dụng hàm importance(). # Xem mức độ quan trọng của các biến trong mô hình importance(fit_rf, type = 2 ) ## MeanDecreaseGini ## clumpThickness 12.504484 ## sizeUniformity 54.770143 ## shapeUniformity 48.662325 ## maginalAdhesion 5.969580 ## singleEpithelialCellSize 14.297239 ## bareNuclei 34.017599 ## blandChromatin 16.243253 ## normalNucleoli 26.337646 ## mitosis 1.814502 MeanDecreaseGini - tổng độ giảm tính không đồng nhất (heterogeneity), hay nói cách khác là tăng tính đồng nhất từ việc chọn biến đó để phân nhánh, tính trung bình trên tất cả các cây. MeanDecreaseGini của biến nào càng cao, thì biến đó càng quan trọng trong mô hình. Kết quả trên cho chúng ta thấy rằng biến sizeUniformity là quan trọng nhất, trong khi đó mitosis là biến ít quan trọng nhất. Sau khi xây dựng được mô hình, để phân loại những quan sát mới từ tập dữ liệu kiểm tra (validation sample), chúng ta sử dụng hàm predict() # Dự báo những quan sát mới rf_pred &lt;- predict(fit_rf, df.validate ) # Đánh giá chất lượng mô hình trên tập validation rf_perf &lt;- table(df.validate$class, rf_pred, dnn = c(&quot;Actual&quot;, &quot;Predicted&quot;) ) rf_perf ## Predicted ## Actual benign malignant ## benign 117 3 ## malignant 1 79 Kết quả dự báo cho ta thấy rằng đối với những quan sát bị missing, mô hình sẽ không phân loại. Tỷ lệ quan sát được dự báo đúng là: (117+79)/200 = 98% (tập dữ liệu kiểm tra có 210 quan sát, nhưng có 10 quan sát bị missing nên mô hình chỉ phân loại 200 quan sát). Ưu điểm của Random Forests là chất lượng dự báo tốt, độ chính xác cao hơn so với những phương pháp như Logistic hay Decision Trees. Hơn nữa, phương pháp này có thể xử lý tốt với những trường hợp dữ liệu lớn (nhiều quan sát, nhiều biến) hoặc dữ liệu bị missing. Ngoài ra, việc tính toán được OOB error và xem xét được mức độ quan trọng của các biến đến kết quả dự báo (variable importance) cũng là một ưu điểm. Tuy nhiên, nhược điểm của Random Forests là rất khó giải thích, vì trong quá trình xây dựng mô hình theo phương pháp này, chúng ta sẽ xây dựng rất nhiều các cây quyết định đơn lẻ khác nhau. Điều đó thể hiện sự đánh đổi giữa chất lượng dự báo và tính giải thích của mô hình. 13.3 Boosting Bây giờ chúng ta sẽ cùng thảo luận về boosting, một phương pháp khác để cải thiện chất lượng dự báo từ việc sử dụng cây quyết định. Giống như các phương pháp trước, boosting có thể áp dụng được đối với cả 2 bài toán: Hồi quy (regression) và phân loại (classification). Nhắc lại một chút, boosting và random forests sử dụng phương pháp boostrap (lấy mẫu ngẫu nhiên có hoàn lại) để tạo các tập dữ liệu con từ dữ liệu training ban đầu, sau đó xây dựng các cây quyết định đối với từng tập dữ liệu con đó (các cây quyết định được xây dựng độc lập với nhau). Cuối cùng, sẽ tổng hợp lại các cây quyết định để ra được kết quả dự báo cuối cùng bằng cách lấy trung bình các kết quả dự báo của các cây quyết định (đối với bài toán hồi quy), hoặc lấy theo số đông các kết quả dự báo của các cây quyết định (đối với bài toán phân loại). Boosting cũng hoạt động theo cách tương tự, nhưng khác ở chỗ là việc xây dựng các cây quyết định từ những tập dữ liệu con khác nhau không phải là độc lập hoàn toàn với nhau như bagging hay random forests. Thay vào đó, boosting xây dựng các cây quyết định một cách có trình tự (sequentially): Mỗi cây kế tiếp được xây dựng bằng cách sử dụng kết quả từ những cây trước đó. Boosting tập trung nhiều hơn vào những quan sát bị dự báo sai từ những cây trước để góp phần cải thiện kết quả dự báo cuối cùng. Boosting không dùng boostrap để chia tập dữ liệu training ban đầu, mà thay vào đó là việc dùng các phiên bản đã được modified từ tập dữ liệu train ban đầu để xây dựng các cây quyết định. Chúng ta sẽ cùng xem xét bài toán hồi quy. Giống như bagging và randomforests, boosting kết hợp nhiều cây quyết định lại với nhau: f1,…, fB. Thuật toán boosting đối với bài toán hồi quy: Bước 1: Đặt fˆ(x) = 0 và ri = yi với mọi i trên tập dữ liệu training Bước 2: Với b = 1,2.., B, lặp lại: Xây dựng một cây fˆb với d splits (d+1 terminal nodes) với dữ liệu training (X,r). Update fˆ bằng việc adding in a shrunken version of the new tree, và update residuals: \\[\\widehat{f}(x) = \\widehat{f}(x) + λ\\widehat{f}^{b}(x)\\] \\[r_i = r_i - λ\\widehat{f}^{b}(x_i)\\] Bước 3: Kết quả của boosted model: \\[\\widehat{f}(x) = \\sum_{b=1}^{P}λ\\widehat{f}^{b}(x)\\] Thay vì việc xây dựng các cây quyết định đơn lẻ với kích thước lớn có thể dẫn đến vấn đề overfitting, phương pháp boosting sẽ “học” chậm (learn slowly). Khi xây dựng xong cây quyết định đầu tiên, chúng ta sẽ xây dựng cây quyết định tiếp theo sử dụng biến đầu ra là phần dư (residuals) của cây trước đó. Sau đó, sẽ xây dựng các cây quyết định tiếp theo để update residuals. Mỗi cây có thể có kích thước nhỏ, với chỉ một vài terminal nodes được quyết định bởi tham số d trong thuật toán. Bằng việc xây dựng những cây nhỏ với residuals, chúng ta có thể dần dần cải thiện fˆ. Tham số shrinkage hay learning rate(tốc độ học của mô hình) λ sẽ làm mô hình “học” chậm và kỹ hơn nữa giúp cải thiện chất lượng mô hình. Lưu ý rằng khác với bagging, đối với boosting thì việc xây dựng các cây quyết định tiếp theo sẽ phụ thuộc vào kết quả của các cây trước đó. Như vậy, chúng ta đã vừa cùng tìm hiểu về boosting regression trees. Bây giờ chúng sẽ cùng tìm hiểu về 2 thuật toán trong boosting là Adaboost và Gradient Boosting. AdaBoost AdaBoost kết hợp các “weak learners” để tạo thành “strong learner” (“weak learners” được hiểu là các cây phân loại chỉ tốt hơn một chút so với việc đoán ngẫu nhiên). Sau mỗi bước lặp, những quan sát bị phân loại sai sẽ được đánh trọng số cao hơn, những quan sát được phân loại đúng sẽ đánh trọng số thấp hơn. Mỗi cây tiếp theo được xây dựng với mục tiêu phân loại đúng những quan sát đã bị phân loại sai ở cây trước đó. Chúng ta sẽ mô tả thuật toán Adaboost thông qua việc sử dụng ví dụ sau đây: Phân loại các quan sát vào 2 nhóm + hoặc -. Chúng ta sẽ thực hiện các bước sau: Diễn giải: Box 1: Đánh trọng số bằng nhau đối với tất cả quan sát và xây dựng một decision stump - D1 (cây chỉ gồm 1 split hay 1 tầng) để phân loại các quan sát thành 2 nhóm + và -. Kết quả cho thấy có 3 quan sát bị phân loại sai (là + nhưng lại bị cho vào nhóm -), 3 quan sát này sẽ được đánh trọng số cao hơn và tiếp tục xây dựng decision stump khác - D2. Box 2: D2 được xây dựng với mục đích phân loại đúng 3 quan sát bị phân loại sai ở D1. Kết quả cho thấy, lại có 3 quan sát bị phân loại sai (là - nhưng bị cho vào nhóm +). Lại tiếp tục đánh trọng số cao hơn đối với những quan sát này và tiếp tục xây dựng decision stump - D3. Box 3: D3 được xây dựng với mục đích phân loại đúng 3 quan sát bị phân loại sai ở D2. Kết quả cho thấy vẫn có những quan sát bị phân loại sai. Box 4: Kết hợp D1, D2, D3 để tạo thành D4 - phân loại tốt hơn so với D1, D2, D3 (nhóm + và - đã được phân loại hoàn toàn). Gradient Boosting Gradient Boosting = Gradient Descent + Boosting Cả AdaBoost và Gradient Boosting đều kết hợp các “weak learners” để tạo thành một “strong learner” và đều tập trung vào những quan sát bị dự báo sai. AdaBoost thì đánh trong số cao hơn vào những quan sát bị dự báo sai tại mỗi cây trước, và cố gắng dự báo đúng những quan sát đó tại cây tiếp theo. Trong khi đó, với Gradient Boosting, mỗi một cây mới sẽ được xây dựng với mục tiêu tối thiểu hóa dần tổng loss của cây trước đó bằng việc sử dụng phương pháp gradient descent. Trong Gradient Boosting, việc tính tổng loss dựa vào việc lựa chọn loại “loss function” nào, ví dụ như: square loss, absolute loss, huber loss. Mỗi loại đều có những ưu/nhược điểm riêng. Square loss: \\[L(y,F) = (y-F)^2/2\\] Absolute loss: \\[L(y,F) = |y-F|\\] Huber loss: \\[\\begin{cases}(y-F)^2/2 &amp; |x-F| \\leqslant \\delta\\\\\\delta(|y-F| - \\delta/2) &amp; |x-F| &gt; \\delta\\end{cases}\\] Ví dụ: Boosting có 3 tham số cơ bản để tối ưu hóa mô hình (tuning parameters): Số lượng cây quyết định (B): Với boosting khi số lượng cây quá nhiều có thể dẫn đến overfitting, nên chúng ta sẽ sử dụng cross-validation để lựa chọn số lượng cây Tốc độ học λ (learning rate hoặc shrinkage): Giá trị nhỏ, dương. λ có thể nhận các giá trị như: 0.1, 0.01, hay 0.001 tùy từng trường hợp. λ càng nhỏ thì mô hình sẽ “học” càng chậm, càng lâu. Số lần splits, hay phân nhánh (d) của mỗi cây: Tham số này dùng để kiểm soát độ phức tạp của mô hình. Tham số này còn có thể gọi là số tầng cây (interactive depth). Nếu d = 1 (tức cây chỉ có 1 tầng hay 1 split) thì cây quyết định đó được gọi là stump. Để thực hành xây dựng mô hình boosting trên R, chúng ta sẽ sử dụng dữ liệu có sẵn trong R - GermanCredit. Đây là dữ liệu ghi nhận về lịch sử vay của khách hàng, với các 61 biến đầu vào cùng với biến đầu ra Class ghi nhận thực tế là các khoản vay đó có phải là khoản nợ xấu hay không. Để xây dựng mô hình boosting trên R, chúng ta sẽ sử dụng hàm gbm() trong package gbm. rm(list = ls()) library(caret) ## Loading required package: lattice ## ## Attaching package: &#39;caret&#39; ## The following object is masked from &#39;package:purrr&#39;: ## ## lift data(&quot;GermanCredit&quot;) data &lt;- GermanCredit rm(GermanCredit) # Hàm tính toán các chỉ số đo lường chất lượng dự báo của mô hình model.performance &lt;- function(confusion_matrix) { a &lt;- confusion_matrix[1,1] b &lt;- confusion_matrix[1,2] c &lt;- confusion_matrix[2,2] d &lt;- confusion_matrix[2,1] tpr &lt;- c/(b+c) precision &lt;- c/(c+d) accuracy &lt;- (a+c)/(a+b+c+d) print(paste(&#39;recall :&#39;,round(tpr,2))) print(paste(&#39;precision :&#39;, round(precision,2))) print(paste(&#39;accuracy :&#39;,round(accuracy,2))) } # Chia data: training/testing tỷ lệ 8/2 set.seed(123) indxTrain &lt;- createDataPartition(y = data$Class,p = 8/10,list = FALSE) training &lt;- data[indxTrain,] testing &lt;- data[-indxTrain,] df.train &lt;- training df.train$Status[df.train$Class == &quot;Good&quot;] &lt;- 1 df.train$Status[df.train$Class == &quot;Bad&quot;] &lt;- 0 df.test &lt;- testing df.test$Status[df.test$Class == &quot;Good&quot;] &lt;- 1 df.test$Status[df.test$Class == &quot;Bad&quot;] &lt;- 0 rm(training,testing) # Gradient Boosting set.seed(9999) # Xây dựng mô hình trên tập train library(gbm) ## Loading required package: survival ## ## Attaching package: &#39;survival&#39; ## The following object is masked from &#39;package:caret&#39;: ## ## cluster ## Loading required package: splines ## Loading required package: parallel ## Loaded gbm 2.1.1 gbm.train &lt;- gbm(Status ~ . - Class, data = df.train, distribution = &quot;bernoulli&quot;, n.trees = 1000, shrinkage = 0.01, interaction.depth = 4) ## Warning in gbm.fit(x, y, offset = offset, distribution = distribution, w = ## w, : variable 26: Purpose.Vacation has no variation. ## Warning in gbm.fit(x, y, offset = offset, distribution = distribution, w = ## w, : variable 44: Personal.Female.Single has no variation. # Dự báo quan sát trên tập test gbm.result &lt;- predict(gbm.train, newdata = df.test, n.trees = 1000, type = &quot;response&quot;) # Confusion matrix gbm.conf &lt;- rep (&quot;Bad&quot;, 200) gbm.conf[gbm.result &gt; 0.5] = &quot;Good&quot; gbm.confusion &lt;- table(gbm.conf, df.test$Class) gbm.confusion ## ## gbm.conf Bad Good ## Bad 35 16 ## Good 25 124 model.performance(gbm.confusion) ## [1] &quot;recall : 0.89&quot; ## [1] &quot;precision : 0.83&quot; ## [1] &quot;accuracy : 0.8&quot; 35+124+25+16 ## [1] 200 Kết quả confusion matrix trên tập dữ liệu testing cho chúng ta thấy: Tỷ lệ dự báo đúng trên tổng quan sát là 80% (tức 35 khách hàng có nợ xấu và 124 khách hàng không có nợ xấu được dự báo đúng trên tổng số 200 khách hàng trên tập dữ liệu testing) Trong số 140 khách hàng thực tế không có nợ xấu thì chúng ta dự báo chính xác 124 khách hàng (tỷ lệ 124/140 = 89%) Trong số 149 khách hàng mà chúng ta dự báo là không có nợ xấu, có 124 khách hàng được dự báo chính xác (tỷ lệ 124/149 = 83%) # ROC library(ROCR) # Dùng để vẽ đường ROC và tính toán AUC ## Loading required package: gplots ## ## Attaching package: &#39;gplots&#39; ## The following object is masked from &#39;package:stats&#39;: ## ## lowess gbm.ROC &lt;- prediction(gbm.result, df.test$Class) gbm.ROCperf_test &lt;- performance(gbm.ROC, &quot;tpr&quot;, &quot;fpr&quot;) # Vẽ ROC plot(gbm.ROCperf_test) # Tính toán AUC gbm.auc_test &lt;- performance(gbm.ROC, &quot;auc&quot;, &quot;cutoff&quot;) gbm.auc_test@y.values ## [[1]] ## [1] 0.8171429 Kết quả AUC ~ 82% trên tập testing cho thấy chất lượng dự báo của mô hình là tương đối tốt. "],
["unsupervised-learning.html", "Chương 14 Unsupervised learning", " Chương 14 Unsupervised learning Khác với bài toán phân loại(classification), khi ta có 1 tập dữ liệu với giá trị phân loại của từng quan sát \\((x^{(i)}, y^{(i)})\\) và mục tiêu là xây dựng hàm dự báo giúp phân loại \\(y^{(i)}\\), bài toán unsupersived learning không có trước hàm mục tiêu. Trong bài toán này, từ tập dữ liệu cho trước, ta phải tìm ra cấu trúc dữ liệu tồn tại trong dữ liệu. Bài toán này rất hữu dụng trong việc phân nhóm khách hàng, phân tích quan hệ giữa các quan sát. "],
["k-means.html", "Chương 15 k-means 15.1 Giới thiệu 15.2 Thuật toán 15.3 Ưu nhược điểm của K-means 15.4 Thực hành trên R", " Chương 15 k-means 15.1 Giới thiệu Trong chương này, chúng ta sẽ cùng tìm hiểu về thuật toán clustering K-means - một thuật toán thuộc về nhóm unsupervised learning (“học không giám sát”). Clustering K-means có thể ứng dụng trong các lĩnh vực như ngân hàng hay thương mại điện tử vào việc phân nhóm khách hàng dựa trên những đặc điểm về hành vi, thói quen chi tiêu và demographics của họ. Điều đó sẽ giúp cho ngân hàng hay các công ty thương mại điện tử có thể hiểu rõ được hành vi khách hàng của mình, để sau đó có thể offer những sản phẩm, dịch vụ phù hợp với nhu cầu của từng khách hàng. Thuật toán K-means sẽ nhóm các đối tượng (quan sát) có những đặc điểm và hành vi tương đồng vào K nhóm bất kỳ. Số lượng nhóm K sẽ phụ thuộc vào mục đích cũng như bối cảnh mà chúng ta đặt ra (thông thường nên từ 2 đến 10 nhóm). Ví dụ, sử dụng dữ liệu iris có sẵn trong R chúng ta sẽ phân nhóm (cluster) các quan sát thành 2 nhóm dựa vào biến Sepal.Length và Petal.length. library(dplyr) # Lấy ngẫu nhiên 100 quan sát, # và chỉ lấy 2 biến `Sepal.Length` và `Petal.Length` set.seed(1) data &lt;- iris %&gt;% select(Sepal.Length, Petal.Length) %&gt;% sample_n(100) library(ggplot2) data %&gt;% ggplot(aes(Sepal.Length, Petal.Length)) + geom_point(size = 2, col = &quot;darkgreen&quot;)+ theme_classic() Bước 1: Chúng ta sẽ lựa chọn ngẫu nhiên 2 tâm (cluster centroids) - tương đương với 2 nhóm, là 2 điểm x (đỏ) và x (xanh dương) như sau: data %&gt;% ggplot(aes(Sepal.Length, Petal.Length)) + geom_point(size = 2, col = &quot;darkgreen&quot;)+ theme_classic()+ geom_point(aes(x=6,y=5), col = &quot;red&quot;, shape=4, size = 5)+ geom_point(aes(x=5,y=1), col = &quot;blue&quot;, shape=4, size = 5) Bước 2: Quan sát nào mà “gần” hơn với điểm x (đỏ) thì chúng ta sẽ nhóm quan sát đó vào một nhóm màu đỏ, còn những quan sát nào mà “gần” hơn với điểm x (xanh dương), sẽ thuộc về nhóm màu xanh dương. Ở đây “gần” hơn được hiểu là khoảng cách từ quan sát đó đến điểm x nào nhỏ hơn. Khoảng cách giữa 2 điểm có tọa độ (x1, y1) và (x2, y2) được tính như sau: \\[d = \\sqrt{(x_2 - x_1)^2 + (y_2 - y_1)^2}\\] data1 &lt;- data %&gt;% # Set ngẫu nhiên 2 centroids mutate(Sepal.Length_red = 6, Petal.Length_red = 5, Sepal.Length_blue = 5, Petal.Length_blue = 1 ) %&gt;% # Tính khoảng cách từ các quan sát đến 2 centroids mutate( distance_red = sqrt((Sepal.Length-Sepal.Length_red)^2 + (Petal.Length - Petal.Length_red)^2), distance_blue = sqrt((Sepal.Length-Sepal.Length_blue)^2 + (Petal.Length - Petal.Length_blue)^2) ) %&gt;% # Tạo biến mới - nhóm mà các quan sát được phân vào mutate(group = case_when( distance_red &lt; distance_blue ~ &quot;red&quot;, TRUE ~ &quot;blue&quot; ) %&gt;% as.factor) # Plot dữ liệu mới data1 %&gt;% ggplot(aes(Sepal.Length, Petal.Length), group = group) + geom_point(size = 2, aes(col = group))+ theme_classic()+ geom_point(aes(x=6,y=5), col = &quot;red&quot;, shape=4, size = 5)+ geom_point(aes(x=5,y=1), col = &quot;blue&quot;, shape=4, size = 5)+ scale_color_manual( values = c(&quot;blue&quot; = &quot;blue&quot;, &quot;red&quot; = &quot;red&quot;) )+ theme(legend.position = &quot;none&quot;) Bước 3: Tiếp theo, chúng ta sẽ di chuyển 2 tâm x (đỏ) và x (xanh dương) (move centroids) đến điểm mới có tọa độ bằng trung bình tọa độ các quan sát thuộc về 2 nhóm nói trên. # Tính tọa độ của 2 centroids mới new_centroids_1 &lt;- data1 %&gt;% group_by(group) %&gt;% summarise(average_sepal = mean(Sepal.Length), average_petal = mean(Petal.Length) ) %&gt;% as.data.frame # Plot di chuyển centroids data1 %&gt;% ggplot(aes(Sepal.Length, Petal.Length), group = group) + geom_point(size = 2, aes(col = group))+ theme_classic()+ geom_point(aes(x=6,y=5), col = &quot;red&quot;, shape=4, size = 5)+ geom_point(aes(x=5,y=1), col = &quot;blue&quot;, shape=4, size = 5)+ scale_color_manual( values = c(&quot;blue&quot; = &quot;blue&quot;, &quot;red&quot; = &quot;red&quot;) )+ theme(legend.position = &quot;none&quot;)+ geom_point(aes(x=new_centroids_1$average_sepal[new_centroids_1$group == &quot;red&quot;], y=new_centroids_1$average_petal[new_centroids_1$group == &quot;red&quot;] ), col = &quot;red&quot;, shape=4, size = 5)+ geom_point(aes(x=new_centroids_1$average_sepal[new_centroids_1$group == &quot;blue&quot;], y=new_centroids_1$average_petal[new_centroids_1$group == &quot;blue&quot;] ), col = &quot;blue&quot;, shape=4, size = 5)+ geom_segment(aes(x = 6, y = 5, xend = new_centroids_1$average_sepal[new_centroids_1$group == &quot;red&quot;], yend = new_centroids_1$average_petal[new_centroids_1$group == &quot;red&quot;] ), col = &quot;black&quot;, arrow = arrow(length = unit(0.015, &quot;npc&quot;)), size = 0.1 )+ geom_segment(aes(x = 5, y = 1, xend = new_centroids_1$average_sepal[new_centroids_1$group == &quot;blue&quot;], yend = new_centroids_1$average_petal[new_centroids_1$group == &quot;blue&quot;] ), col = &quot;black&quot;, arrow = arrow(length = unit(0.015, &quot;npc&quot;)), size = 0.1 ) Bước 4: Bây giờ, chúng ta sẽ lặp lại quá trình đã làm, tức quan sát gần hơn với điểm centroids mới nào thì sẽ thuộc về nhóm tương ứng. Kết quả như sau: data2 &lt;- data %&gt;% # Update 2 centroids mới mutate(Sepal.Length_red = new_centroids_1$average_sepal[new_centroids_1$group == &quot;red&quot;], Petal.Length_red = new_centroids_1$average_petal[new_centroids_1$group == &quot;red&quot;], Sepal.Length_blue = new_centroids_1$average_sepal[new_centroids_1$group == &quot;blue&quot;], Petal.Length_blue = new_centroids_1$average_petal[new_centroids_1$group == &quot;blue&quot;] ) %&gt;% # Tính khoảng cách từ các quan sát đến 2 centroids mutate( distance_red = sqrt((Sepal.Length-Sepal.Length_red)^2 + (Petal.Length - Petal.Length_red)^2), distance_blue = sqrt((Sepal.Length-Sepal.Length_blue)^2 + (Petal.Length - Petal.Length_blue)^2) ) %&gt;% # Tạo biến mới - nhóm mà các quan sát được phân vào mutate(group = case_when( distance_red &lt; distance_blue ~ &quot;red&quot;, TRUE ~ &quot;blue&quot; ) %&gt;% as.factor) # Plot dữ liệu mới data2 %&gt;% ggplot(aes(Sepal.Length, Petal.Length), group = group) + geom_point(size = 2, aes(col = group))+ theme_classic()+ scale_color_manual( values = c(&quot;blue&quot; = &quot;blue&quot;, &quot;red&quot; = &quot;red&quot;) )+ theme(legend.position = &quot;none&quot;)+ geom_point(aes(x=new_centroids_1$average_sepal[new_centroids_1$group == &quot;red&quot;], y=new_centroids_1$average_petal[new_centroids_1$group == &quot;red&quot;] ), col = &quot;red&quot;, shape=4, size = 5)+ geom_point(aes(x=new_centroids_1$average_sepal[new_centroids_1$group == &quot;blue&quot;], y=new_centroids_1$average_petal[new_centroids_1$group == &quot;blue&quot;] ), col = &quot;blue&quot;, shape=4, size = 5)+ annotate(&quot;text&quot;, x = 4.9, y = 3.3, col = &quot;black&quot;, size = 4, hjust = 1, label = &quot;This point: \\n red -&gt; blue&quot; ) Như vậy, với 2 centroids mới việc phân nhóm quan sát có sự thay đổi: Có 1 điểm lúc trước thuộc về nhóm đỏ, nhưng giờ thuộc về nhóm xanh dương. Bước 5: Tiếp tục quá trình như trên, chúng ta sẽ lại di chuyển các centroids để tìm các centroids mới. # Tính tọa độ của 2 centroids mới new_centroids_2 &lt;- data2 %&gt;% group_by(group) %&gt;% summarise(average_sepal = mean(Sepal.Length), average_petal = mean(Petal.Length) ) %&gt;% as.data.frame # Plot di chuyển centroids data2 %&gt;% ggplot(aes(Sepal.Length, Petal.Length), group = group) + geom_point(size = 2, aes(col = group))+ theme_classic()+ scale_color_manual( values = c(&quot;blue&quot; = &quot;blue&quot;, &quot;red&quot; = &quot;red&quot;) )+ theme(legend.position = &quot;none&quot;)+ geom_point(aes(x=new_centroids_1$average_sepal[new_centroids_1$group == &quot;red&quot;], y=new_centroids_1$average_petal[new_centroids_1$group == &quot;red&quot;] ), col = &quot;red&quot;, shape=4, size = 5)+ geom_point(aes(x=new_centroids_1$average_sepal[new_centroids_1$group == &quot;blue&quot;], y=new_centroids_1$average_petal[new_centroids_1$group == &quot;blue&quot;] ), col = &quot;blue&quot;, shape=4, size = 5)+ geom_point(aes(x=new_centroids_2$average_sepal[new_centroids_2$group == &quot;red&quot;], y=new_centroids_2$average_petal[new_centroids_2$group == &quot;red&quot;] ), col = &quot;red&quot;, shape=4, size = 5)+ geom_point(aes(x=new_centroids_2$average_sepal[new_centroids_2$group == &quot;blue&quot;], y=new_centroids_2$average_petal[new_centroids_2$group == &quot;blue&quot;] ), col = &quot;blue&quot;, shape=4, size = 5)+ geom_segment(aes(x = new_centroids_1$average_sepal[new_centroids_1$group == &quot;red&quot;], y = new_centroids_1$average_petal[new_centroids_1$group == &quot;red&quot;], xend = new_centroids_2$average_sepal[new_centroids_2$group == &quot;red&quot;], yend = new_centroids_2$average_petal[new_centroids_2$group == &quot;red&quot;] ), col = &quot;black&quot;, arrow = arrow(length = unit(0.015, &quot;npc&quot;)), size = 0.1 )+ geom_segment(aes(x = new_centroids_1$average_sepal[new_centroids_1$group == &quot;blue&quot;], y = new_centroids_1$average_petal[new_centroids_1$group == &quot;blue&quot;], xend = new_centroids_2$average_sepal[new_centroids_2$group == &quot;blue&quot;], yend = new_centroids_2$average_petal[new_centroids_2$group == &quot;blue&quot;] ), col = &quot;black&quot;, arrow = arrow(length = unit(0.015, &quot;npc&quot;)), size = 0.1 ) Bước 6: Phân nhóm lại theo các centroids mới data3 &lt;- data %&gt;% # Update 2 centroids mới mutate(Sepal.Length_red = new_centroids_2$average_sepal[new_centroids_2$group == &quot;red&quot;], Petal.Length_red = new_centroids_2$average_petal[new_centroids_2$group == &quot;red&quot;], Sepal.Length_blue = new_centroids_2$average_sepal[new_centroids_2$group == &quot;blue&quot;], Petal.Length_blue = new_centroids_2$average_petal[new_centroids_2$group == &quot;blue&quot;] ) %&gt;% # Tính khoảng cách từ các quan sát đến 2 centroids mutate( distance_red = sqrt((Sepal.Length-Sepal.Length_red)^2 + (Petal.Length - Petal.Length_red)^2), distance_blue = sqrt((Sepal.Length-Sepal.Length_blue)^2 + (Petal.Length - Petal.Length_blue)^2) ) %&gt;% # Tạo biến mới - nhóm mà các quan sát được phân vào mutate(group = case_when( distance_red &lt; distance_blue ~ &quot;red&quot;, TRUE ~ &quot;blue&quot; ) %&gt;% as.factor) # Plot dữ liệu mới data3 %&gt;% ggplot(aes(Sepal.Length, Petal.Length), group = group) + geom_point(size = 2, aes(col = group))+ theme_classic()+ scale_color_manual( values = c(&quot;blue&quot; = &quot;blue&quot;, &quot;red&quot; = &quot;red&quot;) )+ theme(legend.position = &quot;none&quot;)+ geom_point(aes(x=new_centroids_2$average_sepal[new_centroids_2$group == &quot;red&quot;], y=new_centroids_2$average_petal[new_centroids_2$group == &quot;red&quot;] ), col = &quot;red&quot;, shape=4, size = 5)+ geom_point(aes(x=new_centroids_2$average_sepal[new_centroids_2$group == &quot;blue&quot;], y=new_centroids_2$average_petal[new_centroids_2$group == &quot;blue&quot;] ), col = &quot;blue&quot;, shape=4, size = 5) Kết quả phân nhóm vẫn giống như kết quả trước đó, thuật toán dừng lại. 15.2 Thuật toán Như vậy, chúng ta có thể khái quát lại thuật toán của K-means như sau: Bước 1: Chọn ngẫu nhiên K tâm/centroids Bước 2: Tính khoảng cách từ các đối tượng/quan sát đến K tâm Bước 3: Dựa vào giá trị khoảng cách ở bước 2, chúng ta nhóm các quan sát vào K nhóm: Các quan sát gần với tâm nào hơn thì sẽ thuộc về nhóm đó. Bước 4: Xác định các tâm mới của K nhóm (tâm mới của mỗi nhóm có tọa độ bằng trung bình tọa độ của các quan sát trong nhóm đó). Bước 5: Lặp lại bước 2 đến bước 4 cho đến khi không có sự thay đổi kết quả phân nhóm các quan sát hoặc theo số lượng iterations mà chúng ta đặt ra cho thuật toán chạy. 15.3 Ưu nhược điểm của K-means Ưu điểm của thuật toán clustering K-means là đơn giản và dễ tính toán. Tuy nhiên thuật toán này có một số nhược điểm sau: Thứ nhất, do lấy ngẫu nhiên tâm/centroid nên khi dữ liệu lớn với rất nhiều các biến đầu vào thuật toán có thể phân loại các quan sát vào các nhóm khác nhau tại mỗi vòng (iteration). Thứ hai, thuật toán sẽ gặp vấn đề nếu các biến đầu vào có các đơn vị khác nhau, ví dụ như: biến chi tiêu hoặc thu nhập của khách hàng đơn vị là triệu đồng, biến đánh dấu khách hàng có sử dụng thẻ tín dụng (dummy 1-0), hoặc biến tỷ lệ chi tiêu thẻ tín dụng trên hạn mức được cấp, v.v. Vì vậy, để khắc phục vấn đề trên trước khi áp dụng thuật toán K-means, chúng ta cần phải chuẩn hóa dữ liệu đầu vào bằng cách scaling dữ liệu: \\[x_i = \\frac{x_i - min(x_i)}{max(x_i) - min(x_i)}\\] 15.4 Thực hành trên R Sau đây chúng ta sẽ cùng tìm hiểu cách thực hiện K-means trên R. Bước 1: Chuẩn hóa dữ liệu (Scaling): # Scale dữ liệu iris_scale &lt;- scale(iris %&gt;% select(-Species)) Bước 2: Xác định số lượng nhóm tối ưu # Xác định số lượng nhóm tối ưu library(factoextra) fviz_nbclust(iris_scale, kmeans, method = &quot;silhouette&quot;) Kết quả cho thấy trường hợp này chúng ta nên chia làm 2 nhóm sẽ tối ưu. Trong thực tế, việc chia thành bao nhiêu nhóm sẽ phụ thuộc vào mục tiêu kinh doanh, thông thường số lượng nhóm nên từ 2 đến 10. Bước 3: Thực hiện clustering K-means thành 2 nhóm # Thực hiện thuật toán K-means k_means_fit &lt;- kmeans(iris_scale, centers = 2 # số lượng nhóm ) # Nhóm của các quan sát k_means_fit$cluster ## [1] 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 ## [36] 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 ## [71] 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 ## [106] 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 ## [141] 1 1 1 1 1 1 1 1 1 1 # Tâm của mỗi nhóm k_means_fit$centers ## Sepal.Length Sepal.Width Petal.Length Petal.Width ## 1 0.5055957 -0.4252069 0.650315 0.6253518 ## 2 -1.0111914 0.8504137 -1.300630 -1.2507035 # Số lượng quan sát tại mỗi nhóm k_means_fit$size ## [1] 100 50 Bước 4: Trực quan hóa kết quả # Trực quan hóa kết quả K-means fviz_cluster(k_means_fit, data = iris_scale, geom = &quot;point&quot;, main = &quot;Iris Cluster&quot; )+ theme_bw()+ theme(axis.text = element_blank(), axis.title = element_blank(), axis.ticks = element_blank(), panel.grid = element_blank() ) Như vậy, chúng ta đã vừa tìm hiểu về thuật toán clustering K-means cũng như cách thực hiện thuật toán trên R. "],
["phan-tich-gio-hang.html", "Chương 16 Phân tích giỏ hàng 16.1 Các khái niệm cơ bản 16.2 Cách thực hiện mô hình 16.3 Ba câu hỏi khi phân tích giỏ hàng 16.4 Ưu nhược điểm", " Chương 16 Phân tích giỏ hàng Phân tích giỏ hàng là 1 phương pháp phân tích với mục đích là để tìm ra được tổ hợp các sản phẩm hay được khách hàng mua cùng nhau. Phân tích giỏ hàng là một nhánh của Frequent Pattern Mining (FPM) là kỹ thuật được dùng trong việc phân tích các hành vi lặp đi lặp lại giữa các yêu tố có liên hệ với nhau. FPM được sử dụng đặc biệt rộng rãi trong các ngành như ecommerce, banking, retail… giúp người bán có thể phân tích hành vi mua sắm của khách hàng. Phương pháp phân tích thương được dùng nhất là sử dụng thuật toánapriori 16.1 Các khái niệm cơ bản Trong phân tích giỏ hàng, chỉ có 5 thuật ngữ đơn giản ta cần phải nhớ là transaction, rule, support, confidence và lift Item: Sản phẩm chứa trong giỏ hàng Transaction: Giao dịch là một hoặc một nhóm các sản phẩm được mua khi khách hàng thực hiện trong cùng một giao dịch Rule: Rule là một quy tắc thể hiện mối quan hệ giữa các sản phẩm có trong cùng một giỏ hàng, có dạng nếu A, thì B. \\[\\left \\{{i_{1},i_{2},...}\\right \\} =&gt; {i_{k}}\\] Ví dụ: {Bánh mỳ} =&gt; {Sữa}: Nếu khách hàng mua bánh mỳ thì khách hàng sẽ mua thêm sữa Support：Tần suất (dưới dạng phần trăm) xuất hiện của các quy tắc trong tổng số các giao dịch. Ví dụ: Một cửa hàng trong tháng 1 có 100 khách hàng, mỗi khách hàng thực hiện một giao dịch. Trong đó 50 khách hàng mua sản phẩm A, 75 khách hàng mua sản phẩm B, và 25 khách hàng mua cả sản phẩm A và B - Support(sản phẩm A) = 50% - Support(Sản phẩm A, sản phẩm B) = 25% Confidence: Cơ hội mua sản phẩm tiếp theo trong hành vi giao dịch của khác hàng. \\[Confidence (i_{m} =&gt; i_{n}) = \\frac{support(i_{m}\\frown i_{n})}{support (i_{m})}\\] &gt; Ví dụ: Confidence (sản phẩm A , sản phẩm B) = 25/50 = 50% - Tức là nếu 1 người mua sản phẩm A, thì xác suất họ cũng mua sản phẩm B là 50% Lift: Nếu khách hàng mua sản phẩm A thì khả năng họ mua sản phẩm B sẽ tăng lên bao nhiêu % \\[Lift(i_{m} =&gt; i_{n}) = \\frac{support(i_{m}\\frown i_{n})}{support(i_{m}) * support (i_{n})} = \\frac{Confidence (i_{m} =&gt; i_{n})}{support (i_{n})}\\] Lift có thể cho 3 loại giá trị Lift &gt; 1: tức là những sản phẩm ở vế trái của rule sẽ làm tăng khả năng xảy ra của những sản phẩm ở vế phải của rule (2 sản phẩm bổ trợ). Ví dụ, mua bia sẽ mua thêm lạc. Lift &lt; 1: tức là những sản phẩm ở vế trái của rule sẽ làm giảm khả năng xảy ra của những sản phẩm ở vế phải của rule ( 2 sản phẩm thay thế được cho nhau). Ví dụ, mua bia sẽ không mua thêm cafe. Lift = 1: Các sản phẩm ở vế trái và vế phải xuất hiện độc lập với nhau về mặt thống kê, ta không thể đưa ra kết luận về tương quan giữa các sản phẩm. 16.2 Cách thực hiện mô hình library(dplyr) library(arules) library(arulesViz) data(&quot;Groceries&quot;) Dữ liệu Groceries chứa gần 10,000 giao dịch với hơn 160 sản phẩm khác nhau. Ta có thể xem chi tiết dưới đây. # Thống kê giao dịch summary(Groceries) ## transactions as itemMatrix in sparse format with ## 9835 rows (elements/itemsets/transactions) and ## 169 columns (items) and a density of 0.02609146 ## ## most frequent items: ## whole milk other vegetables rolls/buns soda ## 2513 1903 1809 1715 ## yogurt (Other) ## 1372 34055 ## ## element (itemset/transaction) length distribution: ## sizes ## 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 ## 2159 1643 1299 1005 855 645 545 438 350 246 182 117 78 77 55 ## 16 17 18 19 20 21 22 23 24 26 27 28 29 32 ## 46 29 14 14 9 11 4 6 1 1 1 1 3 1 ## ## Min. 1st Qu. Median Mean 3rd Qu. Max. ## 1.000 2.000 3.000 4.409 6.000 32.000 ## ## includes extended item information - examples: ## labels level2 level1 ## 1 frankfurter sausage meat and sausage ## 2 sausage sausage meat and sausage ## 3 liver loaf sausage meat and sausage Lưu ý: Dữ liệu phục vụ phân tích giỏ hàng không phải là dữ liệu dạng dataframe thông thường mà được cấu trúc định dạng transaction. # Sử dụng dữ liệu groceries class(Groceries) ## [1] &quot;transactions&quot; ## attr(,&quot;package&quot;) ## [1] &quot;arules&quot; str(Groceries) ## Formal class &#39;transactions&#39; [package &quot;arules&quot;] with 3 slots ## ..@ data :Formal class &#39;ngCMatrix&#39; [package &quot;Matrix&quot;] with 5 slots ## .. .. ..@ i : int [1:43367] 13 60 69 78 14 29 98 24 15 29 ... ## .. .. ..@ p : int [1:9836] 0 4 7 8 12 16 21 22 27 28 ... ## .. .. ..@ Dim : int [1:2] 169 9835 ## .. .. ..@ Dimnames:List of 2 ## .. .. .. ..$ : NULL ## .. .. .. ..$ : NULL ## .. .. ..@ factors : list() ## ..@ itemInfo :&#39;data.frame&#39;: 169 obs. of 3 variables: ## .. ..$ labels: chr [1:169] &quot;frankfurter&quot; &quot;sausage&quot; &quot;liver loaf&quot; &quot;ham&quot; ... ## .. ..$ level2: Factor w/ 55 levels &quot;baby food&quot;,&quot;bags&quot;,..: 44 44 44 44 44 44 44 42 42 41 ... ## .. ..$ level1: Factor w/ 10 levels &quot;canned food&quot;,..: 6 6 6 6 6 6 6 6 6 6 ... ## ..@ itemsetInfo:&#39;data.frame&#39;: 0 obs. of 0 variables # Xem 5 giao dịch đầu tiên Groceries[1:5] %&gt;% inspect ## items ## 1 {citrus fruit, ## semi-finished bread, ## margarine, ## ready soups} ## 2 {tropical fruit, ## yogurt, ## coffee} ## 3 {whole milk} ## 4 {pip fruit, ## yogurt, ## cream cheese , ## meat spreads} ## 5 {other vegetables, ## whole milk, ## condensed milk, ## long life bakery product} Trong thực tế, khi triển khai phân tích, việc đầu tiên ta cần làm là biến đổi từ định dạng dataframe sang định dạng transactions. Ta có thể biến đổi định dạng của dataframe về transactions với hàm as. df &lt;- data.frame( prod_1 = c(1,0, 1) %&gt;% as.factor, prod_2 = c(0,0, 1) %&gt;% as.factor, prod_3 = c(1, 0, 0) %&gt;% as.factor ) df ## prod_1 prod_2 prod_3 ## 1 1 0 1 ## 2 0 0 0 ## 3 1 1 0 df %&gt;% select(-1) %&gt;% as(&quot;transactions&quot;) %&gt;% inspect ## items transactionID ## 1 {prod_2=0,prod_3=1} 1 ## 2 {prod_2=0,prod_3=0} 2 ## 3 {prod_2=1,prod_3=0} 3 Phân tích khám phá nhanh các sản phẩm được mua nhiều nhất. # Vẽ barchart đơn giản về các item phổ biến nhất itemFrequencyPlot(Groceries, type = &quot;absolute&quot;, topN = 20, decreasing = T) 16.3 Ba câu hỏi khi phân tích giỏ hàng Khi sử dụng kỹ thuật phân tích giỏ hàng, có 3 câu hỏi thường gặp về mặt kinh doanh cần phải trả lời là: Các sản phẩm nào hay được mua cùng nhau? Nếu khách hàng mua sản phẩm A rồi thì sẽ hay mua tiếp sản phẩm nào? Khách hàng nếu mua sản phẩm B thì trước đấy hay mua sản phẩm nào? 16.3.1 Các sản phẩm nào hay được mua cùng nhau rules &lt;- apriori(Groceries, parameter = list(supp = 0.001, # support &gt;= 0.1% conf = 0.5)) ## Apriori ## ## Parameter specification: ## confidence minval smax arem aval originalSupport support minlen maxlen ## 0.5 0.1 1 none FALSE TRUE 0.001 1 10 ## target ext ## rules FALSE ## ## Algorithmic control: ## filter tree heap memopt load sort verbose ## 0.1 TRUE TRUE FALSE TRUE 2 TRUE ## ## Absolute minimum support count: 9 ## ## set item appearances ...[0 item(s)] done [0.00s]. ## set transactions ...[169 item(s), 9835 transaction(s)] done [0.00s]. ## sorting and recoding items ... [157 item(s)] done [0.00s]. ## creating transaction tree ... done [0.00s]. ## checking subsets of size 1 2 3 4 5 6 done [0.01s]. ## writing ... [5668 rule(s)] done [0.00s]. ## creating S4 object ... done [0.00s]. # confidence &gt;= 50% # Tổng hợp các rules rules %&gt;% summary ## set of 5668 rules ## ## rule length distribution (lhs + rhs):sizes ## 2 3 4 5 6 ## 11 1461 3211 939 46 ## ## Min. 1st Qu. Median Mean 3rd Qu. Max. ## 2.00 3.00 4.00 3.92 4.00 6.00 ## ## summary of quality measures: ## support confidence lift ## Min. :0.001017 Min. :0.5000 Min. : 1.957 ## 1st Qu.:0.001118 1st Qu.:0.5455 1st Qu.: 2.464 ## Median :0.001322 Median :0.6000 Median : 2.899 ## Mean :0.001668 Mean :0.6250 Mean : 3.262 ## 3rd Qu.:0.001729 3rd Qu.:0.6842 3rd Qu.: 3.691 ## Max. :0.022267 Max. :1.0000 Max. :18.996 ## ## mining info: ## data ntransactions support confidence ## Groceries 9835 0.001 0.5 Như vậy, khi khai phá dữ liệu của tập Groceries, ta có 5668 rules thỏa mãn hai điều kiện: Tần suất xuất hiện đạt ít nhất 1% Confidence của rule đạt ít nhất 50% # Top 5 rules có lift cao nhất rules %&gt;% sort(by = &quot;lift&quot;) %&gt;% head(5) %&gt;% as(&quot;data.frame&quot;) ## rules support ## 53 {Instant food products,soda} =&gt; {hamburger meat} 0.001220132 ## 37 {soda,popcorn} =&gt; {salty snack} 0.001220132 ## 444 {flour,baking powder} =&gt; {sugar} 0.001016777 ## 327 {ham,processed cheese} =&gt; {white bread} 0.001931876 ## 55 {whole milk,Instant food products} =&gt; {hamburger meat} 0.001525165 ## confidence lift ## 53 0.6315789 18.99565 ## 37 0.6315789 16.69779 ## 444 0.5555556 16.40807 ## 327 0.6333333 15.04549 ## 55 0.5000000 15.03823 Giải thích: Với rule đầu tiên là {Instant food products, soda}, ta có thể diễn giải như sau: support = 0.00122 - Tần xuất xuất hiện rules trong tổng số các transaction là 1.22% confidence = 0.63157 - Nếu khách hàng mua đồ ăn nhanh (instant food products), 63.16% khách hàng sẽ mua thêm soda lift = 18.99 - Mối quan hệ giữa hai sản phẩm đồ ăn nhanh và soda cao gần 19 lần so với thông thường (khi hai sản phẩm hoàn toàn độc lập với nhau) Lưu ý: Khi phân tích, ta cần loại bỏ các rule thừa (redundant) khi phân tích dữ liệu. Một rule A được gọi là một rule thừa nếu tồn tại một rule con có confidence lớn hơn hoặc bằng rule A này. Rule B được gọi là rule con của rule A nếu có cùng RHS nhưng các sản phẩm trong rule B ít hơn rule A Ví dụ: Với 2 rule Rule A với {a,b,c} → {d} Rule B với {a,b} → {d} Rule B với {a,b} → {d} được gọi là rule thừa nếu \\(conf(A) &gt;= conf(B)\\) Cách loại bỏ rule thừa trong R subset.matrix &lt;- is.subset(rules, rules) subset.matrix[lower.tri(subset.matrix, diag = T)] &lt;- NA redundant &lt;- colSums(subset.matrix, na.rm = T) &gt;= 1 rules.pruned &lt;- rules[!redundant] rules &lt;- rules.pruned rules %&gt;% summary ## set of 1904 rules ## ## rule length distribution (lhs + rhs):sizes ## 2 3 4 5 ## 11 1381 509 3 ## ## Min. 1st Qu. Median Mean 3rd Qu. Max. ## 2.000 3.000 3.000 3.265 4.000 5.000 ## ## summary of quality measures: ## support confidence lift ## Min. :0.001017 Min. :0.5000 Min. : 1.957 ## 1st Qu.:0.001118 1st Qu.:0.5238 1st Qu.: 2.211 ## Median :0.001525 Median :0.5556 Median : 2.678 ## Mean :0.002070 Mean :0.5776 Mean : 2.907 ## 3rd Qu.:0.002237 3rd Qu.:0.6130 3rd Qu.: 3.180 ## Max. :0.022267 Max. :1.0000 Max. :18.996 ## ## mining info: ## data ntransactions support confidence ## Groceries 9835 0.001 0.5 Như vậy, sau khi lọc bỏ các rule thừa, số lượng rule trong dữ liệu giảm đi gần 3 lần xuống còn 1904 rules khác nhau. Ta có thể vẽ biểu đồ cho nhóm 10 rules có lift cao nhất như sau. rules %&gt;% head(10) %&gt;% plot(method = &quot;graph&quot;) 16.3.2 Khách hàng mua sản phẩm A thì sẽ mua sản phẩm nào tiếp theo? rules &lt;- apriori(data = Groceries, parameter = list(supp = 0.001, conf = 0.15, minlen = 2), appearance = list(default = &quot;rhs&quot;, lhs = &quot;whole milk&quot;), control = list(verbose = F)) rules %&gt;% sort(by = &quot;lift&quot;) %&gt;% head %&gt;% as(&quot;data.frame&quot;) ## rules support confidence lift ## 2 {whole milk} =&gt; {root vegetables} 0.04890696 0.1914047 1.7560310 ## 1 {whole milk} =&gt; {tropical fruit} 0.04229792 0.1655392 1.5775950 ## 4 {whole milk} =&gt; {yogurt} 0.05602440 0.2192598 1.5717351 ## 6 {whole milk} =&gt; {other vegetables} 0.07483477 0.2928770 1.5136341 ## 5 {whole milk} =&gt; {rolls/buns} 0.05663447 0.2216474 1.2050318 ## 3 {whole milk} =&gt; {soda} 0.04006101 0.1567847 0.8991124 Tham số lhs cho phép chúng ta lựa chọn điều kiện về sản phẩm được mua đầu tiên. Trong trường hợp này, ta thấy khách hàng mua sữa sẽ có xu hương mua thêm rau củ quả (root vegetables) 16.3.3 Khách hàng mua sản phẩm gì thì sẽ mua tiếp sản phẩm A? Tương tự với lhs, ta có thể sử dụng tham số rhs để tìm kiếm các khách hàng tiềm năng cho một sản phẩm đã xác định trước. rules &lt;- apriori(data = Groceries, parameter = list(supp = 0.001, conf = 0.08), appearance = list(default = &quot;lhs&quot;, rhs = &quot;whole milk&quot;), control = list(verbose = F)) rules %&gt;% sort(by = &quot;lift&quot;) %&gt;% head %&gt;% as(&quot;data.frame&quot;) ## rules ## 196 {rice,sugar} =&gt; {whole milk} ## 323 {canned fish,hygiene articles} =&gt; {whole milk} ## 1643 {root vegetables,butter,rice} =&gt; {whole milk} ## 1705 {root vegetables,whipped/sour cream,flour} =&gt; {whole milk} ## 1716 {butter,soft cheese,domestic eggs} =&gt; {whole milk} ## 1985 {pip fruit,butter,hygiene articles} =&gt; {whole milk} ## support confidence lift ## 196 0.001220132 1 3.913649 ## 323 0.001118454 1 3.913649 ## 1643 0.001016777 1 3.913649 ## 1705 0.001728521 1 3.913649 ## 1716 0.001016777 1 3.913649 ## 1985 0.001016777 1 3.913649 16.4 Ưu nhược điểm Như vậy, ta vừa thực hiện xong việc ứng dụng phân tích giỏ hàng. Đây là phương pháp cực kỳ hữu hiệu trong việc tìm kiếm các mối quan hệ ẩn, chưa được khám phá giữa các biến. Phương pháp này cũng có các ưu và nhược điểm như sau. Ưu điểm: Thực hiện nhanh Tính giải thích cao, trưc quan hóa Có thể nhanh chóng tìm ra tập khách hàng tiềm năng theo điều kiện và không cần phải chia ra thành các tập train/test như các thuật toán machine learning khác. Nhược điểm: Chỉ dùng cho các biến factor, không dùng được cho các biến dạng số như thu nhập, độ tuổi,… Nếu muốn đưa các biến này vào cần phải biến đổi thành dạng factor Khi có quá nhiều nhóm, tốc độ tính toán có thể rất chậm và tràn bộ nhớ "],
["feature-engineering.html", "Chương 17 Feature Engineering 17.1 Feature engineering cho các biến nhóm 17.2 Các biến liên tục", " Chương 17 Feature Engineering Khi xây dựng mô hình dự báo, ta phải cân bằng giữa độ chính xác và khả năng giải thích của mô hình. Trong nhiều trường hợp, khả năng giải thích được ưu tiên hơn, thể hiện rất rõ trong các mô hình score card của rủi ro trong hệ thống ngân hàng. Tuy nhiên, ta không thể hy sinh độ chính xác để lấy khả năng giải thích nếu độ chính xác của mô hình không đạt đến một ngưỡng nhất định. Khi đó, có hai cách tiếp cận: Cho thêm biến vào mô hình. Thay đổi các biến có sẵn bằng các nhóm biến phái sinh để mô hình tốt hơn - cách tiếp cận này gọi là feature engineering. Feature Engineering là quá trình thể hiện các biến đầu vào (variables) với những cách thức khác nhau để giúp cho tăng độ chính xác của mô hình dự báo. Ví dụ: Địa điểm của khách hàng có thể được thể hiện bằng ZIP code hoặc cùng lúc 2 biến, kinh độ và vĩ độ. Biến dự báo age có thể cho ra kết quả tốt hơn trong mô hình khi ta dùng biến \\(\\frac{1}{age}\\) Với mỗi mô hình, thuật toán khác nhau sẽ yêu cầu những cách thức triển khai và thay đổi biến đầu vào khác nhau. Do đó, khi lựa chọn cách thức biến đổi biến đầu vào, ta phải nắm rất rõ thuật toán và cách thức biến đổi dữ liệu theo từng trường hợp khác nhau. Đối với feature engineering, ta có thể chia làm 2 nhóm chính. Biến đổi các biến định dạng nhóm (categorical) Biến đổi các biến định dạng số (numeric) 17.1 Feature engineering cho các biến nhóm 17.1.1 Tạo dữ liệu giả (dummy data) cho biến không phân biệt thứ tự Trong phương pháp này, toàn bộ các dữ liệu gốc được chuyển sang dạng 0-1. Tuy nhiên, dữ liệu mới được tạo ra sẽ ít hơn dữ liệu gốc 1 trường hợp. Bởi lẽ khi biết giá trị của 6 biến, ta có thể biết được giá trị của biến cuối cùng. Biến gốc Mon Tues Wed Thurs Fri Sat Sun 0 0 0 0 0 0 Mon 1 0 0 0 0 0 Tues 0 1 0 0 0 0 Wed 0 0 1 0 0 0 Thurs 0 0 0 1 0 0 Fri 0 0 0 0 1 0 Sat 0 0 0 0 0 1 zero-variance predictor: là biến chỉ có một giá trị. Khi xây dựng mô hình, ta cần loại biến này. 17.1.2 Dữ liệu có rất nhiều nhóm Đối với các biến có rất nhiều nhóm (ví dụ: 200 chi nhánh trong ngân hàng), ta có 2 cách tiếp cận. Cách một, dựa vào kiến thức nghiệp vụ tự nhóm. Ví dụ, các chi nhánh ở Hà Nội sẽ đánh dấu là HN, ở Hồ Chí Minh là HCM, các chi nhánh còn lại là Others. Cách hai, sử dụng hash function. Trong trường hợp này, các biến category sẽ được tạo thành một biến hoàn toàn mới có giá trị số. Xem ví dụ dưới đây. Giá trị Hash belvedere tiburon 58275378 berkeley 1166288024 Lưu ý: Nhiều thí nghiệm đã được sử dụng để so sánh sự khác biệt giữa việc dùng factor và encoding 0-1 trong dữ liệu. Kết quả cho thấy không có nhiều sự khác biệt giữa hai cách. 17.2 Các biến liên tục Đối với các biến liên tục, khi xây dựng mô hình, ta sẽ gặp phải các vấn đề sau. Các biến có các đơn vị khác nhau. Ví dụ, tuổi có giá trị từ 15-75, thu nhập có giá trị từ 2 triệu VND đến 200 triệu VND Các biến bị lệch sang phải (skewness) Các biến có xuất hiện giá trị ngoại lai (outliers) Các biến có thể bị chặn trai hoặc chặn phải. Ví dụ, độ tuổi có giá trị không quá 80 Đối với các biến số, có ba nhóm kỹ thuật lớn biến đổi dữ liệu. Biến đổi 1:1 - một biến được biến đổi thành một biến khác Biến đổi 1:n - một biến được biến đổi thành nhiều biến khác nhau Biến đổi n:n - n biến gốc được biến đổi cùng lúc thành n biến khác 17.2.1 Biến đổi 1:1 Trong biến đổi 1:1, có rất nhiều cách khác nhau. Biến đổi theo scale của dữ liệu: log, Box-Cox \\[x^{*} = \\left\\{ \\begin{array}{l l} \\frac{x^{\\lambda}-1}{\\lambda\\: \\tilde{x}^{\\lambda-1}}, &amp; \\lambda \\neq 0 \\\\ \\tilde{x} \\: \\log x, &amp; \\lambda = 0 \\\\ \\end{array} \\right.\\] "],
["gii-thiu-v-chui-thi-gian.html", "Chương 18 Giới thiệu về chuỗi thời gian 18.1 Thành phần của chuỗi thời gian 18.2 Dữ liệu chuỗi thời gian 18.3 Smoothing 18.4 Seasonal decomposition 18.5 Exponential Forecasting Model", " Chương 18 Giới thiệu về chuỗi thời gian Khi phân tích dữ liệu, có hai khái niệm đều được gọi là dự báo khi được dịch sang tiếng Việt, đó là forcast và predict. Tuy nhiên, hai khái niệm này rất khác nhau. Predict: Thường được dùng để chỉ việc dự báo xác suất xảy ra các sự kiện. Ví dụ, xác suất vỡ nợ, xác suất khách hàng churn,… Forecast: Thường dùng trong việc dự báo chuỗi thời gian. Ví dụ, dựa vào lịch sử biến động của tổng khách hàng uống cafe theo tuần, ta có thể dự báo được số lượng khách hàng uống cafe của 1 tuần tới, hai tuần tới,… Trong phần trước, chúng ta đã bàn nhiều về nhóm predict, trong phần này, ta sẽ bàn thêm về nhóm forecast, về các cấu phần và cách thức dự báo đối với chuỗi thời gian. Do đó, trong phần này, khi nói về dự báo, chúng ta đang bàn về vấn đề forecast. Phân biệt dự báo, mục tiêu và kế hoạch Dự báo: là quá trình sử dụng các thông tin hiện hữu đưa ra các nhận định chính xác nhất có thể có trong tương lai trong một khoảng thời gian xác định về một chỉ số nào đó. Mục tiêu: là thứ mà cá nhân, tổ chức mong muốn đạt được trong một khoảng thời gian xác định trong tương lai. Thường thì mục tiêu được đặt mà không quan tâm đến bất kỳ đến việc dự báo nào cả. Ví dụ, mục tiêu tăng trưởng của doanh nghiệp thường là năm sau cao gấp đôi năm trước trong khi dự báo chỉ có thể tăng được 30%. Kế hoạch: Là phản ứng của tổ chức, cá nhân đối với dự báo và mục tiêu. Việc lập kế hoạch đòi hỏi nhiều hành động cụ thể để điều hướng dự báo sát sát với mục tiêu (hoặc vượt mục tiêu) Xét về yếu tố thời gian, việc dự báo có thể chia thành dự báo ngắn hạn, trung hạn và dài hạn. Các điểm khi dự báo Khi dự báo, có hai điểm quan trọng chúng ta phải trả lời. Thứ nhất, ta cần dự báo điều gì? Dự báo với từng sản phẩm hay với cả nhóm sản phẩm? Dự báo doanh số bán hàng của từng cửa hàng hay của toàn hệ thống? Thứ hai, yếu tố thời gian xét trong vấn đề dự báo này là gì? Ta cần dự báo trong bao lâu? Tần xuất như thế nào? Ví dụ, dư báo doanh số bán hàng mỗi tháng/tuần 1 lần trong 1 năm tới? Lưu ý: Các bạn phân tích dữ liệu cần phải tìm hiểu các đơn vị nghiệp vụ sẽ sử dụng kết quả dự báo như thế nào, để tránh việc bỏ quá nhiều thời gian và công sức dự báo nhưng không ai sử dụng. 18.1 Thành phần của chuỗi thời gian Trong bất cứ chuỗi thời gian nào, cũng có 3 thành phần sau. Xu hướng (trend) thể hiện chiều hướng tăng hay giảm dài hạn của chuỗi thời gian Mùa vụ (seasonal) thể hiện sự biến đổi của chuỗi thời gian theo chu kỳ biết trước. Ví dụ, vào cuối tuần, khách hàng có xu hướng đi ăn nhà hàng nhiều hơn. Chu kỳ kinh doanh (cyclic) thể hiện xu hướng biến đổi dài hạn của chuỗi thời gian, thường ít nhất hai năm. Chu kỳ kinh doanh khác với yếu tố mùa vụ ở chỗ, chu kỳ biến đổi của yếu tố mùa vụ thường là đã được biết trước và mang tính ngắn hạn. Khi phân tích chuỗi thời gian tập trung vào 2 câu hỏi: Cái gì đã xảy ra Cái gì sẽ xảy ra, với độ tin cậy bao nhiêu phần trăm Phân tích chuỗi thời gian sử dụng rất nhiều trong việc dự báo tăng trưởng, lên kế hoạch. Mô hình này dùng rất nhiều trong Macroeconomics, Finance… Lưu ý: Để phân tích chuỗi thời gian, R lưu object dưới dạng ts (time series) Chuỗi thời gian có 1 loạt các kỹ thuật và câu lệnh phân tích riêng biệt 18.2 Dữ liệu chuỗi thời gian Khi lưu trữ trong R, chuỗi thời gian được lưu dưới dạng ts như sau. #Tạo chuỗi thời gian sales &lt;- c(round(rnorm(24,50,10),0)) #Tạo object ts tsales &lt;- ts(sales, start = c(2003,1), frequency = 12) tsales ## Jan Feb Mar Apr May Jun Jul Aug Sep Oct Nov Dec ## 2003 46 47 44 43 58 41 63 23 45 57 64 45 ## 2004 50 48 26 44 47 37 68 64 48 64 71 64 #Các chỉ số cơ bản #plot plot(tsales, pch=19, type = &quot;o&quot;) #Thời điểm bắt đầu start(tsales) ## [1] 2003 1 #Thời điểm kết thúc end(tsales) ## [1] 2004 12 #Frequency frequency(tsales) ## [1] 12 #Tạo subset tsales.subset &lt;- window(tsales, start = c(2003,9), end = c(2004,3)) tsales.subset ## Jan Feb Mar Apr May Jun Jul Aug Sep Oct Nov Dec ## 2003 45 57 64 45 ## 2004 50 48 26 18.3 Smoothing Centered Moving Average là kỹ thuật làm trơn để giảm đi các yếu tố gây nhiễu trong chuỗi thời gian. \\[S_t=\\frac{Y_{t-q}+...+Y_t+...+Y_{t+q}}{2q+1}\\] \\(S_t\\) được gọi là giá trị làm trơn (smoothed value) tại thời điểm t, \\(k=2q+1\\) là số quan sát được lấy giá trị trung bình. k thường là giá trị lẻ. Mô hình trên được gọi là Moving Average (MA) library(forecast) library(ggfortify) par(mfrow=c(2,2)) plot(Nile) plot(ma(Nile, 3), main = &quot;Moving Average with k=3&quot;) plot(ma(Nile, 10), main = &quot;Moving Average with k=10&quot;) plot(ma(Nile, 15), main = &quot;Moving Average with k=15&quot;) 18.4 Seasonal decomposition Bất kỳ mô hình times series nào cũng được cấu thành bởi ba yếu tố Trend: Xu hương phát triển theo thời gian Seasonal: Yếu tố chu kỳ Irregular component (error): Các yếu tố gây nhiễu Mô hình chuỗi thời gian cơ bản có hai dạng: Additive: \\(Y_t=Trend_t + Seasonal_t + Irregular_t\\) Multiplicative: \\(Y_t=Trend_t * Seasonal_t * Irregular_t\\) Do đó, để phân tích chuỗi thời gian, ta cũng cần bóc tách chuỗi thời gian thành 3 yếu tố như trên bằng hàm stl (Seasonal Decomposition of Time Series by Loess) stl(ts, s.window = , t.window =) Lưu ý: Hàm trên chỉ dùng được cho Additive model Đối với Multiplicative, ta cần biến đổi như sau; \\[log(Y_t) = log(Trend_t * Seasonal_t * Irregular_t)=log(Trend_t)+log(Seasonal_t)+log(Irregular_t)\\] s.window: kiểm soát yếu tố thời vụ (seasonal) ảnh hưởng đên chuối thời gian thế nào. Nếu \\(s.window=&quot;period&quot;\\) thì yếu tố thời vụ sẽ giống hệt nhau qua các năm t.window: kiểm soát yếu tố xu hướng (trend) thay đổi theo thời gian thế nào library(dplyr) par(mfrow=c(1,1)) #Mô hình có dạng Multiplicative plot(AirPassengers) #Chuyển sang dạng Additive lAirPassengers &lt;- log(AirPassengers) plot(AirPassengers) #Phân tích thành phần ts fit &lt;- stl(lAirPassengers, s.window = &quot;period&quot;) autoplot(fit) + theme_bw() #Quay lại dữ liệu gốc fit$time.series %&gt;% head ## seasonal trend remainder ## [1,] -0.09164042 4.829389 -0.01924936 ## [2,] -0.11402828 4.830368 0.05434477 ## [3,] 0.01586585 4.831348 0.03558845 ## [4,] -0.01402759 4.833377 0.04046325 ## [5,] -0.01502478 4.835406 -0.02459053 ## [6,] 0.10978976 4.838166 -0.04268143 fit$time.series %&gt;% exp %&gt;% head ## seasonal trend remainder ## [1,] 0.9124332 125.1344 0.9809347 ## [2,] 0.8922327 125.2571 1.0558486 ## [3,] 1.0159924 125.3798 1.0362293 ## [4,] 0.9860703 125.6345 1.0412930 ## [5,] 0.9850875 125.8897 0.9757094 ## [6,] 1.1160434 126.2377 0.9582166 #Phân tích xu hướng theo tháng và seasonal monthplot(AirPassengers) seasonplot(AirPassengers, col = &quot;blue&quot;, type = &quot;o&quot;) Lưu ý: Nhìn vào bảng, ta có thể có các nhận định sau: Tháng một, yếu tố thời vụ khiến khách hàng giảm 9% (seasonal=-0.091) Tháng bảy, yếu tố thời vụ khiến khách hàng tăng 21% (seasonal = 0.216) 18.5 Exponential Forecasting Model Mô hình dạng này rất đơn giản nhưng có thể đưa ra các dự báo ngắn hạn khá chính xác. Có 3 dạng: Single Exponential Model (Simple Exponential Model) Double Exponential Model (Hold Exponential Model) Triple Exponential Model (Hold-Winters Exponential Model) 18.5.1 Single Exponential Model \\[Y_t=level + irregular_t\\] Dự báo giá trị của \\(Y_{t+1}\\) (1-step ahead forecast) là: \\[Y_{t+1}=c_0Y_t + c_1Y_{t-1}+c_2Y_{t-2}+...\\] với \\[c_i=\\alpha(1-\\alpha)^i\\], \\[\\sum c_i=1\\], \\[i=0,1,2...\\], \\[\\alpha\\in[0,1]\\] đo lường độ ảnh hưởng của các biến trước đó đến giá trị dự báo \\(Y_{t+1}\\): càng gần 1, các biến càng gần có trọng số càng lớn càng gần 0, các biến càng xa có trọng số càng lớn library(forecast) fit &lt;- ets(y = nhtemp, model = &quot;ANN&quot;) fit ## ETS(A,N,N) ## ## Call: ## ets(y = nhtemp, model = &quot;ANN&quot;) ## ## Smoothing parameters: ## alpha = 0.182 ## ## Initial states: ## l = 50.2759 ## ## sigma: 1.1263 ## ## AIC AICc BIC ## 265.9298 266.3584 272.2129 forecast(fit,1) ## Point Forecast Lo 80 Hi 80 Lo 95 Hi 95 ## 1972 51.87045 50.42708 53.31382 49.66301 54.0779 autoplot(forecast(fit, 1), xlab=&quot;Year&quot;, ylab=expression(paste(&quot;Temperature (&quot;, degree*F,&quot;)&quot;,)), main=&quot;New Haven Annual Mean Temperature&quot;) + theme_bw() accuracy(fit) ## ME RMSE MAE MPE MAPE MASE ## Training set 0.1460295 1.126268 0.8951331 0.2418693 1.748922 0.7512497 ## ACF1 ## Training set -0.00653111 Giải thích: \\(\\alpha=0.182\\) cho thấy rất nhiều quan sát được tính đến trong mô hình. Độ chính xác của mô hình được tính bằng các chỉ số thông dụng sau Chỉ số Viết tắt Cách tính Mean error ME \\(mean(\\epsilon_t)\\) Root mean squared error RMSE \\(sqrt(mean(\\epsilon_t^2))\\) Mean absolute error MAE \\(mean(\\epsilon_t)\\) Mean percentage error MPE \\(mean(100*\\epsilon_t/Y_t)\\) Mean absolute percentage error MAPE \\(mean( 100*\\epsilon_t/Y_t)\\) ME (mean ), RMSE, MAE, MPE, MAPE, MASE 18.5.2 Hold &amp; Holt-Winters exponential smoothing Hold exponential smoothing \\[Y_t=level + slope*t + irregular_t\\] \\(\\alpha\\): Độ ảnh hưởng của các biến y trong quá khứ đến biến hiện tại \\(\\beta\\): Độ ảnh hưởng của biến xu hướng trong quá khứ đến hiện tại Hold-Winters exponential smoothing \\[Y_t=level + slope*t + s_t + irregular_t\\] \\(\\alpha\\): Độ ảnh hưởng của các biến y trong quá khứ đến biến hiện tại \\(\\beta\\): Độ ảnh hưởng của biến xu hướng trong quá khứ đến hiện tại \\(\\gamma\\): Độ ảnh hường của biến chu kỳ đến hiện tại Lưu ý: Các ảnh hưởng của slope và chu kỳ (và ) đến giá trị hiện tại được tính theo công thức giống như alpha library(forecast) library(dplyr) fit &lt;- ets(log(AirPassengers), model = &quot;AAA&quot;) fit ## ETS(A,A,A) ## ## Call: ## ets(y = log(AirPassengers), model = &quot;AAA&quot;) ## ## Smoothing parameters: ## alpha = 0.6534 ## beta = 1e-04 ## gamma = 1e-04 ## ## Initial states: ## l = 4.8022 ## b = 0.01 ## s=-0.1047 -0.2186 -0.0761 0.0636 0.2083 0.217 ## 0.1145 -0.011 -0.0111 0.0196 -0.1111 -0.0905 ## ## sigma: 0.0359 ## ## AIC AICc BIC ## -208.3619 -203.5047 -157.8750 #Dự báo pred &lt;- forecast(fit, 5) pred ## Point Forecast Lo 80 Hi 80 Lo 95 Hi 95 ## Jan 1961 6.103667 6.057633 6.149701 6.033264 6.174070 ## Feb 1961 6.093102 6.038107 6.148096 6.008995 6.177208 ## Mar 1961 6.233814 6.171126 6.296502 6.137940 6.329688 ## Apr 1961 6.213130 6.143591 6.282668 6.106780 6.319480 ## May 1961 6.223273 6.147500 6.299047 6.107388 6.339159 autoplot(pred, main=&quot;Forecast for Air Travel&quot;, ylab=&quot;Log(AirPassengers)&quot;, xlab=&quot;Time&quot;) + theme_bw() pred$mean &lt;- exp(pred$mean) pred$lower &lt;- exp(pred$lower) pred$upper &lt;- exp(pred$upper) p &lt;- cbind(pred$mean, pred$lower, pred$upper) dimnames(p)[[2]] &lt;- c(&quot;mean&quot;, &quot;Lo 80&quot;, &quot;Lo 95&quot;, &quot;Hi 80&quot;, &quot;Hi 95&quot;) p %&gt;% head ## mean Lo 80 Lo 95 Hi 80 Hi 95 ## [1,] 447.4958 427.3626 417.0741 468.5774 480.1365 ## [2,] 442.7926 419.0991 407.0741 467.8256 481.6452 ## [3,] 509.6958 478.7246 463.0988 542.6706 560.9814 ## [4,] 499.2613 465.7230 448.8908 535.2148 555.2839 ## [5,] 504.3514 467.5469 449.1637 544.0531 566.3198 class(p) ## [1] &quot;mts&quot; &quot;ts&quot; &quot;matrix&quot; 18.5.3 Tự động xây dựng mô hình Hàm ets cho phép tự động lựa chọn mô hình tốt nhất theo phương pháp maximum likelihood. library(forecast) library(dplyr) (fit &lt;- ets(JohnsonJohnson)) ## ETS(M,A,M) ## ## Call: ## ets(y = JohnsonJohnson) ## ## Smoothing parameters: ## alpha = 0.1481 ## beta = 0.0912 ## gamma = 0.4908 ## ## Initial states: ## l = 0.6146 ## b = 0.005 ## s=0.692 1.2644 0.9666 1.077 ## ## sigma: 0.0889 ## ## AIC AICc BIC ## 166.6964 169.1289 188.5738 forecast(fit) %&gt;% autoplot + theme_bw() "],
["mo-hinh-arima.html", "Chương 19 Mô hình ARIMA 19.1 Chuỗi thời gian dừng 19.2 Mô hình ARIMA 19.3 Ví dụ với R", " Chương 19 Mô hình ARIMA 19.1 Chuỗi thời gian dừng Chuỗi thời gian dừng là yêu cầu bắt buộc để xây dựng mô hình ARIMA. Định nghĩa chuối thời gian dừng: Giá trị trung bình không đổi theo thời gian Phương sai không đổi theo thời gian (homoskedaticity) Covariance của chuỗi thời gian thứ i và (i+m) không đổi 19.2 Mô hình ARIMA ARIMA là viết tắt của Auto-Regressive Integrated Moving Average \\[ARIMA(p,d,q) = AR(p) + I(d) + MA(q)\\] Trong đó: p: Số bậc trong mô hình Auto-Regressive d: Số bậc trong mô hình Itegrated (số lần lấy \\(\\delta\\) để có chuỗi thời gian dừng) q: Số bậc trong mô hình Moving Agerage Integrated model Gọi \\(Y_1,...,Y_t\\) là chuỗi thời gian gốc. Ta có: d=0: \\(y_t=Y_t\\) d=1: \\(y_t=Y_t-Y_{t-1}\\) d=2: \\(y_t=(Y_t-Y_{t-1})-(Y_{t-1}-Y_{t-2})\\) Với d=2, còn được gọi là “the first difference of the first difference”. Sau khi được chuỗi dừng \\(y_t\\), ta có thể dự báo mô hình ARIMA như sau: \\[\\hat{y}_t=\\mu + \\phi_1y_{t-1}+...+\\phi_py_{t-p} + \\theta_1\\epsilon_{t-1}+...+\\theta_q\\epsilon_{t-q}\\] ACF vs. PACF ACF: Autocorrelation - đo correlation giữa các quan sát trong chuỗi PACF: Partial Auto Correlation - đo correlation giữa biến \\(Y_t\\) và \\(Y_{t-k}\\), loại bỏ các biến ở giữa chúng Kỹ thuật tạo chuỗi dừng: Với biến có var biến đổi: log Với biến có mean biến đổi: Sử dụng “Difference” Test chuỗi dừng: Test ADF (Augmented Dickey-Fuller) \\(H_0\\): Chuỗi không dừng (non-stationary) \\(H_1\\): Chuỗi dừng (stationary) Lựa chọn tham số trong ARIMA Model ACF PACF ARIMA(p,d,0) Giảm dần đều về 0 Giảm về 0 sau lag p ARIMA(0,d,q) Giảm về 0 sau lag q Giảm dần đều về 0 ARIMA(p,d,q) Giảm dần đều về 0 Giảm dần đều về 0 Xem xét ACF &amp; PACF trong các chuỗi sau library(forecast) library(tseries) library(dplyr) library(ggfortify) ar1 &lt;- arima.sim(list(ar=c(0.89)), n = 100) ar2 &lt;- arima.sim(list(ar=c(0.89, -0.4858)), n = 100) ma1 &lt;- arima.sim(n = 100, list(ma = c(-0.2279))) ma2 &lt;- arima.sim(n = 100, list(ma = c(-0.2279, 0.2488))) arma &lt;- arima.sim(n = 100, list(ar = c(0.8897, -0.4858), ma = c(-0.2279, 0.2488))) #Tạo function tseries.plot &lt;- function(x){ par(mfrow=c(1,2)); Acf(x, col = &quot;blue&quot;, main = paste(c(&quot;ACF plot&quot;))); Pacf(x, col = &quot;red&quot;, main = paste(c(&quot;PACF plot&quot;))); } purrr::map(list(ar1, ar2, ma1, ma2, arma), tseries.plot) ## [[1]] ## ## Partial autocorrelations of series &#39;x&#39;, by lag ## ## 1 2 3 4 5 6 7 8 9 10 ## 0.840 0.051 -0.199 -0.014 0.078 0.057 -0.234 -0.004 0.011 0.014 ## 11 12 13 14 15 16 17 18 19 20 ## 0.032 0.084 -0.200 -0.138 -0.087 -0.004 0.097 -0.049 0.075 0.011 ## ## [[2]] ## ## Partial autocorrelations of series &#39;x&#39;, by lag ## ## 1 2 3 4 5 6 7 8 9 10 ## 0.643 -0.498 -0.022 0.039 -0.060 -0.171 0.053 0.019 -0.072 -0.192 ## 11 12 13 14 15 16 17 18 19 20 ## 0.062 -0.021 0.086 -0.145 -0.057 -0.017 0.013 -0.039 0.040 -0.163 ## ## [[3]] ## ## Partial autocorrelations of series &#39;x&#39;, by lag ## ## 1 2 3 4 5 6 7 8 9 10 ## -0.351 -0.237 -0.191 -0.118 0.078 0.210 0.004 -0.029 -0.110 0.013 ## 11 12 13 14 15 16 17 18 19 20 ## -0.040 -0.160 -0.032 -0.071 -0.086 0.066 -0.031 -0.112 -0.026 0.105 ## ## [[4]] ## ## Partial autocorrelations of series &#39;x&#39;, by lag ## ## 1 2 3 4 5 6 7 8 9 10 ## -0.356 0.098 0.252 -0.143 -0.060 0.020 0.056 0.046 -0.006 0.088 ## 11 12 13 14 15 16 17 18 19 20 ## 0.028 -0.083 -0.075 0.062 -0.093 0.029 -0.037 0.039 -0.030 -0.089 ## ## [[5]] ## ## Partial autocorrelations of series &#39;x&#39;, by lag ## ## 1 2 3 4 5 6 7 8 9 10 ## 0.602 -0.264 -0.008 -0.187 0.176 0.097 -0.198 -0.141 0.046 -0.086 ## 11 12 13 14 15 16 17 18 19 20 ## 0.005 -0.158 -0.021 0.034 0.010 0.016 -0.158 0.125 0.104 -0.034 19.3 Ví dụ với R library(forecast) library(tseries) plot(Nile) #Tìm giá trị tối ưu của d để loại trend ndiffs(Nile) ## [1] 1 dNile &lt;- diff(Nile, 1) plot(dNile) adf.test(dNile) ## Warning in adf.test(dNile): p-value smaller than printed p-value ## ## Augmented Dickey-Fuller Test ## ## data: dNile ## Dickey-Fuller = -6.5924, Lag order = 4, p-value = 0.01 ## alternative hypothesis: stationary #Xem mô hình par(mfrow=c(1,2)) Acf(dNile) Pacf(dNile) #ACF và PACF đưa ra gợi ý mô hình ARIMA(0,1,1): ACF giảm về 0 sau lag 1, PACF giảm dần về 0 fit &lt;- arima(Nile, order = c(0,1,1)) fit ## ## Call: ## arima(x = Nile, order = c(0, 1, 1)) ## ## Coefficients: ## ma1 ## -0.7329 ## s.e. 0.1143 ## ## sigma^2 estimated as 20600: log likelihood = -632.55, aic = 1269.09 accuracy(fit) ## ME RMSE MAE MPE MAPE MASE ## Training set -11.9358 142.8071 112.1752 -3.574702 12.93594 0.841824 ## ACF1 ## Training set 0.1153593 #Đánh giá mô hình par(mfrow=c(1,1)) names(fit) ## [1] &quot;coef&quot; &quot;sigma2&quot; &quot;var.coef&quot; &quot;mask&quot; &quot;loglik&quot; ## [6] &quot;aic&quot; &quot;arma&quot; &quot;residuals&quot; &quot;call&quot; &quot;series&quot; ## [11] &quot;code&quot; &quot;n.cond&quot; &quot;nobs&quot; &quot;model&quot; qqnorm(fit$residuals) qqline(fit$residuals) Box.test(fit$residuals, type = &quot;Ljung-Box&quot;) ## ## Box-Ljung test ## ## data: fit$residuals ## X-squared = 1.3711, df = 1, p-value = 0.2416 #H0: Autocorrelation của residual bằng 0 #H1: Autocorrelation của residual khác 0 #Dự báo forecast(fit, 3) ## Point Forecast Lo 80 Hi 80 Lo 95 Hi 95 ## 1971 798.3673 614.4307 982.3040 517.0605 1079.674 ## 1972 798.3673 607.9845 988.7502 507.2019 1089.533 ## 1973 798.3673 601.7495 994.9851 497.6663 1099.068 autoplot(forecast(fit,3)) + theme_classic() ## Warning: package &#39;bindrcpp&#39; was built under R version 3.4.4 #Tự động chọn mô hình auto.arima(Nile) ## Series: Nile ## ARIMA(1,1,1) ## ## Coefficients: ## ar1 ma1 ## 0.2544 -0.8741 ## s.e. 0.1194 0.0605 ## ## sigma^2 estimated as 20177: log likelihood=-630.63 ## AIC=1267.25 AICc=1267.51 BIC=1275.04 Mô hình trên cho thấy ARIMA(0,1,1) phản ánh tốt số lượng: Residual có phân phối chuẩn Residual có autocorrelation bằng 0 (p value &gt; 0.24) "],
["phan-tich-chui-thi-gian-vi-tidyquant-va-timetk.html", "Chương 20 Phân tích chuỗi thời gian với tidyquant và timetk 20.1 Lấy dữ liệu với package tidyquant 20.2 Trực quan hóa với geom_ma 20.3 Chuyển từ dataframe thành xts", " Chương 20 Phân tích chuỗi thời gian với tidyquant và timetk Khi phân tích dữ liệu thời gian, cấu trúc chủ yếu của chuỗi thời gian là định dang xts hoặc zoo. Tuy nhiên, khi phân tích thực tế, phần lớn ta phải sử dụng định dạng dataframe và với định dang của xts hay zoo sẽ gây khó khăn trong việc phân tích dữ liệu nhiều chuỗi thời gian trong dataframe. Package tidyquant và timetk cho phép phân tích các chỉ số chuỗi thời gian theo phong cách của tidyverse một cách nhanh chóng 20.1 Lấy dữ liệu với package tidyquant Khi sử dụng tidyquant, ta có thể lấy nhiều định dạng dữ liệu từ sản chứng khoán, thông dụng nhất bao gồm: Thông tin công ty trên sàn chứng khoán Giá chứng khoán Báo cáo tài chính Cổ tức library(tidyquant) library(timetk) library(tidyverse) # Kiểm tra các option lấy dữ liệu tq_get_options() ## [1] &quot;stock.prices&quot; &quot;stock.prices.japan&quot; &quot;financials&quot; ## [4] &quot;key.stats&quot; &quot;key.ratios&quot; &quot;dividends&quot; ## [7] &quot;splits&quot; &quot;economic.data&quot; &quot;exchange.rates&quot; ## [10] &quot;metal.prices&quot; &quot;quandl&quot; &quot;quandl.datatable&quot; # Lấy dữ liệu các công ty trên sàn chứng khoán tq_index(&quot;SP500&quot;) Lấy dữ liệu chỉ số chứng khoán # Lấy dữ liệu stock aapl_price &lt;- tq_get(&quot;AAPL&quot;, get = &quot;stock.prices&quot;, from = &quot;2010-01-01&quot;) aapl_price %&gt;% head ## # A tibble: 6 x 7 ## date open high low close volume adjusted ## &lt;date&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 2010-01-04 30.5 30.6 30.3 30.6 123432400 20.3 ## 2 2010-01-05 30.7 30.8 30.5 30.6 150476200 20.3 ## 3 2010-01-06 30.6 30.7 30.1 30.1 138040000 20.0 ## 4 2010-01-07 30.2 30.3 29.9 30.1 119282800 20.0 ## 5 2010-01-08 30.0 30.3 29.9 30.3 111902700 20.1 ## 6 2010-01-11 30.4 30.4 29.8 30.0 115557400 19.9 20.2 Trực quan hóa với geom_ma aapl_price %&gt;% select(date, close) %&gt;% ggplot(aes(date, close)) + geom_line(col = &quot;grey&quot;) + geom_ma(ma_fun = SMA, n = 30, color = &quot;blue&quot;, size = 1) + geom_ma(ma_fun = SMA,n = 300, color = &quot;red&quot;, size = 1.2) + theme_bw() 20.3 Chuyển từ dataframe thành xts aapl_price %&gt;% timetk::tk_xts(date_var = date) %&gt;% head ## open high low close volume adjusted ## 2010-01-04 30.49000 30.64286 30.34000 30.57286 123432400 20.30787 ## 2010-01-05 30.65714 30.79857 30.46429 30.62571 150476200 20.34298 ## 2010-01-06 30.62571 30.74714 30.10714 30.13857 138040000 20.01940 ## 2010-01-07 30.25000 30.28571 29.86429 30.08286 119282800 19.98239 ## 2010-01-08 30.04286 30.28571 29.86572 30.28286 111902700 20.11524 ## 2010-01-11 30.40000 30.42857 29.77857 30.01572 115557400 19.93779 "],
["trc-quan-hoa-d-liu.html", "Chương 21 Trực quan hóa dữ liệu 21.1 Xây dựng phễu bán hàng theo từng nhóm 21.2 Vẽ biểu đồ warterfall cho acive/inactive users 21.3 Xây dựng biểu đồ lollipop chart 21.4 Trực quan hóa các phần trùng lặp nhau", " Chương 21 Trực quan hóa dữ liệu 21.1 Xây dựng phễu bán hàng theo từng nhóm Trong quá trình phân tích bán hàng, phếu bán hàng (sale funnel) là một kỹ thuật rất hữu dụng để trực quan hóa kết quả kinh doanh theo từng nhóm. Tuy nhiên, hiện ít có biểu đồ nào thể hiện được phễu bán hàng một cách hiệu quả trên R. Trong mục này, tác giả sẽ hướng dẫn một ví dụ thực tiễn trực quan hóa phễu bán hàng một cách hiệu quả. Xem ví dụ điển hình về phễu bán hàng dưới đây data &lt;- read.table(textConnection( c(&quot;step;segment1;segment2;segment3;total 1_visit;1806;11663;12641;26110 2_register;1143;6476;5372;12991 3_login;1806;11663;2694;16163 4_subscribe;21;3322;2694;6037 5_paid;259;422;41;722&quot;)), header = T, sep = &quot;;&quot;) # Dữ liệu data ## step segment1 segment2 segment3 total ## 1 1_visit 1806 11663 12641 26110 ## 2 2_register 1143 6476 5372 12991 ## 3 3_login 1806 11663 2694 16163 ## 4 4_subscribe 21 3322 2694 6037 ## 5 5_paid 259 422 41 722 Trong tập dữ liệu trên, ta sẽ mô phỏng dữ liệu phếu bán hàng của 3 phân khúc khách hàng trên một trang thương mại điện tử mà trong đó, khách hàng sẽ đi qua năm bước khác nhau: Ghé thăm website (visit) Đăng ký (register) Đăng nhập (login) Đăng ký cập nhật các thông tin sản phẩm (subscribe) Mua hàng và trả tiền thành công (paid) Để tạo một biểu đồ phễu bán hàng, ta sẽ thực hiện 3 bước lớn sau. Tạo theme cho biểu đồ Tạo các biểu đồ con cho phễu bán hàng Kết hợp các biểu đồ để tạo thành phễu bán hàng hoàn chỉnh # Gọi library library(tidyverse) library(reshape2) library(forcats) library(ggthemes) # Tạo theme trông cho chart funnel_theme &lt;- theme(axis.title = element_blank(), axis.ticks.x = element_blank(), axis.text.x = element_blank(), legend.position = &quot;none&quot;, panel.grid = element_blank() ) # Phân rã dữ liệu df &lt;- data %&gt;% melt(id.vars = &quot;step&quot;) # Tạo biểu đồ chính p1 &lt;- df %&gt;% mutate(step = fct_rev(step)) %&gt;% filter(variable != &quot;total&quot;) %&gt;% ggplot(aes(step, value)) + geom_bar(aes(fill = variable), stat = &quot;identity&quot;) + facet_grid(~variable, scale = &quot;free&quot;) + coord_flip() + geom_text(aes(label = value), position = position_stack(vjust = .5)) + scale_fill_tableau() + theme_minimal() + scale_y_sqrt() + funnel_theme + theme(plot.margin=grid::unit(c(0,0,0,0), &quot;mm&quot;)) + theme( axis.text.y = element_blank(), strip.text = element_text(size = 14, face = &quot;bold&quot;)) + theme( panel.spacing = unit(0, &quot;mm&quot;)) + annotate(&quot;rect&quot;, xmin = 0.5, xmax = 1.5, ymin = 0, ymax = Inf, alpha = .2) + annotate(&quot;rect&quot;, xmin = 2.5, xmax = 3.5, ymin = 0, ymax = Inf, alpha = .2) + annotate(&quot;rect&quot;, xmin = 4.5, xmax = 5.5, ymin = 0, ymax = Inf, alpha = .2) + theme(axis.text.y = element_blank()) p1 Tạo thêm phần label tổng theo từng segment df %&gt;% mutate(step = fct_rev(step)) %&gt;% filter(variable == &quot;total&quot;) %&gt;% ggplot(aes(step, 0)) + geom_label(aes(label = value), col = &quot;white&quot;, fill = &quot;darkred&quot;, size = 4) + coord_flip() + facet_wrap(~variable) + theme_minimal() + theme(axis.text = element_blank()) + funnel_theme + theme( strip.text.x = element_blank() ) -&gt; p2 p2 Tạo thêm thứ tự các bước trong phễu bán hàng để dễ theo dõi hơn df2 &lt;- data.frame(step = data$step, value = 1:5) df2 %&gt;% mutate(step = fct_rev(step)) %&gt;% ggplot(aes(step, 1)) + geom_hline(yintercept = 1) + geom_point(size = 10, col = &quot;darkgreen&quot;) + geom_text(aes(label = value), col = &quot;white&quot;) + coord_flip() + theme_minimal() + funnel_theme + theme( axis.text = element_text(size = 14) ) -&gt; p3 p3 Cuối cùng, ta có thể tạo ghép các biểu đồ rời rạc để tạo thành phễu bán hàng hoàn chỉnh. Việc kết hợp các biểu đồ trên ggplot2 có thể hoàn thành một cách đơn giản với ggplot2 #devtools::install_github(&quot;thomasp85/patchwork&quot;) library(patchwork) p3 + labs(title = &quot;Sale funnel for 3 segments&quot;) + p1 + p2 + plot_layout(nrow = 1, widths = c(1, 8, 1)) Như vậy, chúng ta đã hoàn thành phễu bán hàng rất chuyên nghiệp với ggplot2. Phễu bán hàng này đặc biệt hiệu quả khi cùng lúc phải so sánh nhiều phân khúc khách hàng khác nhau trên toàn bộ chuỗi bán hàng. 21.2 Vẽ biểu đồ warterfall cho acive/inactive users Trong kỷ nguyên số, chỉ số active user (tạm dịch: người dùng thường xuyên hoạt động) là chỉ số đặc biệt quan trọng với bất kỳ website/ app nào. Công thức tính chỉ số người dùng thường xuyên hoạt động tại khoảng thời gian t được tính như sau: \\[active_{t} = active_{t-1} + new_{t} - churn_{t}\\] Ví dụ về waterfall chart được lấy từ ví dụ của Tableau tại đường link: [https://public.tableau.com/views/CH24_BBOD_ChurnTurnover/SubscriberChurnAnalysis] Trong case study này, chúng ta sẽ tìm cách xây dựng một biểu đồ waterfall chart tương tự # Load library library(tidyverse) library(ggplot2) library(reshape2) library(lubridate) library(grid) library(gridExtra) # Tạo dữ liệu giả lập set.seed(123) data &lt;- data.frame(date = seq(1, 372, by = 31) %&gt;% as_date) data &lt;- data %&gt;% mutate(new = abs(rnorm(12, 100, 10)) %&gt;% round(0)) %&gt;% mutate(churn = abs(rnorm(12, 50, 30)) %&gt;% round(0)) %&gt;% mutate(net = new - churn) %&gt;% mutate(eop = cumsum(net)) %&gt;% select(-net) data ## date new churn eop ## 1 1970-01-02 94 62 32 ## 2 1970-02-02 98 53 77 ## 3 1970-03-05 116 33 160 ## 4 1970-04-05 101 104 157 ## 5 1970-05-06 101 65 193 ## 6 1970-06-06 117 9 301 ## 7 1970-07-07 105 71 335 ## 8 1970-08-07 87 36 386 ## 9 1970-09-07 93 18 461 ## 10 1970-10-08 96 43 514 ## 11 1970-11-08 112 19 607 ## 12 1970-12-09 104 28 683 Trong ví dụ này, dữ liệu được tạo ngẫu nhiên sao cho số lượng active user cuối kỳ (eop - end of period) bằng với số cuối kỳ trước, thêm số lượng mới và trừ đi lượng khách hàng rời bỏ (churn). Để tạo waterfall chart, ta có thể sử dụng geom_segment trong ggplot2 # Xác định độ rộng của segment step &lt;- 0.4*(max(data$date) - min(data$date))/(nrow(data) - 1) # Xác định ymax data &lt;- data %&gt;% mutate(ymax = eop + churn) # Xác định ymin df &lt;- data %&gt;% melt(id.vars = c(&quot;date&quot;, &quot;eop&quot;, &quot;ymax&quot;)) %&gt;% mutate(ymin = ymax - value) %&gt;% rename(group = variable) # Xác định xmin và xmax df &lt;- df %&gt;% mutate(xmin = case_when( group == &quot;new&quot; ~ date - step, TRUE ~ date )) %&gt;% mutate(xmax = case_when( group == &quot;new&quot; ~ date, TRUE ~ date + step )) # Create waterfall chart p1 &lt;- df %&gt;% arrange(date) %&gt;% ggplot() + geom_rect(aes(xmin = xmin, xmax = xmax, ymin = ymin, ymax = ymax, fill = group)) p1 Như vậy, ta đã tạo xong biểu đồ water-fall đơn giản. Ở bước tiếp theo, chúng ta cần điều chỉnh lại các thành phần cho biểu đồ. # Tạo dữ liệu cho biểu đồ đường df2 &lt;- df %&gt;% select(date, eop) %&gt;% distinct() # Điều chỉnh theme p2 &lt;- p1 + geom_line(aes(date, eop), col = &quot;dodgerblue4&quot;, size = 1) + geom_point(aes(date, eop), col = &quot;dodgerblue4&quot;, size = 2.5) + geom_text(aes(date, eop, label = eop), vjust = 1.2, hjust = -0.1) + scale_fill_manual(values = c(&quot;grey60&quot;, &quot;coral2&quot;)) + theme_minimal() + theme( axis.line = element_line(color = &quot;gray40&quot;, size = 0.5), legend.position = &quot;top&quot;) + scale_x_date(breaks = data$date, date_labels = &quot;%b&quot;) + theme(panel.grid.minor.x = element_blank(), legend.title = element_blank()) + ggtitle(&quot;Overview of active users&quot;) + xlab(&quot;Date&quot;) + ylab(&quot;Number of active users&quot;) p2 Bước tiếp theo, ta cần xây dựng biểu đồ bar đơn giản để có thể đưa vào góc phần tư bên trái của biểu đồ vừa tạo. p3 &lt;- df %&gt;% mutate(value = case_when( group == &quot;churn&quot; ~ -1 * value, TRUE ~ value )) %&gt;% ggplot(aes(date, value)) + geom_bar(aes(fill = group), stat = &quot;identity&quot;) + scale_fill_manual(values = c(&quot;grey60&quot;, &quot;coral2&quot;)) + theme_minimal() + theme( legend.position = &quot;none&quot;, axis.title.x = element_blank(), axis.title.y = element_blank(), axis.ticks.y = element_blank(), axis.text.y = element_blank(), panel.grid.minor = element_blank(), panel.grid.major = element_blank(), axis.text.x = element_text(angle = 90) ) + scale_x_date(breaks = data$date, date_labels = &quot;%b&quot;) p3 Cuối cùng, ta có thể nhóm hai biểu đồ trên với grid &amp; gridExtra. grid.newpage() # Xác định vị trí cho biểu đồ chính position_1 &lt;- viewport(width = 1, height = 1, x = 0.5, y = 0.5) # Vị trí cho biểu đồ phụ position_2 &lt;- viewport(width = 0.35, height = 0.25, x = 0.25, y = 0.75) print(p2, vp = position_1) print(p3, vp = position_2) 21.3 Xây dựng biểu đồ lollipop chart Trong trực quan hóa dữ liệu, lollipop chart tuy không phải là một trong những biểu đồ phổ biến nhưng lại rất hiệu quả khi muốn thể hiện sự dịch thay đổi của một chỉ số giữa hai điểm thời gian. Trong case study này, ta xây dựng biểu đồ lollipop chart với ggplot2. # Load library library(tidyverse) library(gapminder) Để xây dựng biểu đồ, ta sử dụng dữ liệu gapminder từ tập dữ liệu gapminder. Mục tiêu là xây dựng biểu đồ thể hiện được sự thay đổi GDP/đầu người của các nước châu Âu trong năm 1952 so với năm 1977. data &lt;- gapminder %&gt;% filter(continent == &quot;Europe&quot;) %&gt;% filter(year %in% c(1952, 1977)) %&gt;% select(country, year, gdpPercap) %&gt;% spread(year, gdpPercap) %&gt;% rename(y1952 = `1952`, y1977 = `1977`) data %&gt;% head(10) ## # A tibble: 10 x 3 ## country y1952 y1977 ## &lt;fct&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 Albania 1601. 3533. ## 2 Austria 6137. 19749. ## 3 Belgium 8343. 19118. ## 4 Bosnia and Herzegovina 974. 3528. ## 5 Bulgaria 2444. 7612. ## 6 Croatia 3119. 11305. ## 7 Czech Republic 6876. 14800. ## 8 Denmark 9692. 20423. ## 9 Finland 6425. 15605. ## 10 France 7030. 18293. Biểu đồ lollipop có thể xây dựng dựa trên geom_point và geom_segment như sau. Biểu đồ đầu tiên data %&gt;% ggplot(aes(x = country)) + # Tạo đường nối giữa hai điểm geom_segment(aes(y = y1952, yend = y1977, x = country, xend = country), size = 1, col = &quot;grey50&quot;) + # Tạo điểm đầu geom_point(aes(country, y1952, color = &quot;1952&quot;), size = 3.5) + # Tạo điểm cuối geom_point(aes(country, y1977, color = &quot;1977&quot;), size = 3.5) + coord_flip() Tuy nhiên, với biểu đồ trên, ta thấy xuất hiện hai lỗi cơ bản sau: Thứ nhất, thứ tự các các quan sát đang để dạng mặc định. Do đó, kết quả trực quan hóa chỉ mang tính thông tin mà chưa có yêu tố kể chuyện (story telling). Ta có thể giải quyết bằng cách sắp xếp lại factor theo thứ tự từ thấp đến cao. Thứ hai, biểu đồ thể hiện theo chiều ngang. Do đó, các thông tin không cần thiết có thể được loại bỏ để biểu đồ gọn gàng và mạch lạc hơn. Ta có thể chỉnh lại biểu đồ như sau. # Tinh chỉnh theme cho biểu đồ my_theme &lt;- function(...) { theme_bw() + theme(plot.background = element_rect(fill = &quot;white&quot;)) + theme(panel.grid.minor = element_blank()) + theme(panel.grid.major.y = element_blank()) + theme(panel.grid.major.x = element_line()) + theme(axis.ticks = element_blank()) + theme(panel.border = element_blank()) + theme(text = element_text(size = 13, color = &quot;black&quot;)) + theme(plot.subtitle = element_text(color = &quot;gray20&quot;, size = 10, face = &quot;italic&quot;)) + theme(legend.title = element_text(size = 10, color = &quot;gray20&quot;)) + theme(legend.position = &quot;top&quot;) } # Tạo biểu đồ mới p1 &lt;- data %&gt;% mutate(country = fct_reorder(country, y1977)) %&gt;% ggplot(aes(x = country)) + geom_segment(aes(y = y1952, yend = y1977, x = country, xend = country), size = 1, col = &quot;grey50&quot;, alpha = 0.7) + geom_point(aes(country, y1952, color = &quot;1952&quot;), size = 3.5, alpha = 0.7) + geom_point(aes(country, y1977, color = &quot;1977&quot;), size = 3.5, alpha = 0.7) + coord_flip() + my_theme() + scale_y_continuous(breaks = seq(0, 30000, by = 5000), limits = c(800, 27000)) + scale_color_manual( name = &quot;Year&quot;, labels = c(&quot;1952&quot;, &quot;1977&quot;), values = c(&quot;darkblue&quot;, &quot;darkred&quot;) ) + labs(x = NULL, y = NULL, title = &quot;An example of lollipop chart&quot;, subtitle = &quot;Changes of GDP per capita in Europe&quot;, caption = &quot;Created by RAnalytics.vn&quot;) p1 Bonus: Để nhấn mạnh hơn sự thay đổi của GDP per capita, ta có thể vẽ thêm các đường nối các điểm trong biểu đồ như sau. p2 &lt;- p1 + geom_line(aes(as.numeric(country), y1977), col = &quot;darkred&quot;) p2 21.4 Trực quan hóa các phần trùng lặp nhau library(tidyverse) library(UpSetR) movies &lt;- read.csv( system.file(&quot;extdata&quot;, &quot;movies.csv&quot;, package = &quot;UpSetR&quot;), header=TRUE, sep=&quot;;&quot; ) movies %&gt;% head ## Name ReleaseDate Action Adventure Children ## 1 Toy Story (1995) 1995 0 0 1 ## 2 Jumanji (1995) 1995 0 1 1 ## 3 Grumpier Old Men (1995) 1995 0 0 0 ## 4 Waiting to Exhale (1995) 1995 0 0 0 ## 5 Father of the Bride Part II (1995) 1995 0 0 0 ## 6 Heat (1995) 1995 1 0 0 ## Comedy Crime Documentary Drama Fantasy Noir Horror Musical Mystery ## 1 1 0 0 0 0 0 0 0 0 ## 2 0 0 0 0 1 0 0 0 0 ## 3 1 0 0 0 0 0 0 0 0 ## 4 1 0 0 1 0 0 0 0 0 ## 5 1 0 0 0 0 0 0 0 0 ## 6 0 1 0 0 0 0 0 0 0 ## Romance SciFi Thriller War Western AvgRating Watches ## 1 0 0 0 0 0 4.15 2077 ## 2 0 0 0 0 0 3.20 701 ## 3 1 0 0 0 0 3.02 478 ## 4 0 0 0 0 0 2.73 170 ## 5 0 0 0 0 0 3.01 296 ## 6 0 0 1 0 0 3.88 940 upset(movies, nsets = 3, nintersects = 30, mb.ratio = c(0.55, 0.45), order.by = c(&quot;freq&quot;, &quot;degree&quot;), decreasing = c(TRUE,FALSE), mainbar.y.label = &quot;# of users by groups&quot;, sets.x.label = &quot;# of users by platforms&quot;, queries = list(list(query = intersects, params = list(&quot;Drama&quot;, &quot;Action&quot;), color = &quot;darkred&quot;, active = T)), main.bar.color = &quot;darkgreen&quot;, text.scale = 1.4, shade.alpha = 0.55, point.size = 3) # Lưu file về máy dev.copy(png,&#39;myplot.png&#39;, width = 600, height = 400) dev.off() "],
["cac-meo-trong-r.html", "Chương 22 Các mẹo trong R 22.1 Hiển thị số bình thường 22.2 Export dữ liệu ra excel 22.3 Làm việc khi proxy bị chặn 22.4 Tự động render ra kết quả phân tích", " Chương 22 Các mẹo trong R 22.1 Hiển thị số bình thường Vấn đề: Khi sử dụng R, dữ liệu thường xuyên hiển thị dưới dạng khoa học (scientific). Giải pháp: Sử dụng options options(scipen = 999) để bỏ hiển thị dạng khoa học trong R 22.2 Export dữ liệu ra excel Vấn đề: Export dữ liệu từ dataframe ra excel Giải pháp: Sử dụng openxlsx để export dữ liệu từ dataframe ra excel library(openxlsx) library(tidyverse) # Tạo dữ liệu ---- df &lt;- data.frame(&quot;Date&quot; = Sys.Date()-0:4, &quot;Logical&quot; = c(TRUE, FALSE, TRUE, TRUE, FALSE), &quot;Currency&quot; = paste(&quot;$&quot;,-2:2), &quot;Accounting&quot; = -2:2, &quot;hLink&quot; = &quot;https://CRAN.R-project.org/&quot;, &quot;Percentage&quot; = seq(-1, 1, length.out=5), &quot;TinyNumber&quot; = runif(5) / 1E9, stringsAsFactors = FALSE) df$Date &lt;- df$Date %&gt;% as.character() class(df$Currency) &lt;- &quot;currency&quot; class(df$Accounting) &lt;- &quot;accounting&quot; class(df$hLink) &lt;- &quot;hyperlink&quot; class(df$Percentage) &lt;- &quot;percentage&quot; class(df$TinyNumber) &lt;- &quot;scientific&quot; ## Format ---- options(&quot;openxlsx.borderStyle&quot; = &quot;thin&quot;) options(&quot;openxlsx.borderColour&quot; = &quot;#4F81BD&quot;) ## Heading format hs1 &lt;- createStyle(fgFill = &quot;darkgreen&quot;, halign = &quot;CENTER&quot;, textDecoration = &quot;Bold&quot;, border = &quot;Bottom&quot;, fontColour = &quot;white&quot;) ## Insert data (simple) ---- wb &lt;- createWorkbook() addWorksheet(wb, &quot;writeData auto-formatting&quot;) writeData(wb, 1, df, startRow = 1, startCol = 1, headerStyle = hs1, borders = &quot;rows&quot;, borderStyle = &quot;thin&quot;) ## Thêm dữ liệu (datatable) addWorksheet(wb, &quot;Sheet2&quot;) setColWidths(wb, 2, 1:100, widths = &quot;auto&quot;) writeDataTable(wb, 2, df, startRow = 1, startCol = 1, tableStyle = &quot;TableStyleLight1&quot;, headerStyle = hs1, withFilter = F) openXL(wb) ## Xóa gridLine cho tât cả các sheet ---- 1:length(wb$sheet_names) %&gt;% map(as_mapper(function(x){ showGridLines(wb, x, showGridLines = F) })) # Lưu dữ liệu ---- saveWorkbook(wb, file = &quot;my_file.xlsx&quot;, overwrite = TRUE) 22.3 Làm việc khi proxy bị chặn Sửa option chọn mặc định proxy: Tools-Global &gt;&gt; Options-Packages &gt;&gt; Uncheck Use Internet Explorer library/proxy for HTTP Restart lại R Gõ câu lệnh file.edit('~/.Renviron') Thêm nội dung sau vào trong R options(internet.info = 0) http_proxy=&quot;http://user_id:password@your_proxy:your_port&quot; # Ví dụ http_proxy=&quot;http://anhhd3:password*@10.128.10.88:8080&quot; 22.4 Tự động render ra kết quả phân tích Lưu ý: Cần có hai dấu cách (space) trước \\n Đặt chế độ result = 'asis' để biến text thành kết quả ```{r result = &#39;asis&#39;} for(i in unique(Month)) { cat(&quot; \\n###&quot;, month.name[i], &quot;Air Quaility \\n&quot;) #print(plot(airquality[airquality$Month == i,])) plot(airquality[airquality$Month == i,]) cat(&quot; \\n&quot;) } ``` "]
]
